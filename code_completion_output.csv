file_path,prefix,middle,suffix,generated_code
../airflow/dags/utils/price_processing.py,"import re


# Helper function to parse price strings into floats
def parse_price(price_str):
    # Remove non-numeric characters except for decimal points or commas
    clean_str = re.sub(r'[^0-9.,]', '', price_str)
    clean_str = clean_str.replace(',', '.').replace(""'"", '.')

    try:
        # If it contains a decimal, treat it as a float
        if '.' in clean_str:
            return float(clean_str)
        # Otherwise, treat the last two digits as the decimal part
        elif len(clean_str) > 2:
            return float(clean_str[:-2] + '.' + clean_str[-2:])
        else:
            return float(clean_str)
    except ValueError:
        return None


# EsoMarket Condition
def process_esomarket(price_str):
    price = parse_price(price_str)
    return price if price else None


def process_penny(price_str, price_type):
    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Clean up extracted prices and convert them to floats
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Common cents values like 90 or 99
    common_cents = [90, 99]

    # Handle cases based on the length of parsed prices
    if len(parsed_prices) == 3:
        # Handle cases like ""19 90 25.90 2""
        item_price = float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")
        initial_price = parsed_prices[2]
        return {""item_price"": item_price, ""initial_price"": initial_price}

    if len(parsed_prices) == 2:
        # If the second price is commonly a ""cents"" part like 90 or 99, merge with the first
        if parsed_prices[1] in common_cents:
            return {""item_price"": float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}

","    if len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Billa Condition
","def process_billa(price_str, price_type):
    # Detect volume keywords: pri koupi, kupte, etc.
    volume_keywords = ['pri', 'koupi', 'kupte', 'ks', 'bodi', 'bodu', 'up te', 'aza']
    volume_detected = any(keyword in price_str.lower() for keyword in volume_keywords)

    # Extract numeric parts from the string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle specific distracted membership or volume words
    if 'bodi' in price_str.lower() or 'bodu' in price_str.lower():
        return {'item_member_price': '75bodi'}

    # Check if there are two prices and handle them
    if len(parsed_prices) == 2:
        # If the second value is an integer <5, treat it as volume, not initial_price
        if parsed_prices[1] < 5 and parsed_prices[1].is_integer():
            return {""item_price"": parsed_prices[0], ""volume"": str(int(parsed_prices[1]))}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Define Albert Hypermarket parsing method
def process_albert_hypermarket(price_str, price_type):
    # Clean string by keeping numbers and relevant separators
    clean_str = re.sub(r'[^0-9\s.,\'\-:]', '', price_str)  # Allow special chars like -, :, '

    # Handle specific cases for '-' or ':' as separators for integer prices
    combined_prices = []
    tokens = clean_str.split()

    for token in tokens:
        # Case 1: Numbers ending with ""-"" or "":""
        if token.endswith('-') or token.endswith(':'):
            token = token[:-1]  # Remove the trailing symbol
            combined_prices.append(parse_price(token))
        elif ""'"" in token:
            # Case 2: Handle cases like ""31'90""
            parts = token.split(""'"")
            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
                combined_price = f""{parts[0]}.{parts[1]}""
                combined_prices.append(parse_price(combined_price))
            else:
                combined_prices.append(parse_price(token))
        else:
            combined_prices.append(parse_price(token))

    # Filter out None values
    parsed_prices = [p for p in combined_prices if p is not None]

    # Condition: If the price is less than 5, treat it as invalid (exclude it)
    if parsed_prices and parsed_prices[0] < 5:
        return None

    # Assign prices based on the price_type
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Function to handle Tesco Supermarket OCR strings
def process_tesco_supermarket(price_str, price_type):
    # Handle dates (e.g., ""12.7. - 14.7."") by ignoring them
    date_pattern = r'\d{1,2}\.\d{1,2}\.\s*-\s*\d{1,2}\.\d{1,2}\.'  # Pattern for dates like ""12.7. - 14.7.""
    clean_str = re.sub(date_pattern, '', price_str)

    # Skip strings with percentages or irrelevant text
    if ""%"" in clean_str or ""HOP"" in clean_str:
        return None

    # Extract price values, specifically for club card or ""cena"" keyword
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Logic to differentiate between item prices and initial prices
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Lidl Condition
def process_lidl(price_str):
    return parse_price(price_str)


# Kaufland Condition
def process_kaufland(price_str, price_type):
    if re.search(r'(\d+[.,]\d+)\s+(\d+[.,]\d+)', price_str):
        return None  # Skip sequences of more than 2 prices

    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[-1], ""initial_price"": parsed_prices[0]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Flop Top Condition
def process_flop_top(price_str, price_type):
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Travel Free Condition
def process_travel_free(price_str, price_type):
    # Removing any € symbols to focus only on numeric data
    clean_str = price_str.replace(""€"", """").strip()

    # Find all the price values in the string
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Ensure prices are sorted correctly (sale price is less than initial price)
    if len(parsed_prices) == 2:
        sale_price = min(parsed_prices)
        initial_price = max(parsed_prices)
        return {""item_price"": sale_price, ""initial_price"": initial_price}

    # If we only have one price, return it as the item price
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# CBA Potraviny Condition
def process_cba_potraviny(price_str):
    return parse_price(price_str)


# Bene Condition
def process_bene(price_str):
    return parse_price(price_str)


# CBA Premium Condition
def process_cba_premium(price_str):
    return parse_price(price_str)


# Lidl Shop Condition
def process_lidl_shop(price_str):
    return parse_price(price_str)


# CBA Market Condition
def process_cba_market(price_str):
    return parse_price(price_str)


# Updated Makro Condition with improved packaging detection
def process_makro(price_str, price_type):
    # Extract packaging information (must be at the beginning of the string)
    packaging_pattern = re.match(r'^(\d+-?\d?\s*(BAL|ks|A VICE|AViCE))', price_str)

    # If packaging is found, extract it and continue processing the price
    packaging = None
    if packaging_pattern:
        packaging = packaging_pattern.group()  # Extract the packaging
        price_str = price_str[len(packaging):].strip()  # Remove packaging from the price string

    # Extract all numeric parts (prices) after the packaging
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Convert extracted prices to float
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If there are two prices, assign them as item_price and initial_price
    if len(parsed_prices) >= 2:
        return {
            ""item_price"": parsed_prices[0],
            ""initial_price"": parsed_prices[1],
            ""packaging"": packaging
        }
    elif len(parsed_prices) == 1:
        # If there's only one price, treat it as the item price
        return {
            ""item_price"": parsed_prices[0],
            ""packaging"": packaging
        }
    else:
        return None


# Function to process Ratio price strings
def process_ratio(price_str):
    # Extract prices ignoring ""bezDPH"" or ""vcetneDPH"" text
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If two prices are found, one should be item_price, the other initial_price
    if len(parsed_prices) == 2:
        return {""cena bez dph"": parsed_prices[0], ""item_price"": parsed_prices[1]}
    return None


# Function to process Globus price strings
def process_globus(price_str, price_type):
    # Skip percentage strings or invalid non-numeric inputs
    if ""%"" in price_str or re.search(r'[^\d.,\'\s-]', price_str):
        return None

    # Handle cases like ""14'90"" or ""44'90"" by replacing apostrophe with a decimal point
    price_str = price_str.replace(""'"", ""."")

    # Handle cases like ""17 90"" by joining them into a valid decimal format
    if re.search(r'\d+\s+\d{2}', price_str):
        price_str = price_str.replace("" "", ""."")

    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle item_price and item_member_price based on price_type
    if price_type == ""item_price"":
        # If one price is found, return it as the item price
        if len(parsed_prices) == 1:
            return {""item_price"": parsed_prices[0]}
    elif price_type == ""item_member_price"":
        # If member price is found, return it
        if len(parsed_prices) == 1:
            return {""item_member_price"": parsed_prices[0]}

    return None


# Function to process Tamda Foods price strings
def process_tamda_foods(price_str, price_type):
    # Skip percentage strings and invalid inputs
    if ""%"" in price_str or ""("" in price_str:
        return None

    # Handle cases like ""1290 KC"", ""3490Kc"", and ""5290KC"" (ignoring the ""KC"" part)
    price_str = re.sub(r'[KCkc]+', '', price_str).strip()

    # Extract numeric parts
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 1:
        if price_type == ""item_member_price"":
            return {""item_member_price"": parsed_prices[0]}
        elif price_type == ""item_price"":
            return {""item_price"": parsed_prices[0]}

    return None


# Function to process all types of prices based on class_id
def process_price_by_class_id(shop_name, got_ocr_text, class_id):
    processed_price = None

    # Check class_id for the type of price
    if class_id == ""item_price"":
        price_type = ""item_price""
    elif class_id == ""item_member_price"":
        price_type = ""item_member_price""
    elif class_id == ""item_initial_price"":
        price_type = ""item_initial_price""
    else:
        return None

    # Dispatch based on shop_name and price_type
    if shop_name == ""EsoMarket"":
        processed_price = process_esomarket(got_ocr_text)
    elif shop_name == ""Penny"":
        processed_price = process_penny(got_ocr_text, price_type)
    elif shop_name == ""Billa"":
        processed_price = process_billa(got_ocr_text, price_type)
    elif shop_name in [""Albert Hypermarket"", ""Albert Supermarket""]:
        processed_price = process_albert_hypermarket(got_ocr_text, price_type)
    elif shop_name in [""Tesco Supermarket"", ""Tesco Hypermarket""]:
        processed_price = process_tesco_supermarket(got_ocr_text, price_type)
    elif shop_name == ""Lidl"":
        processed_price = process_lidl(got_ocr_text)
    elif shop_name == ""Kaufland"":
        processed_price = process_kaufland(got_ocr_text, price_type)
    elif shop_name in [""Flop Top"", ""Flop""]:
        processed_price = process_flop_top(got_ocr_text, price_type)
    elif shop_name == ""Travel Free"":
        processed_price = process_travel_free(got_ocr_text, price_type)
    elif shop_name == ""CBA Potraviny"":
        processed_price = process_cba_potraviny(got_ocr_text)
    elif shop_name == ""Bene"":
        processed_price = process_bene(got_ocr_text)
    elif shop_name == ""CBA Premium"":
        processed_price = process_cba_premium(got_ocr_text)
    elif shop_name == ""Lidl Shop"":
        processed_price = process_lidl_shop(got_ocr_text)
    elif shop_name == ""CBA Market"":
        processed_price = process_cba_market(got_ocr_text)
    elif shop_name == ""Makro"":
        processed_price = process_makro(got_ocr_text, price_type)
    elif shop_name == ""Globus"":
        processed_price = process_globus(got_ocr_text, price_type)
    elif shop_name == ""Tamda Foods"":
        processed_price = process_tamda_foods(got_ocr_text, price_type)
    elif shop_name == ""Ratio"":
        processed_price = process_ratio(got_ocr_text)

    return processed_price
","

    # Logic to compare against previous formatting (price was less than current price)
    if packaging is not None:
        new_price = packaging[: packaging.index('.')]  # Add the packaging index before replacement
        old_price = packaging[packaging.index('.') + 1:-1].strip()

        if old_price!= new_price:
            return False

    return True


# New York Condition
def process_newyork(price_str):
    return parse_price(price_str)


# Tango Condition
def process_tango(price_str):
    return parse_price"
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
","        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
","        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
",   
../airflow/dags/data_pipeline.py,"import json
import ast
import logging
import os
import subprocess
import time

import boto3
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from utils.pdf_utils import split_pdf_to_pages
from utils.yolo_ocr_utils import run_yolo_on_pages, got_text_from_image
from utils.correct_names import process_single_word, Trie, preprocess_text
from airflow.utils.trigger_rule import TriggerRule
from utils.price_processing import process_price_by_class_id
from utils.s3_dynamodb_utils import save_item_to_dynamodb, download_file_from_s3
from airflow.operators.python import BranchPythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.models.variable import Variable
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

region = 'eu-west-1'
instances = ['i-09234cbd67d00b0ce']
ec2 = boto3.client('ec2', region_name=region)

# Constants for directories
PAGES_S3_PATH = 'pages/valid'
TEMP_DIR = '/tmp'
ITEMS_S3_DIR = 'item_detected/images/valid'
DETECTIONS_S3_DIR = 'item_detected/valid'

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

AIRFLOW_HOME = os.getenv('AIRFLOW_HOME', '/home/ubuntu/airflow')
file_path = os.path.join(AIRFLOW_HOME, 'dags/utils/item_names/unique_item_names.txt')


def run_ec2_instances(**kwargs):
    ec2.start_instances(InstanceIds=instances)
    print('started your instances: ' + str(instances))
    time.sleep(60)
    return ""EC2 started""

# Function to delete SageMaker endpoints
def stop_ec2_instances(**kwargs):
    ec2.stop_instances(InstanceIds=instances)
    print('stopped your instances: ' + str(instances))
    time.sleep(60)
    return ""EC2 stopped""

def yolo_on_pages(page_filenames, **kwargs):
    """"""
    Processes YOLO predictions, extracts images, and uploads results to S3.

    Args:
        page_filenames (list or str): List of page filenames or string representation of a list.

    Returns:
        dict: Contains saved S3 paths for predictions and images.
    """"""
    # Convert page_filenames to list if it's a string
    if isinstance(page_filenames, str):
        try:
            page_filenames = ast.literal_eval(page_filenames)
        except (ValueError, SyntaxError):
            raise ValueError(f""Invalid input for page_filenames: {page_filenames}"")

    # Run YOLO on pages
    predictions, s3_saved_images = run_yolo_on_pages(page_filenames, ""item_detection_data"",
                                                     save_images=True, model='model1',
                                                     detection_output_path=DETECTIONS_S3_DIR)
    return json.dumps({'predictions': predictions, 'saved_images': s3_saved_images})


def process_detected_items_step(detection_data, shop_name, valid, **kwargs):
    """"""
    Processes detected items, runs YOLO Model 2, performs OCR, and saves results in DynamoDB.

    Args:
        detection_data (str): Contains S3 paths to detected images and `.txt` files.
        shop_name (str): Name of the shop.

    Returns:
        list: Processed items with OCR text and Model 2 detection results.
    """"""
    logger.info(""Starting process_detected_items_step"")
    # Load and preprocess item names for Trie
    with open(file_path, 'r', encoding='utf-8') as f:
        item_names = f.readlines()
        words = [preprocess_text(line).split() for line in item_names]
        flat_words = [word for sublist in words for word in sublist]

    # Initialize Trie with item names
","    trie = Trie()
    for word in flat_words:
        trie.insert(word)

    # Parse detection_data string to dictionary
","    try:
        detection_data = ast.literal_eval(detection_data)
    except Exception as e:
        logger.error(f""Error parsing detection_data: {e}"")
        raise

    saved_images = detection_data.get('saved_images', [])
    processed_items = []

    if not saved_images:
        logger.info(""No images to process."")
        return processed_items

    try:
        # Run YOLO Model 2 on images and perform OCR
        predictions, s3_saved_images = run_yolo_on_pages(saved_images, ""item_processing_data"",
                                                         model='model2', include_ocr=True)

        # Process detections for each image
        for s3_image_path, detected_object_data in predictions.items():
            try:
                # Download image from S3
                local_image_path = os.path.join(TEMP_DIR, os.path.basename(s3_image_path))
                download_file_from_s3(s3_image_path, local_image_path)

                # Perform OCR on the whole image
                whole_image_text = got_text_from_image(local_image_path)
                os.remove(local_image_path)  # Clean up local file

            except Exception as e:
                logger.error(f""Error processing image {s3_image_path}: {e}"")

            # Initialize detection fields
            object_name = processed_item_name = item_price = processed_item_price = None
            item_member_price = processed_item_member_price = item_initial_price = processed_item_initial_price = None

            # Process detections based on class IDs
            for detection in detected_object_data:
                class_id = detection['class_name']
                ocr_text = detection.get('ocr_text', '')

                if class_id == 'item_name':
                    object_name = ocr_text
                    processed_item_name = process_single_word(ocr_text, trie)
                elif class_id in ['item_price', 'item_member_price', 'item_initial_price']:
                    processed_price = process_price_by_class_id(shop_name, ocr_text, class_id)
                    if class_id == 'item_price':
                        item_price, processed_item_price = ocr_text, processed_price
                    elif class_id == 'item_member_price':
                        item_member_price, processed_item_member_price = ocr_text, processed_price
                    elif class_id == 'item_initial_price':
                        item_initial_price, processed_item_initial_price = ocr_text, processed_price

            # Create detected object data for DynamoDB
            detected_object = {
                ""image_id"": s3_image_path,
                ""item_name"": object_name,
                ""processed_item_name"": processed_item_name,
                ""whole_image_ocr_text"": whole_image_text,
                ""model2_detections"": detected_object_data,
                ""shop_name"": shop_name,
                ""item_price"": item_price,
                ""processed_item_price"": str(processed_item_price),
                ""item_member_price"": item_member_price,
                ""processed_item_member_price"": str(processed_item_member_price),
                ""item_initial_price"": item_initial_price,
                ""processed_item_initial_price"": str(processed_item_initial_price),
                ""valid"": valid == ""True""
            }

            # Save the object to DynamoDB
            save_item_to_dynamodb(""detected_data"", detected_object)
            processed_items.append(detected_object)

    except Exception as e:
        logger.error(f""Error processing images: {e}"")

    logger.info(""Finished processing all detected images"")
    return processed_items


# Function to check if models are already deployed
def check_models_deployed(**kwargs):
    deployed = Variable.get('models_deployed', default_var=False)
    logger.info(f""Current value of models_deployed: {deployed}"")
    if deployed == 'True':
        return ""skip_deploy_models""
    else:
        set_deployed_flag()
        return ""deploy_models""


def set_deployed_flag():
    Variable.update(""models_deployed"", True)

def reset_deployed_flag():
    Variable.set(""models_deployed"", False)

# Define the DAG and tasks
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

dag = DAG('pages_data_pipeline', default_args=default_args, schedule_interval=None, max_active_tasks=1)


def log_params(**context):
    reset_deployed_flag()
    filename = context['dag_run'].conf.get('filename', '')
    shop_name = context['dag_run'].conf.get('shop_name', '')
    logger.info(f""Filename: {filename}, Shop Name: {shop_name}"")
    return filename, shop_name


with (dag):
    # Task to log parameters
    log_task = PythonOperator(
        task_id='log_params',
        python_callable=log_params,
        provide_context=True,
        dag=dag
    )

    # Task to check if models are deployed
    check_deploy_task = BranchPythonOperator(
        task_id='check_deploy_task',
        python_callable=check_models_deployed,
        dag=dag
    )

    # Task to skip deployment if models are already deployed
    skip_deploy_models = PythonOperator(
        task_id='skip_deploy_models',
        python_callable=lambda: time.sleep(60),
        dag=dag
    )

    # Task to deploy models
    deploy_models_task = PythonOperator(
        task_id='deploy_models',
        python_callable=run_ec2_instances,
        dag=dag
    )

    # Task to split the PDF
    split_task = PythonOperator(
        task_id='split_pdf',
        python_callable=split_pdf_to_pages,
        op_kwargs={
            'filename': '{{ dag_run.conf[""filename""] }}',
            'shop_name': '{{ dag_run.conf[""shop_name""] }}',
            'valid': '{{ dag_run.conf[""valid""] }}'
        },
        dag=dag
    )

    # Task to detect items using YOLO
    detect_items_task = PythonOperator(
        task_id='yolo_on_pages',
        python_callable=yolo_on_pages,
        op_kwargs={'page_filenames': '{{ ti.xcom_pull(task_ids=""split_pdf"") }}'},
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to process detected items and save them to DynamoDB
    process_task = PythonOperator(
        task_id='process_detected_items',
        python_callable=process_detected_items_step,
        op_kwargs={
            'detection_data': '{{ ti.xcom_pull(task_ids=""yolo_on_pages"") }}',
            'shop_name': '{{ dag_run.conf[""shop_name""] }}',
            'valid': '{{ dag_run.conf[""valid""] }}'
        },
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to wait for other pipelines
    wait_for_others_task = ExternalTaskSensor(
        task_id='wait_for_other_pipelines',
        external_dag_id='pages_data_pipeline',  # Assuming other pipelines have the same DAG ID
        external_task_id='process_detected_items',  # Wait for this task in all other DAGs
        mode='poke',  # Can be 'reschedule' to avoid loading the database
        poke_interval=10,  # Check every 30 seconds
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to delete endpoints at the end
    delete_endpoints_task = PythonOperator(
        task_id='delete_endpoints',
        python_callable=stop_ec2_instances,
        trigger_rule=TriggerRule.ALL_DONE,  # Only run if all upstream tasks succeed
        dag=dag
    )

    trigger_other_pipeline_task = TriggerDagRunOperator(
        task_id='trigger_other_pipeline',
        trigger_dag_id='check_file_validity_and_update_detected_items',
        dag=dag
    )

    # Task dependencies
    log_task >> split_task >> check_deploy_task
    check_deploy_task >> [skip_deploy_models, deploy_models_task]
    deploy_models_task >> detect_items_task
    skip_deploy_models >> detect_items_task
    detect_items_task >> process_task >> wait_for_others_task >> delete_endpoints_task >> trigger_other_pipeline_task
","    trie = Trie()

    # Run YOLO Model 2 on images and perform OCR
"
../airflow/dags/utils/price_processing.py,"import re


# Helper function to parse price strings into floats
def parse_price(price_str):
    # Remove non-numeric characters except for decimal points or commas
    clean_str = re.sub(r'[^0-9.,]', '', price_str)
    clean_str = clean_str.replace(',', '.').replace(""'"", '.')

    try:
        # If it contains a decimal, treat it as a float
        if '.' in clean_str:
            return float(clean_str)
        # Otherwise, treat the last two digits as the decimal part
        elif len(clean_str) > 2:
            return float(clean_str[:-2] + '.' + clean_str[-2:])
        else:
            return float(clean_str)
    except ValueError:
        return None


# EsoMarket Condition
def process_esomarket(price_str):
    price = parse_price(price_str)
    return price if price else None


def process_penny(price_str, price_type):
    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Clean up extracted prices and convert them to floats
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Common cents values like 90 or 99
    common_cents = [90, 99]

    # Handle cases based on the length of parsed prices
    if len(parsed_prices) == 3:
        # Handle cases like ""19 90 25.90 2""
        item_price = float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")
        initial_price = parsed_prices[2]
","        return {""item_price"": item_price, ""initial_price"": initial_price}

    if len(parsed_prices) == 2:
        # If the second price is commonly a ""cents"" part like 90 or 99, merge with the first
        if parsed_prices[1] in common_cents:
            return {""item_price"": float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}

    if len(parsed_prices) == 1:
","        return {""item_price"": parsed_prices[0]}

    return None


# Billa Condition
def process_billa(price_str, price_type):
    # Detect volume keywords: pri koupi, kupte, etc.
    volume_keywords = ['pri', 'koupi', 'kupte', 'ks', 'bodi', 'bodu', 'up te', 'aza']
    volume_detected = any(keyword in price_str.lower() for keyword in volume_keywords)

    # Extract numeric parts from the string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle specific distracted membership or volume words
    if 'bodi' in price_str.lower() or 'bodu' in price_str.lower():
        return {'item_member_price': '75bodi'}

    # Check if there are two prices and handle them
    if len(parsed_prices) == 2:
        # If the second value is an integer <5, treat it as volume, not initial_price
        if parsed_prices[1] < 5 and parsed_prices[1].is_integer():
            return {""item_price"": parsed_prices[0], ""volume"": str(int(parsed_prices[1]))}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Define Albert Hypermarket parsing method
def process_albert_hypermarket(price_str, price_type):
    # Clean string by keeping numbers and relevant separators
    clean_str = re.sub(r'[^0-9\s.,\'\-:]', '', price_str)  # Allow special chars like -, :, '

    # Handle specific cases for '-' or ':' as separators for integer prices
    combined_prices = []
    tokens = clean_str.split()

    for token in tokens:
        # Case 1: Numbers ending with ""-"" or "":""
        if token.endswith('-') or token.endswith(':'):
            token = token[:-1]  # Remove the trailing symbol
            combined_prices.append(parse_price(token))
        elif ""'"" in token:
            # Case 2: Handle cases like ""31'90""
            parts = token.split(""'"")
            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
                combined_price = f""{parts[0]}.{parts[1]}""
                combined_prices.append(parse_price(combined_price))
            else:
                combined_prices.append(parse_price(token))
        else:
            combined_prices.append(parse_price(token))

    # Filter out None values
    parsed_prices = [p for p in combined_prices if p is not None]

    # Condition: If the price is less than 5, treat it as invalid (exclude it)
    if parsed_prices and parsed_prices[0] < 5:
        return None

    # Assign prices based on the price_type
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Function to handle Tesco Supermarket OCR strings
def process_tesco_supermarket(price_str, price_type):
    # Handle dates (e.g., ""12.7. - 14.7."") by ignoring them
    date_pattern = r'\d{1,2}\.\d{1,2}\.\s*-\s*\d{1,2}\.\d{1,2}\.'  # Pattern for dates like ""12.7. - 14.7.""
    clean_str = re.sub(date_pattern, '', price_str)

    # Skip strings with percentages or irrelevant text
    if ""%"" in clean_str or ""HOP"" in clean_str:
        return None

    # Extract price values, specifically for club card or ""cena"" keyword
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Logic to differentiate between item prices and initial prices
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Lidl Condition
def process_lidl(price_str):
    return parse_price(price_str)


# Kaufland Condition
def process_kaufland(price_str, price_type):
    if re.search(r'(\d+[.,]\d+)\s+(\d+[.,]\d+)', price_str):
        return None  # Skip sequences of more than 2 prices

    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[-1], ""initial_price"": parsed_prices[0]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Flop Top Condition
def process_flop_top(price_str, price_type):
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Travel Free Condition
def process_travel_free(price_str, price_type):
    # Removing any € symbols to focus only on numeric data
    clean_str = price_str.replace(""€"", """").strip()

    # Find all the price values in the string
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Ensure prices are sorted correctly (sale price is less than initial price)
    if len(parsed_prices) == 2:
        sale_price = min(parsed_prices)
        initial_price = max(parsed_prices)
        return {""item_price"": sale_price, ""initial_price"": initial_price}

    # If we only have one price, return it as the item price
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# CBA Potraviny Condition
def process_cba_potraviny(price_str):
    return parse_price(price_str)


# Bene Condition
def process_bene(price_str):
    return parse_price(price_str)


# CBA Premium Condition
def process_cba_premium(price_str):
    return parse_price(price_str)


# Lidl Shop Condition
def process_lidl_shop(price_str):
    return parse_price(price_str)


# CBA Market Condition
def process_cba_market(price_str):
    return parse_price(price_str)


# Updated Makro Condition with improved packaging detection
def process_makro(price_str, price_type):
    # Extract packaging information (must be at the beginning of the string)
    packaging_pattern = re.match(r'^(\d+-?\d?\s*(BAL|ks|A VICE|AViCE))', price_str)

    # If packaging is found, extract it and continue processing the price
    packaging = None
    if packaging_pattern:
        packaging = packaging_pattern.group()  # Extract the packaging
        price_str = price_str[len(packaging):].strip()  # Remove packaging from the price string

    # Extract all numeric parts (prices) after the packaging
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Convert extracted prices to float
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If there are two prices, assign them as item_price and initial_price
    if len(parsed_prices) >= 2:
        return {
            ""item_price"": parsed_prices[0],
            ""initial_price"": parsed_prices[1],
            ""packaging"": packaging
        }
    elif len(parsed_prices) == 1:
        # If there's only one price, treat it as the item price
        return {
            ""item_price"": parsed_prices[0],
            ""packaging"": packaging
        }
    else:
        return None


# Function to process Ratio price strings
def process_ratio(price_str):
    # Extract prices ignoring ""bezDPH"" or ""vcetneDPH"" text
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If two prices are found, one should be item_price, the other initial_price
    if len(parsed_prices) == 2:
        return {""cena bez dph"": parsed_prices[0], ""item_price"": parsed_prices[1]}
    return None


# Function to process Globus price strings
def process_globus(price_str, price_type):
    # Skip percentage strings or invalid non-numeric inputs
    if ""%"" in price_str or re.search(r'[^\d.,\'\s-]', price_str):
        return None

    # Handle cases like ""14'90"" or ""44'90"" by replacing apostrophe with a decimal point
    price_str = price_str.replace(""'"", ""."")

    # Handle cases like ""17 90"" by joining them into a valid decimal format
    if re.search(r'\d+\s+\d{2}', price_str):
        price_str = price_str.replace("" "", ""."")

    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle item_price and item_member_price based on price_type
    if price_type == ""item_price"":
        # If one price is found, return it as the item price
        if len(parsed_prices) == 1:
            return {""item_price"": parsed_prices[0]}
    elif price_type == ""item_member_price"":
        # If member price is found, return it
        if len(parsed_prices) == 1:
            return {""item_member_price"": parsed_prices[0]}

    return None


# Function to process Tamda Foods price strings
def process_tamda_foods(price_str, price_type):
    # Skip percentage strings and invalid inputs
    if ""%"" in price_str or ""("" in price_str:
        return None

    # Handle cases like ""1290 KC"", ""3490Kc"", and ""5290KC"" (ignoring the ""KC"" part)
    price_str = re.sub(r'[KCkc]+', '', price_str).strip()

    # Extract numeric parts
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 1:
        if price_type == ""item_member_price"":
            return {""item_member_price"": parsed_prices[0]}
        elif price_type == ""item_price"":
            return {""item_price"": parsed_prices[0]}

    return None


# Function to process all types of prices based on class_id
def process_price_by_class_id(shop_name, got_ocr_text, class_id):
    processed_price = None

    # Check class_id for the type of price
    if class_id == ""item_price"":
        price_type = ""item_price""
    elif class_id == ""item_member_price"":
        price_type = ""item_member_price""
    elif class_id == ""item_initial_price"":
        price_type = ""item_initial_price""
    else:
        return None

    # Dispatch based on shop_name and price_type
    if shop_name == ""EsoMarket"":
        processed_price = process_esomarket(got_ocr_text)
    elif shop_name == ""Penny"":
        processed_price = process_penny(got_ocr_text, price_type)
    elif shop_name == ""Billa"":
        processed_price = process_billa(got_ocr_text, price_type)
    elif shop_name in [""Albert Hypermarket"", ""Albert Supermarket""]:
        processed_price = process_albert_hypermarket(got_ocr_text, price_type)
    elif shop_name in [""Tesco Supermarket"", ""Tesco Hypermarket""]:
        processed_price = process_tesco_supermarket(got_ocr_text, price_type)
    elif shop_name == ""Lidl"":
        processed_price = process_lidl(got_ocr_text)
    elif shop_name == ""Kaufland"":
        processed_price = process_kaufland(got_ocr_text, price_type)
    elif shop_name in [""Flop Top"", ""Flop""]:
        processed_price = process_flop_top(got_ocr_text, price_type)
    elif shop_name == ""Travel Free"":
        processed_price = process_travel_free(got_ocr_text, price_type)
    elif shop_name == ""CBA Potraviny"":
        processed_price = process_cba_potraviny(got_ocr_text)
    elif shop_name == ""Bene"":
        processed_price = process_bene(got_ocr_text)
    elif shop_name == ""CBA Premium"":
        processed_price = process_cba_premium(got_ocr_text)
    elif shop_name == ""Lidl Shop"":
        processed_price = process_lidl_shop(got_ocr_text)
    elif shop_name == ""CBA Market"":
        processed_price = process_cba_market(got_ocr_text)
    elif shop_name == ""Makro"":
        processed_price = process_makro(got_ocr_text, price_type)
    elif shop_name == ""Globus"":
        processed_price = process_globus(got_ocr_text, price_type)
    elif shop_name == ""Tamda Foods"":
        processed_price = process_tamda_foods(got_ocr_text, price_type)
    elif shop_name == ""Ratio"":
        processed_price = process_ratio(got_ocr_text)

    return processed_price
","        parsed_prices = [parse_price(p) for p in parsed_prices if parse_price(p) is not None]

        # Handle case where no previous price is found
        if parsed_prices and parsed_prices[0]!= parsed_prices[1]:
            return None

    # Restrict matching prices using the volume keyword and normalize by first digit
    volumes = re.findall(r""\d*[.,]?\d*"", price_str)

    # Normalize each volume separately
    volume_normalized = {}
    for volume in volumes:
        volume_normalized[volume] = int(volume)

    #"
../airflow/dags/utils/yolo_ocr_utils.py,"import json
import os
import tempfile

import cv2
import requests  # For sending HTTP requests to the deployed YOLO model
import logging
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, save_item_to_dynamodb

TEMP_DIR = ""/tmp""

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


def got_text_from_image(image_path):
    url = 'http://34.246.217.135:80/extract_text'  # Adjust the URL if necessary
    try:
        with open(image_path, 'rb') as image_file:
            files = {'image': image_file}
            response = requests.post(url, files=files)

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image: {response.status_code} - {response.text}"")
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


def got_text_from_image_box(image_path, box):
    url = 'http://34.246.217.135:80/extract_text_with_box'  # OCR API route
    try:
        logger.info(f""Sending bounding box to OCR API: {box}"")  # Log the bounding box

","        with open(image_path, 'rb') as image_file:
            # Prepare the multipart/form-data request
            files = {'image': image_file}
            # Send JSON data separately from form-data (image)
            json_data = {'box': box}

            response = requests.post(url, files=files, data={'json': json.dumps(json_data)})

        if response.status_code == 200:
            data = response.json()
","            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image_box: {response.status_code} - {response.text}"")
    except Exception as e:
        logger.error(f""Exception in got_text_from_image_box: {e}"")
        raise Exception(f""Exception in extract_text_from_image_box: {e}"")


def run_yolo_on_pages(s3_input_images_filepaths, dynamodb_table_name, model='model1',
                      save_images=False, detection_output_path=None, include_ocr=False, padding=0):
    """"""
    This function runs the YOLO model on a list of images from S3 or local paths, saves the detection details
    to DynamoDB, and returns the detection results as a dictionary. Optionally, it can also save the detected ROI images.

    Args:
        s3_input_images_filepaths (list): List of S3 paths of input image files.
        detection_output_path (str): S3 directory where the detection .txt files should be saved.
        dynamodb_table_name (str): The name of the DynamoDB table to store detection details.
        save_images (bool): Whether to save the detected ROIs as images.
        model (str): The model to be used ('model1' or 'model2').

    Returns:
        dict: A dictionary of predictions with image paths as keys and detections as values.
        list: A list of saved ROI image paths if save_images is True.
    """"""
    logger.info(f""Starting YOLO processing on {len(s3_input_images_filepaths)} images using model: {model}"")

    predictions = {}  # Dictionary to store detections for each image
    s3_saved_images = []  # List to store S3 paths of saved ROI images if save_images is True

    for filepath in s3_input_images_filepaths:
        try:
            logger.info(f""Processing image: {filepath}"")

            # Download the image from S3 to local TMP_DIR
            local_image_path = os.path.join(TEMP_DIR, os.path.basename(filepath))  # Corrected local path
            download_file_from_s3(filepath, local_image_path)
            logger.info(f""Downloaded image from S3 to {local_image_path}"")

            # Run the prediction and detection using the deployed YOLO model
            with open(local_image_path, 'rb') as image_file:
                response = requests.post(
                    f""http://34.246.217.135:80/predict"",  # YOLO model endpoint
                    files={'image': image_file},
                    params={'model': model}
                )

            if response.status_code == 200:
                detections = response.json().get('detections', [])
                logger.info(f""Received {len(detections)} detections for {filepath}"")
            else:
                raise Exception(f""Error from YOLO model: {response.status_code} - {response.text}"")

            img = cv2.imread(local_image_path)  # Load the image for ROI extraction (if needed)

            # Store detections for this image
            predictions[filepath] = []  # Initialize a list to store all detections for this image

            # Initialize a dictionary to store detections grouped by class
            detections_by_class = {}

            height, width = img.shape[:2]  # Get the image dimensions (height, width)

            for i, det in enumerate(detections):
                x1, y1, x2, y2 = det['box']  # Get bounding box coordinates
                class_name = det['class']  # Class name (e.g., 'shop_item')
                confidence = det['confidence']  # Confidence score for detection

                # Calculate width and height of the bounding box
                box_width = x2 - x1
                box_height = y2 - y1

                # Calculate 10% padding for width and height
                padding_w = int(box_width * 0.10)
                padding_h = int(box_height * 0.10)

                # Increase the bounding box by 10% padding on all sides, ensuring it stays within the image boundaries
                x1 = max(0, x1 - padding_w)
                y1 = max(0, y1 - padding_h)
                x2 = min(width, x2 + padding_w)
                y2 = min(height, y2 + padding_h)

                # Build the bounding box information, and add class_name to the detection item
                detection_item = {
                    'class_name': class_name,  # Add the class name here
                    'bounding_box': {
                        'x1': str(x1), 'y1': str(y1), 'x2': str(x2), 'y2': str(y2)
                    },
                    'confidence': str(confidence)
                }

                # Perform OCR if include_ocr is True
                if include_ocr:
                    # Prepare the bounding box for OCR
                    ocr_box = [x1, y1, x2, y2]  # Box format: [x1, y1, x2, y2]

                    # Step 1: Perform OCR directly on the bounding box area of the original image
                    object_text = got_text_from_image_box(local_image_path, ocr_box)

                    # Add OCR text to the detection item
                    detection_item['ocr_text'] = object_text
                    logger.info(f""OCR extracted text for class {class_name} in bounding box: {object_text}"")

                # Append detection item under the corresponding class_name
                if class_name not in detections_by_class:
                    detections_by_class[class_name] = []
                detections_by_class[class_name].append(detection_item)

                # Append the detection to the image's list of detections in the predictions dictionary
                predictions[filepath].append(detection_item)  # Now appending within the loop

            # After processing all detections, prepare the item for DynamoDB
            item_to_save = {
                'image_id': filepath,
                'detections': detections_by_class  # Grouped detections by class
            }

            # Save the detections to DynamoDB
            save_item_to_dynamodb(dynamodb_table_name, item_to_save)
            logger.info(f""Saved all detections for image {filepath} to DynamoDB"")

            # If save_images is True, extract ROI and save as PNG
            if save_images:
                for i, det in enumerate(detections):
                    x1, y1, x2, y2 = det['box']
                    class_name = det['class']
                    roi = img[y1:y2, x1:x2]  # Extract ROI from image
                    roi_filename = f""{os.path.basename(filepath).replace('.png', '')}_det_{i}_{class_name}.png""
                    roi_local_path = os.path.join(TEMP_DIR, roi_filename)

                    # Save the ROI image locally as PNG
                    cv2.imwrite(roi_local_path, roi)
                    logger.info(f""Saved ROI to {roi_local_path}"")

                    # Define the S3 path where the ROI will be uploaded
                    s3_roi_path = f""{detection_output_path}/images/{roi_filename}""

                    # Upload the ROI image to S3
                    upload_file_to_s3(roi_local_path, s3_roi_path)
                    s3_saved_images.append(s3_roi_path)
                    logger.info(f""Uploaded ROI to S3: {s3_roi_path}"")

                    # Clean up the temporary local ROI file after uploading
                    os.remove(roi_local_path)
                    logger.info(f""Deleted local ROI file: {roi_local_path}"")

        except Exception as e:
            logger.error(f""Error processing image {filepath}: {e}"")

    # Return the predictions dictionary and saved image paths if save_images is True
    return predictions, s3_saved_images if save_images else predictions","        elif response.status_code == 200:
            data = response.json()

        logger.info(f""Got text from ImageBox: {data.get('extracted_text')} - {data.get('detected_boxes'):>8d}"")
        return data.get('extracted_text', '')
    except Exception as e:
        logger.error(f""Exception in got_text_from_image_box: {e}"")
        raise Exception(f""Exception in got_text_from_image_box: {e}"")


def got_text_from_image"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/six.py,"# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""""""Utilities for writing code that runs on Python 2 and 3""""""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.16.0""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith(""java""):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):

            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """"""Add documentation to a function.""""""
    func.__doc__ = doc


def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):

    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = [""__doc__"", ""__name__""]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """"""
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """"""

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + ""."" + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + ""."" + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError(""This loader does not know module "" + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """"""
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """"""
        return hasattr(self.__get_module(fullname), ""__path__"")

    def get_code(self, fullname):
        """"""Return None

        Required, if is_package is implemented""""""
        self.__get_module(fullname)  # eventually raises ImportError
        return None
    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass

_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """"""Lazy loading of moved objects""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""cStringIO"", ""cStringIO"", ""io"", ""StringIO""),
    MovedAttribute(""filter"", ""itertools"", ""builtins"", ""ifilter"", ""filter""),
    MovedAttribute(""filterfalse"", ""itertools"", ""itertools"", ""ifilterfalse"", ""filterfalse""),
    MovedAttribute(""input"", ""__builtin__"", ""builtins"", ""raw_input"", ""input""),
    MovedAttribute(""intern"", ""__builtin__"", ""sys""),
    MovedAttribute(""map"", ""itertools"", ""builtins"", ""imap"", ""map""),
    MovedAttribute(""getcwd"", ""os"", ""os"", ""getcwdu"", ""getcwd""),
    MovedAttribute(""getcwdb"", ""os"", ""os"", ""getcwd"", ""getcwdb""),
    MovedAttribute(""getoutput"", ""commands"", ""subprocess""),
    MovedAttribute(""range"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""reload_module"", ""__builtin__"", ""importlib"" if PY34 else ""imp"", ""reload""),
    MovedAttribute(""reduce"", ""__builtin__"", ""functools""),
    MovedAttribute(""shlex_quote"", ""pipes"", ""shlex"", ""quote""),
    MovedAttribute(""StringIO"", ""StringIO"", ""io""),
    MovedAttribute(""UserDict"", ""UserDict"", ""collections""),
    MovedAttribute(""UserList"", ""UserList"", ""collections""),
    MovedAttribute(""UserString"", ""UserString"", ""collections""),
    MovedAttribute(""xrange"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""zip"", ""itertools"", ""builtins"", ""izip"", ""zip""),
    MovedAttribute(""zip_longest"", ""itertools"", ""itertools"", ""izip_longest"", ""zip_longest""),
    MovedModule(""builtins"", ""__builtin__""),
    MovedModule(""configparser"", ""ConfigParser""),
    MovedModule(""collections_abc"", ""collections"", ""collections.abc"" if sys.version_info >= (3, 3) else ""collections""),
    MovedModule(""copyreg"", ""copy_reg""),
    MovedModule(""dbm_gnu"", ""gdbm"", ""dbm.gnu""),
    MovedModule(""dbm_ndbm"", ""dbm"", ""dbm.ndbm""),
    MovedModule(""_dummy_thread"", ""dummy_thread"", ""_dummy_thread"" if sys.version_info < (3, 9) else ""_thread""),
    MovedModule(""http_cookiejar"", ""cookielib"", ""http.cookiejar""),
    MovedModule(""http_cookies"", ""Cookie"", ""http.cookies""),
    MovedModule(""html_entities"", ""htmlentitydefs"", ""html.entities""),
    MovedModule(""html_parser"", ""HTMLParser"", ""html.parser""),
    MovedModule(""http_client"", ""httplib"", ""http.client""),
    MovedModule(""email_mime_base"", ""email.MIMEBase"", ""email.mime.base""),
    MovedModule(""email_mime_image"", ""email.MIMEImage"", ""email.mime.image""),
    MovedModule(""email_mime_multipart"", ""email.MIMEMultipart"", ""email.mime.multipart""),
    MovedModule(""email_mime_nonmultipart"", ""email.MIMENonMultipart"", ""email.mime.nonmultipart""),
    MovedModule(""email_mime_text"", ""email.MIMEText"", ""email.mime.text""),
    MovedModule(""BaseHTTPServer"", ""BaseHTTPServer"", ""http.server""),
    MovedModule(""CGIHTTPServer"", ""CGIHTTPServer"", ""http.server""),
    MovedModule(""SimpleHTTPServer"", ""SimpleHTTPServer"", ""http.server""),
    MovedModule(""cPickle"", ""cPickle"", ""pickle""),
    MovedModule(""queue"", ""Queue""),
    MovedModule(""reprlib"", ""repr""),
    MovedModule(""socketserver"", ""SocketServer""),
    MovedModule(""_thread"", ""thread"", ""_thread""),
    MovedModule(""tkinter"", ""Tkinter""),
    MovedModule(""tkinter_dialog"", ""Dialog"", ""tkinter.dialog""),
    MovedModule(""tkinter_filedialog"", ""FileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_scrolledtext"", ""ScrolledText"", ""tkinter.scrolledtext""),
    MovedModule(""tkinter_simpledialog"", ""SimpleDialog"", ""tkinter.simpledialog""),
    MovedModule(""tkinter_tix"", ""Tix"", ""tkinter.tix""),
    MovedModule(""tkinter_ttk"", ""ttk"", ""tkinter.ttk""),
    MovedModule(""tkinter_constants"", ""Tkconstants"", ""tkinter.constants""),
    MovedModule(""tkinter_dnd"", ""Tkdnd"", ""tkinter.dnd""),
    MovedModule(""tkinter_colorchooser"", ""tkColorChooser"",
                ""tkinter.colorchooser""),
    MovedModule(""tkinter_commondialog"", ""tkCommonDialog"",
                ""tkinter.commondialog""),
    MovedModule(""tkinter_tkfiledialog"", ""tkFileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_font"", ""tkFont"", ""tkinter.font""),
    MovedModule(""tkinter_messagebox"", ""tkMessageBox"", ""tkinter.messagebox""),
    MovedModule(""tkinter_tksimpledialog"", ""tkSimpleDialog"",
                ""tkinter.simpledialog""),
    MovedModule(""urllib_parse"", __name__ + "".moves.urllib_parse"", ""urllib.parse""),
    MovedModule(""urllib_error"", __name__ + "".moves.urllib_error"", ""urllib.error""),
    MovedModule(""urllib"", __name__ + "".moves.urllib"", __name__ + "".moves.urllib""),
    MovedModule(""urllib_robotparser"", ""robotparser"", ""urllib.robotparser""),
    MovedModule(""xmlrpc_client"", ""xmlrpclib"", ""xmlrpc.client""),
    MovedModule(""xmlrpc_server"", ""SimpleXMLRPCServer"", ""xmlrpc.server""),
]
# Add windows specific modules.
if sys.platform == ""win32"":
    _moved_attributes += [
        MovedModule(""winreg"", ""_winreg""),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
    if isinstance(attr, MovedModule):
        _importer._add_module(attr, ""moves."" + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + "".moves"")
_importer._add_module(moves, ""moves"")


class Module_six_moves_urllib_parse(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""SplitResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qs"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qsl"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urldefrag"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urljoin"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""quote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""quote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_to_bytes"", ""urllib"", ""urllib.parse"", ""unquote"", ""unquote_to_bytes""),
    MovedAttribute(""urlencode"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitquery"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splittag"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splituser"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitvalue"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""uses_fragment"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_netloc"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_params"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_query"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_relative"", ""urlparse"", ""urllib.parse""),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(Module_six_moves_urllib_parse(__name__ + "".moves.urllib_parse""),
                      ""moves.urllib_parse"", ""moves.urllib.parse"")


class Module_six_moves_urllib_error(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_error""""""


_urllib_error_moved_attributes = [
    MovedAttribute(""URLError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""HTTPError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""ContentTooShortError"", ""urllib"", ""urllib.error""),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(Module_six_moves_urllib_error(__name__ + "".moves.urllib.error""),
                      ""moves.urllib_error"", ""moves.urllib.error"")


class Module_six_moves_urllib_request(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_request""""""


_urllib_request_moved_attributes = [
    MovedAttribute(""urlopen"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""install_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""build_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""pathname2url"", ""urllib"", ""urllib.request""),
    MovedAttribute(""url2pathname"", ""urllib"", ""urllib.request""),
    MovedAttribute(""getproxies"", ""urllib"", ""urllib.request""),
    MovedAttribute(""Request"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""OpenerDirector"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDefaultErrorHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPRedirectHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPCookieProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""BaseHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgr"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgrWithDefaultRealm"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPSHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FileHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""CacheFTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""UnknownHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPErrorProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""urlretrieve"", ""urllib"", ""urllib.request""),
    MovedAttribute(""urlcleanup"", ""urllib"", ""urllib.request""),
    MovedAttribute(""URLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""FancyURLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""proxy_bypass"", ""urllib"", ""urllib.request""),
    MovedAttribute(""parse_http_list"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""parse_keqv_list"", ""urllib2"", ""urllib.request""),
]
","for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
","del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(Module_six_moves_urllib_request(__name__ + "".moves.urllib.request""),
                      ""moves.urllib_request"", ""moves.urllib.request"")


class Module_six_moves_urllib_response(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_response""""""


_urllib_response_moved_attributes = [
    MovedAttribute(""addbase"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addclosehook"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfo"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfourl"", ""urllib"", ""urllib.response""),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(Module_six_moves_urllib_response(__name__ + "".moves.urllib.response""),
                      ""moves.urllib_response"", ""moves.urllib.response"")


class Module_six_moves_urllib_robotparser(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_robotparser""""""


_urllib_robotparser_moved_attributes = [
    MovedAttribute(""RobotFileParser"", ""robotparser"", ""urllib.robotparser""),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes

_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + "".moves.urllib.robotparser""),
                      ""moves.urllib_robotparser"", ""moves.urllib.robotparser"")


class Module_six_moves_urllib(types.ModuleType):

    """"""Create a six.moves.urllib namespace that resembles the Python 3 namespace""""""
    __path__ = []  # mark as package
    parse = _importer._get_module(""moves.urllib_parse"")
    error = _importer._get_module(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
    response = _importer._get_module(""moves.urllib_response"")
    robotparser = _importer._get_module(""moves.urllib_robotparser"")

    def __dir__(self):
        return ['parse', 'error', 'request', 'response', 'robotparser']

_importer._add_module(Module_six_moves_urllib(__name__ + "".moves.urllib""),
                      ""moves.urllib"")


def add_move(move):
    """"""Add an item to six.moves.""""""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """"""Remove item from six.moves.""""""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError(""no such move, %r"" % (name,))


if PY3:
    _meth_func = ""__func__""
    _meth_self = ""__self__""

    _func_closure = ""__closure__""
    _func_code = ""__code__""
    _func_defaults = ""__defaults__""
    _func_globals = ""__globals__""
else:
    _meth_func = ""im_func""
    _meth_self = ""im_self""

    _func_closure = ""func_closure""
    _func_code = ""func_code""
    _func_defaults = ""func_defaults""
    _func_globals = ""func_globals""


try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any(""__call__"" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """"""Get the function out of a possibly unbound function"""""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:
    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller(""keys"")

    viewvalues = operator.methodcaller(""values"")

    viewitems = operator.methodcaller(""items"")
else:
    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller(""viewkeys"")

    viewvalues = operator.methodcaller(""viewvalues"")

    viewitems = operator.methodcaller(""viewitems"")

_add_doc(iterkeys, ""Return an iterator over the keys of a dictionary."")
_add_doc(itervalues, ""Return an iterator over the values of a dictionary."")
_add_doc(iteritems,
         ""Return an iterator over the (key, value) pairs of a dictionary."")
_add_doc(iterlists,
         ""Return an iterator over the (key, [values]) pairs of a dictionary."")


if PY3:
    def b(s):
        return s.encode(""latin-1"")

    def u(s):
        return s
    unichr = chr
    import struct
    int2byte = struct.Struct("">B"").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = ""assertCountEqual""
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = ""assertRaisesRegexp""
        _assertRegex = ""assertRegexpMatches""
        _assertNotRegex = ""assertNotRegexpMatches""
    else:
        _assertRaisesRegex = ""assertRaisesRegex""
        _assertRegex = ""assertRegex""
        _assertNotRegex = ""assertNotRegex""
else:
    def b(s):
        return s
    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r'\\', r'\\\\'), ""unicode_escape"")
    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])
    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = ""assertItemsEqual""
    _assertRaisesRegex = ""assertRaisesRegexp""
    _assertRegex = ""assertRegexpMatches""
    _assertNotRegex = ""assertNotRegexpMatches""
_add_doc(b, """"""Byte literal"""""")
_add_doc(u, """"""Text literal"""""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, ""exec"")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """"""Execute code in a namespace.""""""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(""""""exec _code_ in _globs_, _locs_"""""")

    exec_(""""""def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""""")


if sys.version_info[:2] > (3,):
    exec_(""""""def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""""")
else:
    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, ""print"", None)
if print_ is None:
    def print_(*args, **kwargs):
        """"""The new-style print function for Python 2.4 and 2.5.""""""
        fp = kwargs.pop(""file"", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError(""sep must be None or a string"")
        end = kwargs.pop(""end"", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError(""end must be None or a string"")
        if kwargs:
            raise TypeError(""invalid keyword arguments to print()"")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode(""\n"")
            space = unicode("" "")
        else:
            newline = ""\n""
            space = "" ""
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get(""file"", sys.stdout)
        flush = kwargs.pop(""flush"", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()

_add_doc(reraise, """"""Reraise an exception."""""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(wrapper, wrapped,
                        assigned=functools.WRAPPER_ASSIGNMENTS,
                        updated=functools.WRAPPER_UPDATES):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper
    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
              updated=functools.WRAPPER_UPDATES):
        return functools.partial(_update_wrapper, wrapped=wrapped,
                                 assigned=assigned, updated=updated)
    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):

        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d['__orig_bases__'] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})


def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get('__slots__')
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        if hasattr(cls, '__qualname__'):
            orig_vars['__qualname__'] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper


def ensure_binary(s, encoding='utf-8', errors='strict'):
    """"""Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """"""
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError(""not expecting type '%s'"" % type(s))


def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    return s


def ensure_text(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError(""not expecting type '%s'"" % type(s))


def python_2_unicode_compatible(klass):
    """"""
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """"""
    if PY2:
        if '__str__' not in klass.__dict__:
            raise ValueError(""@python_2_unicode_compatible cannot be applied ""
                             ""to %s because it doesn't define __str__()."" %
                             klass.__name__)
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get(""__spec__"") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another ""instance"" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (type(importer).__name__ == ""_SixMetaPathImporter"" and
                importer.name == __name__):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)
","args, **kwargs)


if PY3:
    def getsetdefault(obj, default=None):
        return set(getattr(obj, _setdefault_attribute),
                   frozenset(getattr(obj, _unsetdefault_attribute)))

    def getsetnotdefault(obj, default=None):
        return set(getattr(obj, _setnotdefault_attribute),
                   getattr(obj, _unsetnotdefault_attribute))


if PY3:
    def getdelete(obj, key, default=False):
        return getattr(obj, _delelete_attribute)[key] is not None"
../airflow/dags/utils/s3_dynamodb_utils.py,"import boto3

# Initialize AWS S3 and DynamoDB clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Retrieve an item from the DynamoDB table based on filename and shop_name
def get_pdf_item_from_dynamodb(filename, shop_name, table=None, table_name=""pdf_metadata""):
    """"""
    Retrieve an item from a specified DynamoDB table based on filename and shop_name.
    """"""
    # Use the provided table instance if given; otherwise, access by table_name
    table = table or dynamodb.Table(table_name)
    return table.get_item(Key={'filename': filename, 'shop_name': shop_name})

# Function to download a file from an S3 bucket to a local path
def download_file_from_s3(filename_path, local_path, bucket_name=""salestelegrambot""):
    """"""
    Download a file from an S3 bucket.
    """"""
    try:
        s3.download_file(bucket_name, filename_path, local_path)
        print(f""Downloaded {filename_path} to {local_path}"")
    except Exception as e:
        print(f""Error downloading {filename_path}: {e}"")

# Function to upload a file from a local path to an S3 bucket
def upload_file_to_s3(local_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Upload a file to an S3 bucket.
    """"""
    try:
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/{s3_path}"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")

# Function to save an item to a DynamoDB table
def save_item_to_dynamodb(table_name, item, table=None):
    """"""
    Save an item to a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.put_item(Item=item)
        print(f""Saved item {item} to DynamoDB table {table_name}"")
    except Exception as e:
        print(f""Error saving item to DynamoDB: {e}"")

# Function to update specific fields of an item in DynamoDB
def update_item_in_dynamodb(table_name, key, update_expression, expression_attribute_values, table=None):
    """"""
    Update specific fields of an item in a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.update_item(
            Key=key,
            UpdateExpression=update_expression,
            ExpressionAttributeValues=expression_attribute_values
        )
","        print(f""Updated item in table {table_name} with key {key}"")
    except Exception as e:
","        print(f""Error updating item in DynamoDB: {e}"")
","    except Exception as e:
"
../airflow/dags/utils/correct_names.py,"import re
import hunspell
import itertools

# Mapping Czech characters to their English equivalents
czech_to_english_map = str.maketrans(
    ""áčçďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""accdeeinorstuuyzACDEEINORSTUUYZ""
)

# Preprocess text by removing tabs, newlines, and non-ASCII characters,
# converting to lowercase, and replacing Czech characters with English equivalents.
def preprocess_text(text):
    text = text.replace('\t', '').replace('\n', '').replace('\u00A0', ' ').replace(""|"", """")
    text = text.lower()
    text = text.translate(czech_to_english_map)  # Apply Czech to English conversion
    text = re.sub(r'[^\x00-\x7F]', ' ', text)  # Replace non-ASCII characters with space
    cleaned_text = ' '.join(text.split())
    return cleaned_text

# Initialize Hunspell with the Czech dictionary
hunspell_checker = hunspell.HunSpell('/usr/share/hunspell/cs_CZ.dic', '/usr/share/hunspell/cs_CZ.aff')

# Generate all possible variants of a word by replacing 'i', 'l', and '1' with each other.
def generate_il1rjeo_combinations(word):
    substitutions = {
        'i': ['i', 'l', '1'],
        'l': ['i', 'l', '1'],
        '1': ['i', 'l', '1'],
        'r': ['r', 'j'],
        'j': ['r', 'j'],
        'e': ['e', 'o'],
        'o': ['e', 'o'],
    }
    # Find positions of characters in the word that can be substituted
    positions = [i for i, char in enumerate(word) if char in substitutions]

    if not positions:
        return [word]

    # Generate all combinations by substituting at the found positions
    variants = []
    for variant in itertools.product(*[substitutions[word[pos]] for pos in positions]):
        modified_word = list(word)
        for idx, pos in enumerate(positions):
            modified_word[pos] = variant[idx]
        variants.append(''.join(modified_word))

    return variants

# Trie data structure to efficiently store and search words
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    # Insert all variants of a word into the Trie
    def insert(self, word):
        variants = generate_il1rjeo_combinations(word)
        for variant in variants:
            node = self.root
            for char in variant:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.is_word = True

    # Search for a word in the Trie
    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.is_word

    # Find all valid words in a given text using the Trie
    def find_all_words(self, text):
        """"""
        Finds all valid word candidates using the trie for the given text.
        Returns a list of tuples (word, start, end), where start and end are indices in the text.
        """"""
        words = []
","        for start in range(len(text)):
            node = self.root
            for end in range(start, len(text)):
","                char = text[end]
                if char not in node.children:
                    break
                node = node.children[char]
                if node.is_word:
                    words.append((text[start:end + 1], start, end + 1))
        return words

# Penalize small words to avoid splitting text into short, meaningless words
def calculate_penalty(word):
    if len(word) <= 3:
        return -10  # Penalize very small words
    return len(word)  # Reward longer words

# Dynamic programming function to find the best word combination based on penalties
def best_word_combination(words, text_length):
    dp = [(-float('inf'), [])] * (text_length + 1)
    dp[0] = (0, [])

    for word, start, end in words:
        score = calculate_penalty(word)
        if dp[start][0] + score > dp[end][0]:
            dp[end] = (dp[start][0] + score, dp[start][1] + [word])

    return dp[text_length][1]

# Main function to process a single word by finding valid word combinations
def process_single_word(word, trie):
    # Preprocess the word by removing spaces and converting to lowercase
    concatenated_text = """".join(preprocess_text(word).split())

    # Use the Trie to find all possible valid words in the preprocessed text
    found_words = trie.find_all_words(concatenated_text)

    # Use dynamic programming to find the best combination of words
    best_split = best_word_combination(found_words, len(concatenated_text))

    # Check words in the split against Hunspell dictionary for suggestions if not found in the Trie
    final_processed_words = []
    for word in best_split:
        if not trie.search(word):
            if hunspell_checker.spell(word):
                final_processed_words.append(word)
            else:
                suggestions = hunspell_checker.suggest(word)
                if suggestions:
                    final_processed_words.append(suggestions[0])
                else:
                    final_processed_words.append(word)
        else:
            final_processed_words.append(word)

    # Return the final processed word as a string
    return "" "".join(final_processed_words)
","        start = 0
        end = len(text)

        while start < end:
            # We'll use the recursion limit here until we hit our target index position
            # If we're not out of bounds, then there's no solution so far
            if start >= len(text):
                break

            # If we've already reached the target index, remove it from the text and add its children
            if start == end or text[start]!= '|':
            #if start > 0 and text[start - 1] == '-' and end - 1 == start:
            #    print(""Found target %"
../airflow/dags/utils/price_processing.py,"import re


# Helper function to parse price strings into floats
def parse_price(price_str):
    # Remove non-numeric characters except for decimal points or commas
    clean_str = re.sub(r'[^0-9.,]', '', price_str)
    clean_str = clean_str.replace(',', '.').replace(""'"", '.')

    try:
        # If it contains a decimal, treat it as a float
        if '.' in clean_str:
            return float(clean_str)
        # Otherwise, treat the last two digits as the decimal part
        elif len(clean_str) > 2:
            return float(clean_str[:-2] + '.' + clean_str[-2:])
        else:
            return float(clean_str)
    except ValueError:
        return None


# EsoMarket Condition
def process_esomarket(price_str):
    price = parse_price(price_str)
    return price if price else None


def process_penny(price_str, price_type):
    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Clean up extracted prices and convert them to floats
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Common cents values like 90 or 99
    common_cents = [90, 99]

    # Handle cases based on the length of parsed prices
    if len(parsed_prices) == 3:
        # Handle cases like ""19 90 25.90 2""
        item_price = float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")
        initial_price = parsed_prices[2]
        return {""item_price"": item_price, ""initial_price"": initial_price}

    if len(parsed_prices) == 2:
        # If the second price is commonly a ""cents"" part like 90 or 99, merge with the first
        if parsed_prices[1] in common_cents:
            return {""item_price"": float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}

    if len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Billa Condition
def process_billa(price_str, price_type):
    # Detect volume keywords: pri koupi, kupte, etc.
    volume_keywords = ['pri', 'koupi', 'kupte', 'ks', 'bodi', 'bodu', 'up te', 'aza']
    volume_detected = any(keyword in price_str.lower() for keyword in volume_keywords)

    # Extract numeric parts from the string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle specific distracted membership or volume words
    if 'bodi' in price_str.lower() or 'bodu' in price_str.lower():
        return {'item_member_price': '75bodi'}

    # Check if there are two prices and handle them
    if len(parsed_prices) == 2:
        # If the second value is an integer <5, treat it as volume, not initial_price
        if parsed_prices[1] < 5 and parsed_prices[1].is_integer():
            return {""item_price"": parsed_prices[0], ""volume"": str(int(parsed_prices[1]))}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Define Albert Hypermarket parsing method
def process_albert_hypermarket(price_str, price_type):
    # Clean string by keeping numbers and relevant separators
    clean_str = re.sub(r'[^0-9\s.,\'\-:]', '', price_str)  # Allow special chars like -, :, '

    # Handle specific cases for '-' or ':' as separators for integer prices
    combined_prices = []
    tokens = clean_str.split()

    for token in tokens:
        # Case 1: Numbers ending with ""-"" or "":""
        if token.endswith('-') or token.endswith(':'):
            token = token[:-1]  # Remove the trailing symbol
            combined_prices.append(parse_price(token))
        elif ""'"" in token:
            # Case 2: Handle cases like ""31'90""
            parts = token.split(""'"")
            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
                combined_price = f""{parts[0]}.{parts[1]}""
                combined_prices.append(parse_price(combined_price))
            else:
                combined_prices.append(parse_price(token))
        else:
            combined_prices.append(parse_price(token))

    # Filter out None values
    parsed_prices = [p for p in combined_prices if p is not None]

    # Condition: If the price is less than 5, treat it as invalid (exclude it)
    if parsed_prices and parsed_prices[0] < 5:
        return None

    # Assign prices based on the price_type
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Function to handle Tesco Supermarket OCR strings
def process_tesco_supermarket(price_str, price_type):
    # Handle dates (e.g., ""12.7. - 14.7."") by ignoring them
    date_pattern = r'\d{1,2}\.\d{1,2}\.\s*-\s*\d{1,2}\.\d{1,2}\.'  # Pattern for dates like ""12.7. - 14.7.""
    clean_str = re.sub(date_pattern, '', price_str)

    # Skip strings with percentages or irrelevant text
    if ""%"" in clean_str or ""HOP"" in clean_str:
        return None

    # Extract price values, specifically for club card or ""cena"" keyword
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Logic to differentiate between item prices and initial prices
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Lidl Condition
def process_lidl(price_str):
    return parse_price(price_str)


# Kaufland Condition
def process_kaufland(price_str, price_type):
    if re.search(r'(\d+[.,]\d+)\s+(\d+[.,]\d+)', price_str):
        return None  # Skip sequences of more than 2 prices

    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[-1], ""initial_price"": parsed_prices[0]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Flop Top Condition
def process_flop_top(price_str, price_type):
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Travel Free Condition
def process_travel_free(price_str, price_type):
    # Removing any € symbols to focus only on numeric data
    clean_str = price_str.replace(""€"", """").strip()

    # Find all the price values in the string
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Ensure prices are sorted correctly (sale price is less than initial price)
    if len(parsed_prices) == 2:
        sale_price = min(parsed_prices)
        initial_price = max(parsed_prices)
        return {""item_price"": sale_price, ""initial_price"": initial_price}

    # If we only have one price, return it as the item price
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# CBA Potraviny Condition
def process_cba_potraviny(price_str):
    return parse_price(price_str)


# Bene Condition
def process_bene(price_str):
    return parse_price(price_str)


# CBA Premium Condition
def process_cba_premium(price_str):
    return parse_price(price_str)


# Lidl Shop Condition
def process_lidl_shop(price_str):
    return parse_price(price_str)


# CBA Market Condition
def process_cba_market(price_str):
    return parse_price(price_str)


# Updated Makro Condition with improved packaging detection
def process_makro(price_str, price_type):
    # Extract packaging information (must be at the beginning of the string)
    packaging_pattern = re.match(r'^(\d+-?\d?\s*(BAL|ks|A VICE|AViCE))', price_str)

    # If packaging is found, extract it and continue processing the price
    packaging = None
    if packaging_pattern:
        packaging = packaging_pattern.group()  # Extract the packaging
        price_str = price_str[len(packaging):].strip()  # Remove packaging from the price string

    # Extract all numeric parts (prices) after the packaging
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Convert extracted prices to float
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If there are two prices, assign them as item_price and initial_price
    if len(parsed_prices) >= 2:
        return {
            ""item_price"": parsed_prices[0],
            ""initial_price"": parsed_prices[1],
            ""packaging"": packaging
        }
    elif len(parsed_prices) == 1:
        # If there's only one price, treat it as the item price
        return {
            ""item_price"": parsed_prices[0],
            ""packaging"": packaging
        }
    else:
        return None


# Function to process Ratio price strings
def process_ratio(price_str):
    # Extract prices ignoring ""bezDPH"" or ""vcetneDPH"" text
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If two prices are found, one should be item_price, the other initial_price
    if len(parsed_prices) == 2:
        return {""cena bez dph"": parsed_prices[0], ""item_price"": parsed_prices[1]}
    return None


# Function to process Globus price strings
def process_globus(price_str, price_type):
    # Skip percentage strings or invalid non-numeric inputs
    if ""%"" in price_str or re.search(r'[^\d.,\'\s-]', price_str):
        return None

    # Handle cases like ""14'90"" or ""44'90"" by replacing apostrophe with a decimal point
    price_str = price_str.replace(""'"", ""."")

    # Handle cases like ""17 90"" by joining them into a valid decimal format
    if re.search(r'\d+\s+\d{2}', price_str):
        price_str = price_str.replace("" "", ""."")

    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle item_price and item_member_price based on price_type
    if price_type == ""item_price"":
        # If one price is found, return it as the item price
        if len(parsed_prices) == 1:
            return {""item_price"": parsed_prices[0]}
    elif price_type == ""item_member_price"":
        # If member price is found, return it
        if len(parsed_prices) == 1:
            return {""item_member_price"": parsed_prices[0]}

    return None


# Function to process Tamda Foods price strings
def process_tamda_foods(price_str, price_type):
    # Skip percentage strings and invalid inputs
    if ""%"" in price_str or ""("" in price_str:
        return None

    # Handle cases like ""1290 KC"", ""3490Kc"", and ""5290KC"" (ignoring the ""KC"" part)
    price_str = re.sub(r'[KCkc]+', '', price_str).strip()

    # Extract numeric parts
","    prices = re.findall(r'\d+[.,]?\d*', price_str)
","    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 1:
        if price_type == ""item_member_price"":
            return {""item_member_price"": parsed_prices[0]}
        elif price_type == ""item_price"":
            return {""item_price"": parsed_prices[0]}

    return None


# Function to process all types of prices based on class_id
def process_price_by_class_id(shop_name, got_ocr_text, class_id):
    processed_price = None

    # Check class_id for the type of price
    if class_id == ""item_price"":
        price_type = ""item_price""
    elif class_id == ""item_member_price"":
        price_type = ""item_member_price""
    elif class_id == ""item_initial_price"":
        price_type = ""item_initial_price""
    else:
        return None

    # Dispatch based on shop_name and price_type
    if shop_name == ""EsoMarket"":
        processed_price = process_esomarket(got_ocr_text)
    elif shop_name == ""Penny"":
        processed_price = process_penny(got_ocr_text, price_type)
    elif shop_name == ""Billa"":
        processed_price = process_billa(got_ocr_text, price_type)
    elif shop_name in [""Albert Hypermarket"", ""Albert Supermarket""]:
        processed_price = process_albert_hypermarket(got_ocr_text, price_type)
    elif shop_name in [""Tesco Supermarket"", ""Tesco Hypermarket""]:
        processed_price = process_tesco_supermarket(got_ocr_text, price_type)
    elif shop_name == ""Lidl"":
        processed_price = process_lidl(got_ocr_text)
    elif shop_name == ""Kaufland"":
        processed_price = process_kaufland(got_ocr_text, price_type)
    elif shop_name in [""Flop Top"", ""Flop""]:
        processed_price = process_flop_top(got_ocr_text, price_type)
    elif shop_name == ""Travel Free"":
        processed_price = process_travel_free(got_ocr_text, price_type)
    elif shop_name == ""CBA Potraviny"":
        processed_price = process_cba_potraviny(got_ocr_text)
    elif shop_name == ""Bene"":
        processed_price = process_bene(got_ocr_text)
    elif shop_name == ""CBA Premium"":
        processed_price = process_cba_premium(got_ocr_text)
    elif shop_name == ""Lidl Shop"":
        processed_price = process_lidl_shop(got_ocr_text)
    elif shop_name == ""CBA Market"":
        processed_price = process_cba_market(got_ocr_text)
    elif shop_name == ""Makro"":
        processed_price = process_makro(got_ocr_text, price_type)
    elif shop_name == ""Globus"":
        processed_price = process_globus(got_ocr_text, price_type)
    elif shop_name == ""Tamda Foods"":
        processed_price = process_tamda_foods(got_ocr_text, price_type)
    elif shop_name == ""Ratio"":
        processed_price = process_ratio(got_ocr_text)

    return processed_price
","    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle cena and item_member_price based on price_type
"
../airflow/dags/utils/correct_names.py,"import re
import hunspell
import itertools

# Mapping Czech characters to their English equivalents
czech_to_english_map = str.maketrans(
    ""áčçďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""accdeeinorstuuyzACDEEINORSTUUYZ""
)

# Preprocess text by removing tabs, newlines, and non-ASCII characters,
# converting to lowercase, and replacing Czech characters with English equivalents.
def preprocess_text(text):
    text = text.replace('\t', '').replace('\n', '').replace('\u00A0', ' ').replace(""|"", """")
    text = text.lower()
    text = text.translate(czech_to_english_map)  # Apply Czech to English conversion
    text = re.sub(r'[^\x00-\x7F]', ' ', text)  # Replace non-ASCII characters with space
    cleaned_text = ' '.join(text.split())
    return cleaned_text

# Initialize Hunspell with the Czech dictionary
hunspell_checker = hunspell.HunSpell('/usr/share/hunspell/cs_CZ.dic', '/usr/share/hunspell/cs_CZ.aff')

# Generate all possible variants of a word by replacing 'i', 'l', and '1' with each other.
def generate_il1rjeo_combinations(word):
    substitutions = {
        'i': ['i', 'l', '1'],
        'l': ['i', 'l', '1'],
        '1': ['i', 'l', '1'],
        'r': ['r', 'j'],
        'j': ['r', 'j'],
        'e': ['e', 'o'],
        'o': ['e', 'o'],
    }
    # Find positions of characters in the word that can be substituted
    positions = [i for i, char in enumerate(word) if char in substitutions]

    if not positions:
        return [word]

    # Generate all combinations by substituting at the found positions
    variants = []
    for variant in itertools.product(*[substitutions[word[pos]] for pos in positions]):
        modified_word = list(word)
        for idx, pos in enumerate(positions):
            modified_word[pos] = variant[idx]
","        variants.append(''.join(modified_word))

    return variants

# Trie data structure to efficiently store and search words
","class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    # Insert all variants of a word into the Trie
    def insert(self, word):
        variants = generate_il1rjeo_combinations(word)
        for variant in variants:
            node = self.root
            for char in variant:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.is_word = True

    # Search for a word in the Trie
    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.is_word

    # Find all valid words in a given text using the Trie
    def find_all_words(self, text):
        """"""
        Finds all valid word candidates using the trie for the given text.
        Returns a list of tuples (word, start, end), where start and end are indices in the text.
        """"""
        words = []
        for start in range(len(text)):
            node = self.root
            for end in range(start, len(text)):
                char = text[end]
                if char not in node.children:
                    break
                node = node.children[char]
                if node.is_word:
                    words.append((text[start:end + 1], start, end + 1))
        return words

# Penalize small words to avoid splitting text into short, meaningless words
def calculate_penalty(word):
    if len(word) <= 3:
        return -10  # Penalize very small words
    return len(word)  # Reward longer words

# Dynamic programming function to find the best word combination based on penalties
def best_word_combination(words, text_length):
    dp = [(-float('inf'), [])] * (text_length + 1)
    dp[0] = (0, [])

    for word, start, end in words:
        score = calculate_penalty(word)
        if dp[start][0] + score > dp[end][0]:
            dp[end] = (dp[start][0] + score, dp[start][1] + [word])

    return dp[text_length][1]

# Main function to process a single word by finding valid word combinations
def process_single_word(word, trie):
    # Preprocess the word by removing spaces and converting to lowercase
    concatenated_text = """".join(preprocess_text(word).split())

    # Use the Trie to find all possible valid words in the preprocessed text
    found_words = trie.find_all_words(concatenated_text)

    # Use dynamic programming to find the best combination of words
    best_split = best_word_combination(found_words, len(concatenated_text))

    # Check words in the split against Hunspell dictionary for suggestions if not found in the Trie
    final_processed_words = []
    for word in best_split:
        if not trie.search(word):
            if hunspell_checker.spell(word):
                final_processed_words.append(word)
            else:
                suggestions = hunspell_checker.suggest(word)
                if suggestions:
                    final_processed_words.append(suggestions[0])
                else:
                    final_processed_words.append(word)
        else:
            final_processed_words.append(word)

    # Return the final processed word as a string
    return "" "".join(final_processed_words)
","        for position in positions:
            modified_word[position] = substitutions[modified_word[position]]

        variants.append(modified_word)
    return variants

"
../airflow/dags/utils/s3_dynamodb_utils.py,"import boto3

# Initialize AWS S3 and DynamoDB clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Retrieve an item from the DynamoDB table based on filename and shop_name
def get_pdf_item_from_dynamodb(filename, shop_name, table=None, table_name=""pdf_metadata""):
    """"""
    Retrieve an item from a specified DynamoDB table based on filename and shop_name.
    """"""
    # Use the provided table instance if given; otherwise, access by table_name
    table = table or dynamodb.Table(table_name)
    return table.get_item(Key={'filename': filename, 'shop_name': shop_name})

# Function to download a file from an S3 bucket to a local path
def download_file_from_s3(filename_path, local_path, bucket_name=""salestelegrambot""):
    """"""
    Download a file from an S3 bucket.
    """"""
    try:
        s3.download_file(bucket_name, filename_path, local_path)
        print(f""Downloaded {filename_path} to {local_path}"")
    except Exception as e:
        print(f""Error downloading {filename_path}: {e}"")

# Function to upload a file from a local path to an S3 bucket
def upload_file_to_s3(local_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Upload a file to an S3 bucket.
    """"""
","    try:
","        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/{s3_path}"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")

# Function to save an item to a DynamoDB table
def save_item_to_dynamodb(table_name, item, table=None):
    """"""
    Save an item to a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.put_item(Item=item)
        print(f""Saved item {item} to DynamoDB table {table_name}"")
    except Exception as e:
        print(f""Error saving item to DynamoDB: {e}"")

# Function to update specific fields of an item in DynamoDB
def update_item_in_dynamodb(table_name, key, update_expression, expression_attribute_values, table=None):
    """"""
    Update specific fields of an item in a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.update_item(
            Key=key,
            UpdateExpression=update_expression,
            ExpressionAttributeValues=expression_attribute_values
        )
        print(f""Updated item in table {table_name} with key {key}"")
    except Exception as e:
        print(f""Error updating item in DynamoDB: {e}"")
","    try:
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")


# Function to delete a file from an S3 bucket
def delete_file_from_s3(filename_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Delete a file from an S3 bucket.
    """"""
    try:
"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/lambda_function.py,"import ast
import copy
import re
import tempfile
import boto3
import requests
import json
import os
from boto3.dynamodb.conditions import Attr
import logging
from fuzzywuzzy import fuzz
from itertools import islice

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger()


# Function to initialize global constants and resources
def initialize_globals():
    global BUCKET_NAME, TOKEN, API_URL, dynamodb, s3, user_preferences_table, pdf_metadata_table, detected_data_table

    BUCKET_NAME = os.environ.get('BUCKET_NAME')
    TOKEN = os.environ.get('TOKEN')
    API_URL = f""https://api.telegram.org/bot{TOKEN}""

    # Initialize AWS resources
    dynamodb = boto3.resource('dynamodb')
    s3 = boto3.client('s3')

    # DynamoDB table references
","    user_preferences_table = dynamodb.Table('user_preferences')
    pdf_metadata_table = dynamodb.Table('pdf_metadata')
    detected_data_table = dynamodb.Table(""detected_data"")


# Call the initialization function
initialize_globals()


# --------------- AWS Handling ---------------
","def download_file_from_s3(filename_path, local_path):
    """"""
    Downloads a file from the specified S3 bucket to a local file path.

    :param filename_path: The path of the file in the S3 bucket.
    :param local_path: The local path where the file should be saved.

    :raises ValueError: If the provided filename_path is not a valid string.
    """"""
    # Ensure the S3 file path is a valid string before proceeding with the download
    if not isinstance(filename_path, str) or not filename_path:
        raise ValueError(f""Invalid S3 filename path: {filename_path}"")

    # Log the download operation for debugging purposes
    logger.debug(f""Downloading file from S3 bucket: {filename_path} to local path: {local_path}"")

    # Perform the file download from the specified S3 bucket to the local path
    s3.download_file(BUCKET_NAME, filename_path, local_path)


# --------------- User Preferences Handling ---------------

def get_user_preferences(chat_id):
    """"""
    Retrieves user preferences from DynamoDB for the given chat_id.
    If no preferences are found, it returns a default preference with 'new_user' state.
    """"""
    response = user_preferences_table.get_item(Key={'chat_id': str(chat_id)})
    return response.get('Item', {""state"": ""new_user""})


def save_user_preferences(chat_id, preferences):
    """"""
    Saves or updates user preferences in DynamoDB for the given chat_id.
    """"""
    user_preferences_table.put_item(Item={
        'chat_id': str(chat_id),
        **preferences
    })


def get_user_language(chat_id):
    """"""
    Retrieves the language preference of the user from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('language', None)


def set_user_language(chat_id, language_code):
    """"""
    Sets the language preference for the user and saves it in DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['language'] = language_code
    save_user_preferences(chat_id, preferences)


def save_user_state(chat_id, state):
    """"""
    Saves the user's current interaction state in DynamoDB, allowing for persistence of state across interactions.

    :param chat_id: The chat ID of the user.
    :param state: The interaction state (e.g., menu state, ongoing search, etc.) to be saved.
    """"""
    preferences = get_user_preferences(chat_id)  # Retrieve the current preferences for the user
    preferences['state'] = state  # Set the new state for the user
    save_user_preferences(chat_id, preferences)  # Save the updated preferences (including the state) to DynamoDB


def get_user_state(chat_id):
    """"""
    Retrieves the user's current interaction state from DynamoDB.

    :param chat_id: The chat ID of the user.
    :return: The current state of the user, or None if no state is set.
    """"""
    preferences = get_user_preferences(chat_id)  # Fetch the user preferences from DynamoDB
    return preferences.get('state', None)  # Return the current state, or None if no state is set


# --------------- Shop Selection History Handling ---------------

def save_user_selected_shops_history(chat_id, shops):
    """"""
    Saves the user's selected shop history in DynamoDB.
    Ensures the history only contains the 10 most recent entries.
    """"""
    preferences = get_user_preferences(chat_id)
    history = preferences.setdefault('selected_shops_history', [])

    if shops not in history:
        history.append(copy.deepcopy(shops))
        if len(history) > 10:  # Keep history to last 10 items
            history.pop(0)

    preferences['selected_shops_history'] = history
    save_user_preferences(chat_id, preferences)


def get_user_selected_shops_history(chat_id):
    """"""
    Retrieves the user's selected shops history from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('selected_shops_history', [])


# --------------- Tracked Items Handling ---------------


def get_tracked_items(chat_id):
    """"""
    Retrieves the list of items the user is tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('tracked_items', [])


def add_tracked_item(chat_id, item_name):
    """"""
    Adds an item to the user's tracking list, if it's not already being tracked.
    Returns True if the item is newly added, False if it already exists.
    """"""

    # Preprocess the item name: lowercase and convert Czech characters to English equivalents
    preprocessed_item_name = item_name.lower().translate(czech_to_english_map).strip()

    # Retrieve user preferences
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.setdefault('tracked_items', [])

    # Check if the preprocessed item name is already in the tracked items
    if preprocessed_item_name not in tracked_items:
        # Add the preprocessed item to the tracking list
        tracked_items.append(preprocessed_item_name)
        save_user_preferences(chat_id, preferences)
        return True

    return False


def remove_tracked_item(chat_id, item_name):
    """"""
    Removes an item from the user's tracking list.
    """"""
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.get('tracked_items', [])

    if item_name in tracked_items:
        tracked_items.remove(item_name)
        save_user_preferences(chat_id, preferences)


# --------------- Shop Inclusion and Exclusion Handling ---------------

def exclude_all_shops(chat_id):
    """"""
    Excludes all shops by retrieving all shop names from the pdf_metadata table
    and storing them in the user's preferences.
    """"""
    unique_shops = set()
    exclusive_start_key = None

    while True:
        scan_kwargs = {'ProjectionExpression': 'shop_name'}
        if exclusive_start_key:
            scan_kwargs['ExclusiveStartKey'] = exclusive_start_key

        response = pdf_metadata_table.scan(**scan_kwargs)

        for item in response.get('Items', []):
            unique_shops.add(item['shop_name'])

        exclusive_start_key = response.get('LastEvaluatedKey')
        if not exclusive_start_key:
            break

    preferences = get_user_preferences(chat_id)
    preferences['excluded_shops'] = list(unique_shops)
    save_user_preferences(chat_id, preferences)

    return preferences


def get_excluded_shops(chat_id):
    """"""
    Retrieves the list of excluded shops from the user's preferences.
    """"""
    preferences = get_user_preferences(chat_id)
    return set(preferences.get('excluded_shops', []))


def get_included_shops(chat_id):
    """"""
    Returns the list of shops that are included for tracking by comparing all shops
    to the excluded shops in the user's preferences.
    """"""
    excluded_shops = get_excluded_shops(chat_id)
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    all_shops = set(item['shop_name'] for item in response['Items'])
    included_shops = all_shops - excluded_shops
    return sorted(included_shops) if included_shops else []


def exclude_shop(chat_id, shop_name):
    """"""
    Adds a shop to the user's excluded shops list.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name not in excluded_shops:
        excluded_shops.add(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


def include_shop(chat_id, shop_name):
    """"""
    Removes a shop from the user's excluded shops list, thereby including it in tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name in excluded_shops:
        excluded_shops.remove(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


# --------------- General Shop Management ---------------

def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


# --------------- Sale Sheet and Media Preferences Handling ---------------

def is_pdf_receive_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('receive_pdf_enabled', True)  # Default is True


def set_pdf_receive_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['receive_pdf_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_photo_group_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('photo_group_enabled', True)  # Default is True


def set_photo_group_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['photo_group_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_text_info_enabled(chat_id):
    """"""
    Returns whether text info is enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('text_info_enabled', False)  # Default is False


def set_text_info_enabled(chat_id, enabled):
    """"""
    Enables or disables text info for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['text_info_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


# --------------- Search Handling ---------------

czech_to_english_map = str.maketrans(
    ""áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""acdeeinorstuuyzACDEEINORSTUUYZ""
)


def preprocess_text(text):
    """"""Lowercases, converts Czech characters, and removes spaces for consistent comparison.""""""
    text = text.lower().translate(czech_to_english_map).replace("" "", """").strip()
    return text


def custom_rolling_similarity_score(part, text, tolerance=1):
    """"""Calculate similarity score between part and a substring in text using rolling hash.""""""
    part_length = len(part)

    # Check if part length is greater than text length
    if part_length > len(text):
        return 0  # Return 0 if text is too short for a match

    # Calculate initial hash values for the part and the first substring of the text
    part_hash = sum(ord(c) for c in part)
    substring_hash = sum(ord(text[i]) for i in range(part_length))

    max_score = 0

    for i in range(len(text) - part_length + 1):
        # Check if the hash difference is within tolerance * average char value
        if abs(substring_hash - part_hash) <= tolerance * 10:  # Adjusting tolerance scale for leniency
            candidate = text[i:i + part_length]
            # Calculate actual similarity score based on character matching
            score = sum(1 for x, y in zip(part, candidate) if x == y) / part_length * 100
            max_score = max(max_score, score)

        # Update hash by sliding the window
        if i + part_length < len(text):
            substring_hash += ord(text[i + part_length]) - ord(text[i])

    return max_score


def find_item(item_names, shop_name=None, included_shops=None, pdf_filename=None, similarity_threshold=75, penalty=10):
    """"""
    Searches for an item in the detected_data_table based on the given item_name,
    optional shop_name, and list of included_shops. It uses fuzzy matching for flexible name search.

    :param item_names: The name(s) of the items to search for.
    :param shop_name: (Optional) The specific shop to search in.
    :param included_shops: (Optional) A list of shops to limit the search.
    :param pdf_filename: (Optional) The original PDF filename where the item was extracted.
    :param similarity_threshold: (Optional) The threshold of similarity to consider a match (default is 80).
    :param penalty: (Optional) The penalty to apply when no match is found for any word (default is 10).
    :return: A list of matching items with their prices and other metadata.
    """"""
    results = []
    shop_name = shop_name or """"
    included_shops = included_shops or []

    if isinstance(item_names, str):
        item_names = [item_names]

    # Preprocess input item names: lowercase, convert Czech characters, and split by non-alphanumeric characters
    preprocessed_item_names = [
        [preprocess_text(word) for word in re.split(r'\W+', item_name)]
        for item_name in item_names
    ]

    scan_kwargs = {
        'FilterExpression': Attr('valid').eq(True)
    }

    if shop_name:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').eq(shop_name)
    if included_shops:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').is_in(included_shops)
    if pdf_filename:
        pdf_base_name = pdf_filename.replace("".pdf"", """")
        scan_kwargs['FilterExpression'] &= Attr('image_id').contains(pdf_base_name)

    response = detected_data_table.scan(**scan_kwargs)

    for item in response.get('Items', []):
        item_shop_name = item.get('shop_name', 'Unknown Shop')

        # Preprocess item names for matching
        db_item_name = preprocess_text(item.get('item_name', '') or '')
        processed_item_name = preprocess_text(item.get('processed_item_name', '') or '')

        for preprocessed_item_name_parts in preprocessed_item_names:
            total_score = 0
            match_found = False

            for part in preprocessed_item_name_parts:
                # Calculate similarity scores of the entire part against db_item_name, processed_item_name, and image_text
                db_score = fuzz.partial_ratio(part, db_item_name)
                processed_score = fuzz.partial_ratio(part, processed_item_name)

                # Only check image_text if db_score and processed_score are below the threshold
                if db_score < similarity_threshold and processed_score < similarity_threshold:
                    # Use rolling hash similarity scoring for image_text
                    image_text = preprocess_text(item.get('whole_image_ocr_text', '') or '')
                    image_score = custom_rolling_similarity_score(part, image_text, tolerance=1)
                else:
                    image_score = 0

                # Select the maximum score out of the four scores
                max_score = max(db_score, processed_score, image_score)

                # Apply penalty or update total score based on max_score
                if max_score == 0:
                    total_score -= penalty
                else:
                    total_score += max_score
                    match_found = True

            # Calculate the average score for the item
            avg_score = total_score / len(preprocessed_item_name_parts) if preprocessed_item_name_parts else 0

            # Add item to results if the average score meets the threshold
            if avg_score >= similarity_threshold and match_found:
                price = find_price_for_item(item)
                results.append({
                    'item_name': item.get('item_name', ''),
                    'price': price,
                    'shop_name': item_shop_name,
                    'similarity_score': avg_score,
                    'image_name': item.get('image_id')
                })

    # Sort the results by similarity score in descending order
    results = sorted(results, key=lambda x: x['similarity_score'], reverse=True)

    return results


def find_price_for_item(obj):
    """"""
    Extracts different price types (item_price, item_initial_price, item_member_price)
    from a DynamoDB item and returns them as a formatted string.

    :param obj: The DynamoDB item containing price information.
    :return: A formatted string containing price information or ""Price not found"" if no prices exist.
    """"""
    prices = []

    def try_convert_to_dict(value):
        """"""
        Utility function to safely convert a string into a dictionary if possible.

        :param value: The value to attempt conversion.
        :return: A dictionary if successful, otherwise the original value.
        """"""
        if isinstance(value, str):
            try:
                return ast.literal_eval(value)  # Attempt to evaluate as a Python literal
            except (ValueError, SyntaxError):
                try:
                    return json.loads(value)  # Attempt to parse as JSON
                except (ValueError, TypeError):
                    return value  # Return original value if conversion fails
        return value

    # Try to convert price fields to dictionaries, if applicable
    processed_price = try_convert_to_dict(obj.get('processed_item_price', None))
    initial_price = try_convert_to_dict(obj.get('processed_item_initial_price', None))
    member_price = try_convert_to_dict(obj.get('processed_item_member_price', None))

    # Handle processed_item_price
    if isinstance(processed_price, dict):
        prices.append(f""Price: {processed_price.get('item_price')}\n"")
    elif processed_price:  # If it's a string or number
        prices.append(f""Price: {processed_price}\n"")

    # Handle processed_item_initial_price
    if isinstance(initial_price, dict):
        prices.append(f""Initial price: {initial_price.get('item_initial_price')}\n"")
    elif initial_price:
        prices.append(f""Initial price: {initial_price}\n"")

    # Handle processed_item_member_price
    if isinstance(member_price, dict):
        prices.append(f""Member price: {member_price.get('item_member_price')}\n"")
    elif member_price:
        prices.append(f""Member price: {member_price}\n"")

    # Return the price strings or ""Price not found"" if no prices are available
    return """".join(prices) if prices else ""Price not found""


# --------------- User Interaction Handling ---------------

def get_available_languages():
    """"""
    Returns a dictionary of supported languages and their respective codes.
    """"""
    return {
        'en': 'English',
        'ru': 'Русский',
        'uk': 'Українська',
        'cs': 'Čeština'
    }


# Language selection prompt
def language_selection(chat_id):
    """"""
    Sends a language selection prompt to the user with inline buttons for language choices.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""inline_keyboard"": [
            [{""text"": ""English"", ""callback_data"": ""lang_en""}],
            [{""text"": ""Русский"", ""callback_data"": ""lang_ru""}],
            [{""text"": ""Українська"", ""callback_data"": ""lang_uk""}],
            [{""text"": ""Čeština"", ""callback_data"": ""lang_cs""}],
        ]
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Welcome! Please select your language:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Main menu display
def main_menu(chat_id):
    """"""
    Displays the main menu to the user with various options for tracking and comparing shop items.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""keyboard"": [
            [{""text"": ""🔍 Search for item""}],
            [{""text"": ""🛒 Add shop item to track price""}],
            [{""text"": ""🛍 Compare shopping list over shops""}],
            [{""text"": ""⚙️ Settings""}],
            [{""text"": ""ℹ️ About project""}],
        ],
        ""resize_keyboard"": True
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Main Menu"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Including shop to track at the start
def include_user_tracking_shops(chat_id):
    """"""
    Sends a list of shops for the user to start tracking items in. If no shops are excluded,
    it loads all shops into the excluded list first.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    shops = list(get_excluded_shops(chat_id))

    # If no excluded shops, exclude all shops initially
    if shops == []:
        exclude_all_shops(chat_id)

    # Send the list of shops with options to select from
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop from the list for item search and tracking. You can change this later in 'Settings'."",
        ""reply_markup"": {
            ""keyboard"": [[shop] for shop in get_excluded_shops(chat_id)] + [[""⬅️ Back to main menu""]],
            ""resize_keyboard"": True
        }
    })

    # Save the user's state as selecting shops
    save_user_state(chat_id, '/start_selecting_shops')


# Settings menu display with dynamic labels for photo group and text info
def settings_menu(chat_id):
    """"""
    Displays the settings menu with dynamic labels for photo group and text info toggling options.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""

    # Get the current state of photo groups and text info to display in the button labels
    photo_group_state = ""Enabled"" if is_photo_group_enabled(chat_id) else ""Disabled""
    text_info_state = ""Enabled"" if is_text_info_enabled(chat_id) else ""Disabled""
    receive_pdf_state = ""Enabled"" if is_pdf_receive_enabled(chat_id) else ""Disabled""

    # Create buttons with dynamic labels based on current settings
    buttons = {
        ""keyboard"": [
            [{""text"": ""🚫 Exclude some shops from tracking""}],
            [{""text"": ""✅ Include some shops in tracking""}],
            [{""text"": ""🛑 Remove shop item from tracking price""}],
            [{""text"": f""📄 Turn {'off' if receive_pdf_state == 'Enabled' else 'on'} Receiving New PDFs""}],
            [{""text"": f""📄 Turn {'off' if photo_group_state == 'Enabled' else 'on'} items photo groups""}],
            [{""text"": f""📄 Turn {'off' if text_info_state == 'Enabled' else 'on'} items text info""}],
            [{""text"": ""🌐 Change interface language""}],
            [{""text"": ""⬅️ Back to main menu""}],
        ],
        ""resize_keyboard"": True
    }

    # Send the settings menu
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Select an option from Settings:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Sending images as an album
def send_images_as_album(chat_id, media_group, shop_name):
    """"""
    Sends images in batches of up to 10 (as albums) to the user via Telegram.

    :param chat_id: The Telegram chat ID of the user.
    :param media_group: A list of tuples (S3 image path, local temp path) representing images to send.
    :param shop_name: The name of the shop to include in the caption of the first image in each batch.
    """"""

    def chunks(iterable, size):
        """"""Helper function to divide media_group into chunks of a given size.""""""
        iterator = iter(iterable)
        while True:
            batch = list(islice(iterator, size))
            if not batch:
                break
            yield batch

    for batch_index, batch in enumerate(chunks(media_group, 10)):
        media = []
        files = {}

        # Loop through the current batch of images
        for i, (s3_image_path, temp_path) in enumerate(batch):
            try:
                # Extract the image filename from the S3 path
                image_name = os.path.basename(s3_image_path)

                # Log the S3 path and temporary file for debugging
                logger.debug(f""Downloading image from S3: {s3_image_path} to {temp_path}"")

                # Use a temporary file to handle the downloaded image
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file_path = temp_file.name
                    # Download the image from S3 into the temporary file
                    download_file_from_s3(s3_image_path, temp_file_path)

                    # Read the file content for use in the Telegram API
                    with open(temp_file_path, 'rb') as image_file:
                        image_content = image_file.read()

                    # Prepare the files dictionary (it must have unique keys for each image)
                    files[f""photo{batch_index}_{i}""] = (image_name, image_content)

                    # Prepare the media array with references to the attached photos
                    media.append({
                        ""type"": ""photo"",
                        ""media"": f""attach://photo{batch_index}_{i}"",
                        ""caption"": shop_name if i == 0 else """"  # Add shop name as caption only to the first image
                    })

                    # Remove the temporary file after reading
                    os.remove(temp_file_path)

            except Exception as e:
                logger.error(f""Error processing image {image_name}: {str(e)}"")

        # If no valid media is available, log an error and return
        if not media:
            logger.error(""No valid media to send."")
            continue

        # Send the media group using the Telegram API
        try:
            response = requests.post(f""{API_URL}/sendMediaGroup"", files=files, data={
                ""chat_id"": chat_id,
                ""media"": json.dumps(media)  # Convert the media list to a JSON string
            })

            logger.debug(f""Telegram API response for sendMediaGroup: {response.status_code}, {response.text}"")
        except Exception as e:
            logger.error(f""Error sending media group: {str(e)}"")


def send_single_pdf(
        chat_ids,
        file_source,
        shop_name
):
    """"""
    Sends a single PDF file to multiple users via Telegram using a file path.

    :param chat_ids: A list of Telegram chat IDs of the users.
    :param file_source: The file path of the PDF to send.
    :param shop_name: The name of the shop to include in the caption of the PDF.
    """"""
    try:
        # Download the file from S3 only once
        # Extract the original filename from the S3 path
        original_filename = os.path.basename(file_source)
        logger.debug(f""Original filename extracted: {original_filename}"")

        # Create a temporary file with the original filename and extension
        with tempfile.NamedTemporaryFile(delete=False, suffix="".pdf"") as temp_file:
            temp_file_path = temp_file.name
            logger.debug(f""Temporary file created at {temp_file_path}"")

            # Download the file from S3 (replace this with your actual download logic)
            download_file_from_s3(file_source, temp_file_path)

        # Iterate over each chat_id and send the file to each user
        for chat_id in chat_ids:
            logger.debug(f""Sending PDF to chat_id: {chat_id}"")

            # Prepare the base payload for each user
            data = {
                ""chat_id"": chat_id,
                ""caption"": f""New {shop_name} letak available""
            }

            # Send via multipart/form-data (local file upload)
            with open(temp_file_path, 'rb') as pdf_file:
                logger.debug(f""Opened local file: {temp_file_path} for chat_id: {chat_id}"")

                # Prepare multipart form-data
                files = {
                    ""document"": (original_filename, pdf_file, ""application/pdf"")
                }

                # Send the document via multipart form-data
                logger.debug(f""Sending document via multipart/form-data for chat_id: {chat_id}..."")
                response = requests.post(f""{API_URL}/sendDocument"", files=files, data=data)

                # Log the response from Telegram for debugging purposes
                logger.debug(
                    f""Telegram API response for sendDocument (chat_id: {chat_id}): {response.status_code}, {response.text}"")

        # Clean up the temporary file after sending to all users
        logger.debug(f""Cleaning up temporary file: {temp_file_path}"")
        os.remove(temp_file_path)
        logger.debug(f""Temporary file {temp_file_path} removed successfully"")

    except ValueError as ve:
        logger.error(f""ValueError: {str(ve)}"")
    except Exception as e:
        logger.error(f""Error sending document '{file_source}' to chat IDs {chat_ids}: {str(e)}"")


# --------------- Message Processing ---------------

def handle_start_command(chat_id, state):
    description = ""Welcome to the Smart Shopping Bot! I will help you track prices, manage sale sheets, and find the best shopping paths.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": description})

    if state == ""new_user"":
        # Set default preferences for new users
        preferences = get_user_preferences(chat_id)
        if 'photo_group_enabled' not in preferences:
            preferences['photo_group_enabled'] = True  # Default to show photo groups
        if 'text_info_enabled' not in preferences:
            preferences['text_info_enabled'] = False  # Default to hide text info
        if 'receive_pdf_enabled' not in preferences:
            preferences['receive_pdf_enabled'] = True  # Default to send pdf after new validation pipeline check
        save_user_preferences(chat_id, preferences)

        # Guide user through initial steps of setup: language selection or shop inclusion
        if get_user_language(chat_id) is None:
            language_selection(chat_id)
            save_user_state(chat_id, None)
        elif get_included_shops(chat_id) is None:
            include_user_tracking_shops(chat_id)
            save_user_state(chat_id, None)
        else:
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # If user is not new, take them directly to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_shop_selection(chat_id, text):
    if text == ""➕ Add another shop"":
        include_user_tracking_shops(chat_id)
    elif text == ""➡️ Save tracking shop list. Return to the main menu"":
        if not get_included_shops(chat_id):
            # No shops have been included yet, notify the user
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select at least one shop from the list:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(get_excluded_shops(chat_id))] + [
                        [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True}
            })
        else:
            # At least one shop is included, allow returning to the menu
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # Get the list of excluded shops
        shops = list(get_excluded_shops(chat_id))
        # Check if the input text is a valid shop
        if text in shops:
            include_shop(chat_id, text)
            # Ask if the user wants to add more shops or return to the main menu
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Save tracking shop list. Return to the main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, '/start_selecting_shops')
        elif text == ""⬅️ Back to main menu"":
            # Check if any shops are included before allowing return to the main menu
            if not get_included_shops(chat_id):
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please select at least one shop before returning to the menu:"",
                    ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                     ""resize_keyboard"": True}
                })
            else:
                main_menu(chat_id)
                save_user_state(chat_id, None)

        else:
            # The user input is not a valid shop, ask them to select again
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a valid shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_search_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please enter the name of the item you want to search for."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'searching_item')


def handle_add_shop_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please provide the name of the shop item you want to track."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'adding_item')


def handle_settings_button(chat_id):
    settings_menu(chat_id)
    save_user_state(chat_id, 'in_settings')


def handle_compare_shop_list_button(chat_id):
    # Clean preferences after unexpected last user manipulations
    preferences = get_user_preferences(chat_id)
    preferences['selected_shops'] = []
    preferences['item_list'] = []
    save_user_preferences(chat_id, preferences)

    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop or list of shops from your history."",
        ""reply_markup"": {
            ""keyboard"": [[""List of all shops""], [""Lists of shops from history""], [""⬅️ Back to main menu""]],
            ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'selecting_shops')


def handle_about_button(chat_id):
    about_text = ""This bot helps you optimize your shopping by tracking prices, managing sale sheets, and finding the best shopping routes.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": about_text})
    main_menu(chat_id)
    save_user_state(chat_id, None)


def handle_item_adding_searching_state(chat_id, state, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Handle the case where user provides a single item name or multiple item names (split by commas or new lines)
        item_names = [name.strip() for name in text.split("","")]

        # Call the new find_item method with a list of item names
        found_items = find_item(item_names=item_names, included_shops=get_included_shops(chat_id))

        # If adding the item for tracking
        if state == 'adding_item':
            # Loop through each item name and handle them separately for adding to tracking
            for item_name in item_names:
                added = add_tracked_item(chat_id, item_name)

                if added:
                    response = f""'{item_name}' saved for tracking. I will notify you when '{item_name}' has a valid sale.""
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                else:
                    tracked_items = get_tracked_items(chat_id)
                    response = f""'{item_name}' is already in your tracking list. Here is your current list:\n"" + ""\n"".join(
                        tracked_items)
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                    main_menu(chat_id)
                    save_user_state(chat_id, None)
                    return

        # If searching for the item(s)
        if found_items:
            items_by_shop = {}

            # Group found items by shop
            for found_item in found_items:
                shop_name = found_item['shop_name']
                if shop_name not in items_by_shop:
                    items_by_shop[shop_name] = []
                items_by_shop[shop_name].append(found_item)

            photo_group_enabled = is_photo_group_enabled(chat_id)
            text_info_enabled = is_text_info_enabled(chat_id)

            # Iterate over each shop and its associated items
            for shop_name, shop_items in items_by_shop.items():
                # Prepare the initial response message
                response = f""Here is what I found for '{', '.join(item_names)}' in {shop_name}:\n""
                media_group = []

                # Process each found item for the shop
                for found_item in shop_items:
                    if text_info_enabled:
                        response += f""- {found_item['item_name']} at {shop_name}: {found_item['price']}\n""

                    s3_image_dir = found_item.get('image_name')
                    # Collecting images for the media group (album) if photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")

                # Send media group (photos) if enabled
                if media_group and photo_group_enabled:
                    logger.debug(f""Sending media group for {shop_name} with {len(media_group)} images"")
                    send_images_as_album(chat_id, media_group, shop_name)

                # Send text response with item details if text info is enabled
                if text_info_enabled:
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

        else:
            # If no items found, notify the user
            requests.post(f""{API_URL}/sendMessage"",
                          json={""chat_id"": chat_id, ""text"": f""No items found for '{', '.join(item_names)}'.""})

        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_in_settings_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""📄 Turn off Receiving New PDFs"" or text == ""📄 Turn on Receiving New PDFs"":
        current_receive_pdf_state = is_pdf_receive_enabled(chat_id)
        # Safe to toggle photo groups
        set_pdf_receive_enabled(chat_id, not current_receive_pdf_state)
        new_state = ""enabled"" if not current_receive_pdf_state else ""disabled""
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": f""Receiving of new PDFs has been {new_state}.""
        })
        settings_menu(chat_id)
    elif text == ""📄 Turn on items photo groups"" or text == ""📄 Turn off items photo groups"":
        # Toggle the photo group setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_text_info_state and current_photo_group_state:
            # Cannot disable photo groups if text info is already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Text info is already disabled, so photo groups cannot be turned off.""
            })
        else:
            # Safe to toggle photo groups
            set_photo_group_enabled(chat_id, not current_photo_group_state)
            new_state = ""enabled"" if not current_photo_group_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item photo groups are now {new_state}.""
            })
            settings_menu(chat_id)

    elif text == ""📄 Turn on items text info"" or text == ""📄 Turn off items text info"":
        # Toggle the text info setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_photo_group_state and current_text_info_state:
            # Cannot disable text info if photo groups are already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Photo groups are already disabled, so text info cannot be turned off.""
            })
        else:
            # Safe to toggle text info
            set_text_info_enabled(chat_id, not current_text_info_state)
            new_state = ""enabled"" if not current_text_info_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item text info is now {new_state}.""
            })
            settings_menu(chat_id)
    elif text == ""🚫 Exclude some shops from tracking"":
        # Retrieve the included shops that can be excluded
        included_shops = get_included_shops(chat_id)

        # If there are no included shops, inform the user
        if not included_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""No shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of shops that can be excluded
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to exclude from tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in included_shops] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'excluding_shop')
    elif text == ""✅ Include some shops in tracking"":
        # Retrieve the excluded shops that can be included
        excluded_shops = get_excluded_shops(chat_id)

        # If all shops are already included, inform the user
        if not excluded_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""All shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of excluded shops that can be included
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to include in tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(excluded_shops)] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'including_shop')
    elif text == ""🛑 Remove shop item from tracking price"":
        items_to_remove = get_tracked_items(chat_id)
        if items_to_remove:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Select an item to remove from tracking:"",
                ""reply_markup"": {""keyboard"": [[item] for item in items_to_remove] + [[""⬅️ Back to settings""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'removing_item')
        else:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any items being tracked."",
            })
            settings_menu(chat_id)
            save_user_state(chat_id, 'in_settings')


def handle_excluding_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        exclude_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' excluded from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_including_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        include_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' included for tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_removing_item_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        remove_tracked_item(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Item '{text}' removed from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_shop_list_history_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        try:
            index = int(text) - 1
            shop_history = get_user_selected_shops_history(chat_id)
            if 0 <= index < len(shop_history):
                selected_history_list = shop_history[index]
                preferences = get_user_preferences(chat_id)
                preferences['selected_shops'] = selected_history_list
                save_user_preferences(chat_id, preferences)
                # Proceed to item entry
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please provide your shopping list, one per line and send."",
                    ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
                })
                preferences['item_list'] = []
                save_user_preferences(chat_id, preferences)
                save_user_state(chat_id, 'entering_items')
            else:
                raise IndexError
        except (ValueError, IndexError):
            # Handle invalid input
            shop_history = get_user_selected_shops_history(chat_id)
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]
            text_message = ""Invalid selection. Please choose a number from the list:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })


def handle_selecting_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')
    elif text == ""Lists of shops from history"":
        # Get the user's shop history
        shop_history = get_user_selected_shops_history(chat_id)
        if shop_history:
            # Build the keyboard buttons with numbers
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]

            # Corrected prompt and display
            text_message = ""Please select a shop list from your history:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""

            # Send the message with the keyboard
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })

            # Save the user state
            save_user_state(chat_id, 'shop_list_history')
        else:
            shops = get_all_shops()
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any history saved list. Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
    else:
        # Get the list of available shops
        shops = get_all_shops()
        # Check if the input text is a valid shop
        if text in shops:
            # Save the selected shop
            preferences = get_user_preferences(chat_id)
            selected_shops = preferences.get('selected_shops', [])
            if text not in selected_shops:
                selected_shops.append(text)
                preferences['selected_shops'] = selected_shops
                save_user_preferences(chat_id, preferences)
            # Ask if the user wants to add more shops or continue
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Continue with shop list""],
                                 [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

        else:
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_confirming_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➕ Add another shop"":
        # Exclude already selected shops
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        shops = [shop for shop in get_all_shops() if shop not in selected_shops]
        if shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [
                    [""⬅️ Back to main menu""] + [""➡️ Continue with shop list""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            # All shops have been selected
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have selected all available shops."",
                ""reply_markup"": {
                    ""keyboard"": [[""➡️ Continue with shop list""], [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')

    else:
        # Handle unexpected input
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": ""Please select an option from the menu.""
        })


def handle_entering_items_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Add item to the list
        preferences = get_user_preferences(chat_id)
        item_list = preferences.get('item_list', [])
        item_list.extend(text.split('\n'))
        preferences['item_list'] = item_list
        save_user_preferences(chat_id, preferences)
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        item_list = preferences.get('item_list', [])
        response = ""Here are the items found in the selected shops:\n""
        # Retrieve user preferences for photo group and text info settings
        photo_group_enabled = is_photo_group_enabled(chat_id)
        text_info_enabled = is_text_info_enabled(chat_id)
        # List to collect all images for the media group
        for shop in selected_shops:
            if text_info_enabled:
                response += f""\nItems in {shop}:\n""
            media_group = []  # List to collect all images for the media group
            # Call find_item once for all items in the current shop
            found_items = find_item(item_names=item_list, shop_name=shop)

            if found_items:
                for found_item in found_items:
                    # Extract price and image path details from the found item
                    price = found_item.get('price')
                    s3_image_dir = found_item.get('image_name')

                    logger.debug(f""Found item: {found_item}"")
                    logger.debug(f""Price: {price}, Image Path: {s3_image_dir}"")

                    # Include price details in the response if text info is enabled
                    if text_info_enabled:
                        # Add the item price or ""Price not found"" based on availability
                        if price:
                            response += f""- {found_item['item_name']} at {shop}: {price}\n""
                        else:
                            response += f""- {found_item['item_name']} at {shop}: Price not found\n""

                    # Process the image only if the photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        # Extract the filename from the full S3 path
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""

                        # Add the image filename and its corresponding local path to the media group
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")
            else:
                # If none of the items are found in the shop, add a not found message to the response
                for item_name in item_list:
                    response += f""- {item_name}: Not found in {shop}\n""

            # After looping through items, send the images as an album if there are any and photo group is enabled
            logger.debug(f""Media group length: {len(media_group)}. Photo group enabled: {photo_group_enabled}"")
            if media_group and photo_group_enabled:
                logger.debug(f""Sending media group for shop: {shop}"")
                send_images_as_album(chat_id, media_group, shop)
                media_group.clear()  # Clear the media group after sending to avoid duplicate entries
            else:
                logger.debug(f""No images to send or photo group is disabled"")

            # Send the final response with text results if text info is enabled
        if text_info_enabled:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

            # Go back to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)

        # Clear selected shops and item list
        preferences['selected_shops'] = []
        preferences['item_list'] = []
        save_user_preferences(chat_id, preferences)


def process_message(update):
    chat_id = update['message']['chat']['id']
    text = update['message'].get('text')

    # Get current user state
    state = get_user_state(chat_id)

    # Main dispatcher based on text command or state
    if text == ""/start"":
        handle_start_command(chat_id, state)
    elif state == '/start_selecting_shops':
        handle_shop_selection(chat_id, text)
    elif text == ""🔍 Search for item"":
        handle_search_button(chat_id)
    elif text == ""🛒 Add shop item to track price"":
        handle_add_shop_button(chat_id)
    elif text == ""⚙️ Settings"":
        handle_settings_button(chat_id)
    elif text == ""🛍 Compare shopping list over shops"":
        handle_compare_shop_list_button(chat_id)
    elif text == ""ℹ️ About project"":
        handle_about_button(chat_id)
    elif state == 'adding_item' or state == ""searching_item"":
        handle_item_adding_searching_state(chat_id, state, text)
    elif state == 'in_settings':
        handle_in_settings_state(chat_id, text)
    elif state == 'excluding_shop':
        handle_excluding_shop_state(chat_id, text)
    elif state == 'including_shop':
        handle_including_shop_state(chat_id, text)
    elif state == 'removing_item':
        handle_removing_item_state(chat_id, text)
    elif state == ""shop_list_history"":
        handle_shop_list_history_state(chat_id, text)
    elif state == 'selecting_shops':
        handle_selecting_shops_state(chat_id, text)
    elif state == 'confirming_shops':
        handle_confirming_shops_state(chat_id, text)
    elif state == 'entering_items':
        handle_entering_items_state(chat_id, text)
    else:
        if text == ""⬅️ Back to main menu"":
            main_menu(chat_id)
            save_user_state(chat_id, None)
        else:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id,
                                                          ""text"": ""I'm sorry, I didn't understand that. Please choose an option from the menu.""})


# Handle callback queries from inline buttons (e.g., language selection)
def process_callback_query(update):
    """"""
    Processes the callback queries triggered by inline buttons, such as language selection.
    :param update: The update payload containing the callback query details.
    """"""
    query = update['callback_query']
    chat_id = query['message']['chat']['id']  # Extract the chat ID of the user
    message_id = query['message']['message_id']  # Extract the message ID to edit it later
    data = query.get('data')  # Retrieve the callback data (e.g., 'lang_en')
    callback_query_id = query['id']  # The ID for answering the callback query

    # Check if the callback is related to language selection (data starts with 'lang_')
    if data.startswith('lang_'):
        language_code = data.split('_')[1]  # Extract the language code (e.g., 'en', 'ru')
        set_user_language(chat_id, language_code)  # Save the selected language to user preferences

        # Answer the callback query to stop Telegram's ""loading"" animation
        answer_payload = {
            ""callback_query_id"": callback_query_id,
            ""text"": ""Language updated!"",  # Confirmation message to the user
            ""show_alert"": False  # Do not show a popup, just stop the loading animation
        }
        requests.post(f""{API_URL}/answerCallbackQuery"", json=answer_payload)

        # Edit the original message to remove the inline keyboard and update the text
        new_text = f""Language selected! You have set your language to: {get_available_languages().get(language_code, 'Unknown')}""
        edit_payload = {
            ""chat_id"": chat_id,
            ""message_id"": message_id,
            ""text"": new_text,
            ""reply_markup"": {}  # Remove the inline keyboard by setting an empty reply_markup
        }
        requests.post(f""{API_URL}/editMessageText"", json=edit_payload)

        # Proceed to include the shops tracking list for the user
        include_user_tracking_shops(chat_id)


def handle_pdf_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')

    s3_pdf_path = ""pdfs/"" + pdf_file
    send_single_pdf(users_id_list, s3_pdf_path, shop_name)


def handle_tracked_items_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')
    tracked_items_list = update.get('tracked_items_list', [])  # A list of tracked items for each user

    # Prepare global variables for tracking all responses and media to send in batches
    user_responses = {}  # Dictionary to hold responses per user
    user_media_groups = {}  # Dictionary to hold media groups per user

    # Iterate over each user and their tracked items
    for i, user_id in enumerate(users_id_list):
        user_tracked_items = tracked_items_list[i]

        # Check if text and photo group sending is enabled for the user
        text_info_enabled = is_text_info_enabled(user_id)
        photo_group_enabled = is_photo_group_enabled(user_id)

        # If there are no tracked items for the user, skip to the next user
        if not user_tracked_items:
            continue

        # Call the find_item method once for all tracked items at once
        matched_items = find_item(item_names=user_tracked_items, shop_name=shop_name, pdf_filename=pdf_file)

        # If no matched items are found, skip to the next user
        if not matched_items:
            if text_info_enabled:
                user_responses[user_id] = ""No items found in this flyer.""
            continue

        # Prepare lists to store matched items and media for this user
        user_response = ""Your tracked items are now available:\n\n""  # Friendly message
        media_group = []

        # Process the matched items
        for found_item in matched_items:
            price = found_item.get('price')
            s3_image_dir = found_item.get('image_name')

            # Include price details in the response if text info is enabled
            if text_info_enabled:
                if price:
                    user_response += f""- {found_item['item_name']} at {shop_name}: {price}\n""
                else:
                    user_response += f""- {found_item['item_name']} at {shop_name}: Price not found\n""

            # Process the image only if the photo group is enabled
            if photo_group_enabled and s3_image_dir:
                image_filename = os.path.basename(s3_image_dir)
                local_image_path = f""/tmp/{image_filename}""
                media_group.append((s3_image_dir, local_image_path))

        # Add the user's response and media group to the respective dictionaries
        if text_info_enabled and user_response.strip():
            user_responses[user_id] = user_response
        if media_group and photo_group_enabled:
            user_media_groups[user_id] = media_group

    # Now send out the responses and media groups in batches to minimize API calls
    message = f""Your tracked items are now available at {shop_name}""
    # Send media groups for users
    for user_id, media_group in user_media_groups.items():
        send_images_as_album(user_id, media_group, message)

    # Send text responses for users
    for user_id, response in user_responses.items():
        requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": user_id, ""text"": response})


def lambda_handler(event, context):
    """"""
    This function will act as the webhook to handle Telegram updates when deployed to AWS Lambda.
    It will process both regular messages and callback queries.
    """"""
    try:
        # Parse the body from the incoming event
        if 'body' in event:
            update = json.loads(event['body'])  # Extract the JSON body from the Lambda event

            # Process message or callback query based on the update type
            if 'message' in update:
                process_message(update)  # Call your process_message function
            elif 'callback_query' in update:
                process_callback_query(update)  # Call your process_callback_query function

            process_type = update.get('process_type', None)
            if process_type == 'tracked_items_list':
                # Process the tracked items-related webhook
                handle_tracked_items_newsletter(update)
            elif process_type == 'pdf_newsletter':
                handle_pdf_newsletter(update)

            # Return a success response to Telegram
            return {
                'statusCode': 200,
                'body': json.dumps({'status': 'ok'})
            }
        else:
            logger.error('No body found in the request')
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Bad Request'})
            }

    except Exception as e:
        logger.error(f""Error processing the request: {str(e)}"")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal Server Error'})
        }
",") if unique_shops else []


def create_shop_from_pdf_data(chat_id, data, language='en'):
    """"""
    Creates a new shop from the PDF data in the specified format.
    """"""
    # Create the new shop using the data as an empty dictionary
    shop = {}

    # Convert the data into a JSON object
    try:
        json_object = json.dumps(data, indent=4, sort_keys=True, separators=(ast.literal_eval,))
    except Exception as e:
        print(""JSON exception parsing:"", type(e).__name__)
        json"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/six.py,"# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""""""Utilities for writing code that runs on Python 2 and 3""""""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.16.0""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith(""java""):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):

            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """"""Add documentation to a function.""""""
    func.__doc__ = doc


def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):

    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = [""__doc__"", ""__name__""]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """"""
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """"""

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + ""."" + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + ""."" + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError(""This loader does not know module "" + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """"""
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """"""
        return hasattr(self.__get_module(fullname), ""__path__"")

    def get_code(self, fullname):
        """"""Return None

        Required, if is_package is implemented""""""
        self.__get_module(fullname)  # eventually raises ImportError
        return None
    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass

_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """"""Lazy loading of moved objects""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""cStringIO"", ""cStringIO"", ""io"", ""StringIO""),
    MovedAttribute(""filter"", ""itertools"", ""builtins"", ""ifilter"", ""filter""),
    MovedAttribute(""filterfalse"", ""itertools"", ""itertools"", ""ifilterfalse"", ""filterfalse""),
    MovedAttribute(""input"", ""__builtin__"", ""builtins"", ""raw_input"", ""input""),
    MovedAttribute(""intern"", ""__builtin__"", ""sys""),
    MovedAttribute(""map"", ""itertools"", ""builtins"", ""imap"", ""map""),
    MovedAttribute(""getcwd"", ""os"", ""os"", ""getcwdu"", ""getcwd""),
    MovedAttribute(""getcwdb"", ""os"", ""os"", ""getcwd"", ""getcwdb""),
    MovedAttribute(""getoutput"", ""commands"", ""subprocess""),
    MovedAttribute(""range"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""reload_module"", ""__builtin__"", ""importlib"" if PY34 else ""imp"", ""reload""),
    MovedAttribute(""reduce"", ""__builtin__"", ""functools""),
    MovedAttribute(""shlex_quote"", ""pipes"", ""shlex"", ""quote""),
    MovedAttribute(""StringIO"", ""StringIO"", ""io""),
    MovedAttribute(""UserDict"", ""UserDict"", ""collections""),
    MovedAttribute(""UserList"", ""UserList"", ""collections""),
    MovedAttribute(""UserString"", ""UserString"", ""collections""),
    MovedAttribute(""xrange"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""zip"", ""itertools"", ""builtins"", ""izip"", ""zip""),
    MovedAttribute(""zip_longest"", ""itertools"", ""itertools"", ""izip_longest"", ""zip_longest""),
    MovedModule(""builtins"", ""__builtin__""),
    MovedModule(""configparser"", ""ConfigParser""),
    MovedModule(""collections_abc"", ""collections"", ""collections.abc"" if sys.version_info >= (3, 3) else ""collections""),
    MovedModule(""copyreg"", ""copy_reg""),
    MovedModule(""dbm_gnu"", ""gdbm"", ""dbm.gnu""),
    MovedModule(""dbm_ndbm"", ""dbm"", ""dbm.ndbm""),
    MovedModule(""_dummy_thread"", ""dummy_thread"", ""_dummy_thread"" if sys.version_info < (3, 9) else ""_thread""),
    MovedModule(""http_cookiejar"", ""cookielib"", ""http.cookiejar""),
    MovedModule(""http_cookies"", ""Cookie"", ""http.cookies""),
    MovedModule(""html_entities"", ""htmlentitydefs"", ""html.entities""),
    MovedModule(""html_parser"", ""HTMLParser"", ""html.parser""),
    MovedModule(""http_client"", ""httplib"", ""http.client""),
    MovedModule(""email_mime_base"", ""email.MIMEBase"", ""email.mime.base""),
    MovedModule(""email_mime_image"", ""email.MIMEImage"", ""email.mime.image""),
    MovedModule(""email_mime_multipart"", ""email.MIMEMultipart"", ""email.mime.multipart""),
    MovedModule(""email_mime_nonmultipart"", ""email.MIMENonMultipart"", ""email.mime.nonmultipart""),
    MovedModule(""email_mime_text"", ""email.MIMEText"", ""email.mime.text""),
    MovedModule(""BaseHTTPServer"", ""BaseHTTPServer"", ""http.server""),
    MovedModule(""CGIHTTPServer"", ""CGIHTTPServer"", ""http.server""),
    MovedModule(""SimpleHTTPServer"", ""SimpleHTTPServer"", ""http.server""),
    MovedModule(""cPickle"", ""cPickle"", ""pickle""),
    MovedModule(""queue"", ""Queue""),
    MovedModule(""reprlib"", ""repr""),
    MovedModule(""socketserver"", ""SocketServer""),
    MovedModule(""_thread"", ""thread"", ""_thread""),
    MovedModule(""tkinter"", ""Tkinter""),
    MovedModule(""tkinter_dialog"", ""Dialog"", ""tkinter.dialog""),
    MovedModule(""tkinter_filedialog"", ""FileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_scrolledtext"", ""ScrolledText"", ""tkinter.scrolledtext""),
    MovedModule(""tkinter_simpledialog"", ""SimpleDialog"", ""tkinter.simpledialog""),
    MovedModule(""tkinter_tix"", ""Tix"", ""tkinter.tix""),
    MovedModule(""tkinter_ttk"", ""ttk"", ""tkinter.ttk""),
    MovedModule(""tkinter_constants"", ""Tkconstants"", ""tkinter.constants""),
    MovedModule(""tkinter_dnd"", ""Tkdnd"", ""tkinter.dnd""),
    MovedModule(""tkinter_colorchooser"", ""tkColorChooser"",
                ""tkinter.colorchooser""),
    MovedModule(""tkinter_commondialog"", ""tkCommonDialog"",
                ""tkinter.commondialog""),
    MovedModule(""tkinter_tkfiledialog"", ""tkFileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_font"", ""tkFont"", ""tkinter.font""),
    MovedModule(""tkinter_messagebox"", ""tkMessageBox"", ""tkinter.messagebox""),
    MovedModule(""tkinter_tksimpledialog"", ""tkSimpleDialog"",
                ""tkinter.simpledialog""),
    MovedModule(""urllib_parse"", __name__ + "".moves.urllib_parse"", ""urllib.parse""),
    MovedModule(""urllib_error"", __name__ + "".moves.urllib_error"", ""urllib.error""),
    MovedModule(""urllib"", __name__ + "".moves.urllib"", __name__ + "".moves.urllib""),
    MovedModule(""urllib_robotparser"", ""robotparser"", ""urllib.robotparser""),
    MovedModule(""xmlrpc_client"", ""xmlrpclib"", ""xmlrpc.client""),
    MovedModule(""xmlrpc_server"", ""SimpleXMLRPCServer"", ""xmlrpc.server""),
]
# Add windows specific modules.
if sys.platform == ""win32"":
    _moved_attributes += [
        MovedModule(""winreg"", ""_winreg""),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
    if isinstance(attr, MovedModule):
        _importer._add_module(attr, ""moves."" + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + "".moves"")
_importer._add_module(moves, ""moves"")


class Module_six_moves_urllib_parse(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""SplitResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qs"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qsl"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urldefrag"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urljoin"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""quote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""quote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_to_bytes"", ""urllib"", ""urllib.parse"", ""unquote"", ""unquote_to_bytes""),
    MovedAttribute(""urlencode"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitquery"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splittag"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splituser"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitvalue"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""uses_fragment"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_netloc"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_params"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_query"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_relative"", ""urlparse"", ""urllib.parse""),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(Module_six_moves_urllib_parse(__name__ + "".moves.urllib_parse""),
                      ""moves.urllib_parse"", ""moves.urllib.parse"")


class Module_six_moves_urllib_error(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_error""""""


_urllib_error_moved_attributes = [
    MovedAttribute(""URLError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""HTTPError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""ContentTooShortError"", ""urllib"", ""urllib.error""),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(Module_six_moves_urllib_error(__name__ + "".moves.urllib.error""),
                      ""moves.urllib_error"", ""moves.urllib.error"")


class Module_six_moves_urllib_request(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_request""""""


_urllib_request_moved_attributes = [
    MovedAttribute(""urlopen"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""install_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""build_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""pathname2url"", ""urllib"", ""urllib.request""),
    MovedAttribute(""url2pathname"", ""urllib"", ""urllib.request""),
    MovedAttribute(""getproxies"", ""urllib"", ""urllib.request""),
    MovedAttribute(""Request"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""OpenerDirector"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDefaultErrorHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPRedirectHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPCookieProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""BaseHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgr"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgrWithDefaultRealm"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPSHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FileHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""CacheFTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""UnknownHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPErrorProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""urlretrieve"", ""urllib"", ""urllib.request""),
    MovedAttribute(""urlcleanup"", ""urllib"", ""urllib.request""),
    MovedAttribute(""URLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""FancyURLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""proxy_bypass"", ""urllib"", ""urllib.request""),
    MovedAttribute(""parse_http_list"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""parse_keqv_list"", ""urllib2"", ""urllib.request""),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(Module_six_moves_urllib_request(__name__ + "".moves.urllib.request""),
                      ""moves.urllib_request"", ""moves.urllib.request"")


class Module_six_moves_urllib_response(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_response""""""


_urllib_response_moved_attributes = [
    MovedAttribute(""addbase"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addclosehook"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfo"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfourl"", ""urllib"", ""urllib.response""),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(Module_six_moves_urllib_response(__name__ + "".moves.urllib.response""),
                      ""moves.urllib_response"", ""moves.urllib.response"")


class Module_six_moves_urllib_robotparser(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_robotparser""""""


_urllib_robotparser_moved_attributes = [
    MovedAttribute(""RobotFileParser"", ""robotparser"", ""urllib.robotparser""),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes

_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + "".moves.urllib.robotparser""),
                      ""moves.urllib_robotparser"", ""moves.urllib.robotparser"")


class Module_six_moves_urllib(types.ModuleType):

    """"""Create a six.moves.urllib namespace that resembles the Python 3 namespace""""""
    __path__ = []  # mark as package
    parse = _importer._get_module(""moves.urllib_parse"")
    error = _importer._get_module(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
    response = _importer._get_module(""moves.urllib_response"")
    robotparser = _importer._get_module(""moves.urllib_robotparser"")

    def __dir__(self):
        return ['parse', 'error', 'request', 'response', 'robotparser']

_importer._add_module(Module_six_moves_urllib(__name__ + "".moves.urllib""),
                      ""moves.urllib"")


def add_move(move):
    """"""Add an item to six.moves.""""""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """"""Remove item from six.moves.""""""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError(""no such move, %r"" % (name,))


","if PY3:
    _meth_func = ""__func__""
    _meth_self = ""__self__""

    _func_closure = ""__closure__""
    _func_code = ""__code__""
    _func_defaults = ""__defaults__""
    _func_globals = ""__globals__""
else:
    _meth_func = ""im_func""
    _meth_self = ""im_self""

    _func_closure = ""func_closure""
    _func_code = ""func_code""
    _func_defaults = ""func_defaults""
    _func_globals = ""func_globals""


try:
","    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any(""__call__"" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """"""Get the function out of a possibly unbound function"""""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:
    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller(""keys"")

    viewvalues = operator.methodcaller(""values"")

    viewitems = operator.methodcaller(""items"")
else:
    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller(""viewkeys"")

    viewvalues = operator.methodcaller(""viewvalues"")

    viewitems = operator.methodcaller(""viewitems"")

_add_doc(iterkeys, ""Return an iterator over the keys of a dictionary."")
_add_doc(itervalues, ""Return an iterator over the values of a dictionary."")
_add_doc(iteritems,
         ""Return an iterator over the (key, value) pairs of a dictionary."")
_add_doc(iterlists,
         ""Return an iterator over the (key, [values]) pairs of a dictionary."")


if PY3:
    def b(s):
        return s.encode(""latin-1"")

    def u(s):
        return s
    unichr = chr
    import struct
    int2byte = struct.Struct("">B"").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = ""assertCountEqual""
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = ""assertRaisesRegexp""
        _assertRegex = ""assertRegexpMatches""
        _assertNotRegex = ""assertNotRegexpMatches""
    else:
        _assertRaisesRegex = ""assertRaisesRegex""
        _assertRegex = ""assertRegex""
        _assertNotRegex = ""assertNotRegex""
else:
    def b(s):
        return s
    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r'\\', r'\\\\'), ""unicode_escape"")
    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])
    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = ""assertItemsEqual""
    _assertRaisesRegex = ""assertRaisesRegexp""
    _assertRegex = ""assertRegexpMatches""
    _assertNotRegex = ""assertNotRegexpMatches""
_add_doc(b, """"""Byte literal"""""")
_add_doc(u, """"""Text literal"""""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, ""exec"")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """"""Execute code in a namespace.""""""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(""""""exec _code_ in _globs_, _locs_"""""")

    exec_(""""""def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""""")


if sys.version_info[:2] > (3,):
    exec_(""""""def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""""")
else:
    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, ""print"", None)
if print_ is None:
    def print_(*args, **kwargs):
        """"""The new-style print function for Python 2.4 and 2.5.""""""
        fp = kwargs.pop(""file"", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError(""sep must be None or a string"")
        end = kwargs.pop(""end"", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError(""end must be None or a string"")
        if kwargs:
            raise TypeError(""invalid keyword arguments to print()"")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode(""\n"")
            space = unicode("" "")
        else:
            newline = ""\n""
            space = "" ""
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get(""file"", sys.stdout)
        flush = kwargs.pop(""flush"", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()

_add_doc(reraise, """"""Reraise an exception."""""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(wrapper, wrapped,
                        assigned=functools.WRAPPER_ASSIGNMENTS,
                        updated=functools.WRAPPER_UPDATES):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper
    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
              updated=functools.WRAPPER_UPDATES):
        return functools.partial(_update_wrapper, wrapped=wrapped,
                                 assigned=assigned, updated=updated)
    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):

        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d['__orig_bases__'] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})


def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get('__slots__')
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        if hasattr(cls, '__qualname__'):
            orig_vars['__qualname__'] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper


def ensure_binary(s, encoding='utf-8', errors='strict'):
    """"""Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """"""
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError(""not expecting type '%s'"" % type(s))


def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    return s


def ensure_text(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError(""not expecting type '%s'"" % type(s))


def python_2_unicode_compatible(klass):
    """"""
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """"""
    if PY2:
        if '__str__' not in klass.__dict__:
            raise ValueError(""@python_2_unicode_compatible cannot be applied ""
                             ""to %s because it doesn't define __str__()."" %
                             klass.__name__)
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get(""__spec__"") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another ""instance"" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (type(importer).__name__ == ""_SixMetaPathImporter"" and
                importer.name == __name__):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)
","``
    # attribute at the top so that we can safely call this function instead of
    # passing it directly to python's built-in modules.
    update_wrapper = importlib.util.update_wrapper
else:
    update_wrapper = functools.update_wrapper
    def get_child(cls, key):
        return moves.__dict__.setdefault(key, _MovedItem())


_MovedItem = TypeVar(""_MovedItem"", bound=""MovedObject"")
_MovedObjects = Dict[_MovedItem, _MovedItem]
_MovedFunctions = Tuple[""MovedFunction"",...]
_MovedClassMethods"
../airflow/dags/utils/pdf_utils.py,"import os
from pdf2image import convert_from_path
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, get_pdf_item_from_dynamodb
import logging

TEMP_DIR = '/tmp'  # Modify if needed for your environment
PDF_S3_PATH = 'pdfs'  # Define the S3 directory where your PDF files are stored
PAGES_S3_PATH = 'pages/valid'  # Directory in S3 where pages are uploaded
poppler_path = ""/usr/bin""

def split_pdf_to_pages(filename, shop_name):
    """"""Split PDF into pages and upload to S3, returning full S3 paths for the pages.""""""
    if not filename or not shop_name:
        raise Exception(""Filename or Shop Name missing!"")

    # Fetch metadata from DynamoDB
    response = get_pdf_item_from_dynamodb(filename, shop_name)
    file_entry = response.get('Item')

    if not file_entry:
        raise Exception(f""File {filename} not found in DynamoDB"")

    # Check if the pages already exist in S3
    page_s3_paths = []  # Store full S3 paths for pages
    base_filename = os.path.splitext(filename)[0]

    logging.info(f""Checking if pages for {filename} already exist in S3..."")

    # Define the path for the PDF in S3 (in the 'pdfs' directory)
    s3_pdf_path = f'{PDF_S3_PATH}/{filename}'

    # Download the PDF from S3 to a temporary location
","    file_path = os.path.join(TEMP_DIR, filename)

    # Log paths for debugging
    logging.info(f""Checking if file exists in S3 path: {s3_pdf_path}"")

    logging.info(f""Downloading file from S3 path: {s3_pdf_path} to local path: {file_path}"")

    try:
        download_file_from_s3(s3_pdf_path, file_path)
    except Exception as e:
        logging.error(f""Failed to download file from S3: {e}"")
        raise e

    # Convert PDF into image pages
    images = convert_from_path(file_path, dpi=250, poppler_path=poppler_path)

","    for i, image in enumerate(images):
        page_filename = f""{base_filename}_page_{i + 1}.png""
        page_path = os.path.join(TEMP_DIR, page_filename)

        # Save the image locally
        image.save(page_path, 'PNG')

        # Upload each page to S3 in the 'pages/valid/' directory
        s3_page_path = f'{PAGES_S3_PATH}/{page_filename}'
        upload_file_to_s3(page_path, s3_page_path)

        # Add full S3 path of the page to the list
        page_s3_paths.append(s3_page_path)

    # Return the list of full S3 paths for the uploaded pages
    return page_s3_paths
","    try:
        with open(os.path.join(tempfile.gettempdir(), ""upload.txt""), ""w+"") as f:
            f.write(str(response['Body'])))

            # Upload this PDF item to S3 in the 'pages/valid/' directory
            response = download_file_from_s3(s3_pdf_path, TEMP_DIR)
            upload_file_to_s3(s3_pdf_path, response['Body'])

            # Update the original filename so that we don't overwrite the old one
            filename = os.path.basename("
../airflow/dags/data_pipeline.py,"import json
import ast
import logging
import os
import subprocess
import time

import boto3
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from utils.pdf_utils import split_pdf_to_pages
from utils.yolo_ocr_utils import run_yolo_on_pages, got_text_from_image
from utils.correct_names import process_single_word, Trie, preprocess_text
from airflow.utils.trigger_rule import TriggerRule
from utils.price_processing import process_price_by_class_id
from utils.s3_dynamodb_utils import save_item_to_dynamodb, download_file_from_s3
from airflow.operators.python import BranchPythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.models.variable import Variable
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

region = 'eu-west-1'
instances = ['i-09234cbd67d00b0ce']
ec2 = boto3.client('ec2', region_name=region)

# Constants for directories
PAGES_S3_PATH = 'pages/valid'
TEMP_DIR = '/tmp'
ITEMS_S3_DIR = 'item_detected/images/valid'
DETECTIONS_S3_DIR = 'item_detected/valid'

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

AIRFLOW_HOME = os.getenv('AIRFLOW_HOME', '/home/ubuntu/airflow')
file_path = os.path.join(AIRFLOW_HOME, 'dags/utils/item_names/unique_item_names.txt')


def run_ec2_instances(**kwargs):
    ec2.start_instances(InstanceIds=instances)
    print('started your instances: ' + str(instances))
    time.sleep(60)
    return ""EC2 started""

# Function to delete SageMaker endpoints
def stop_ec2_instances(**kwargs):
    ec2.stop_instances(InstanceIds=instances)
    print('stopped your instances: ' + str(instances))
    time.sleep(60)
    return ""EC2 stopped""

def yolo_on_pages(page_filenames, **kwargs):
    """"""
    Processes YOLO predictions, extracts images, and uploads results to S3.

    Args:
        page_filenames (list or str): List of page filenames or string representation of a list.

    Returns:
        dict: Contains saved S3 paths for predictions and images.
    """"""
    # Convert page_filenames to list if it's a string
    if isinstance(page_filenames, str):
        try:
            page_filenames = ast.literal_eval(page_filenames)
        except (ValueError, SyntaxError):
            raise ValueError(f""Invalid input for page_filenames: {page_filenames}"")

    # Run YOLO on pages
    predictions, s3_saved_images = run_yolo_on_pages(page_filenames, ""item_detection_data"",
                                                     save_images=True, model='model1',
                                                     detection_output_path=DETECTIONS_S3_DIR)
    return json.dumps({'predictions': predictions, 'saved_images': s3_saved_images})


def process_detected_items_step(detection_data, shop_name, valid, **kwargs):
    """"""
    Processes detected items, runs YOLO Model 2, performs OCR, and saves results in DynamoDB.

    Args:
        detection_data (str): Contains S3 paths to detected images and `.txt` files.
        shop_name (str): Name of the shop.

    Returns:
        list: Processed items with OCR text and Model 2 detection results.
    """"""
    logger.info(""Starting process_detected_items_step"")
    # Load and preprocess item names for Trie
    with open(file_path, 'r', encoding='utf-8') as f:
        item_names = f.readlines()
        words = [preprocess_text(line).split() for line in item_names]
        flat_words = [word for sublist in words for word in sublist]

    # Initialize Trie with item names
    trie = Trie()
    for word in flat_words:
        trie.insert(word)

    # Parse detection_data string to dictionary
    try:
        detection_data = ast.literal_eval(detection_data)
    except Exception as e:
        logger.error(f""Error parsing detection_data: {e}"")
        raise

    saved_images = detection_data.get('saved_images', [])
    processed_items = []

    if not saved_images:
        logger.info(""No images to process."")
        return processed_items

    try:
        # Run YOLO Model 2 on images and perform OCR
        predictions, s3_saved_images = run_yolo_on_pages(saved_images, ""item_processing_data"",
                                                         model='model2', include_ocr=True)

        # Process detections for each image
        for s3_image_path, detected_object_data in predictions.items():
            try:
                # Download image from S3
                local_image_path = os.path.join(TEMP_DIR, os.path.basename(s3_image_path))
                download_file_from_s3(s3_image_path, local_image_path)

                # Perform OCR on the whole image
                whole_image_text = got_text_from_image(local_image_path)
                os.remove(local_image_path)  # Clean up local file

            except Exception as e:
                logger.error(f""Error processing image {s3_image_path}: {e}"")

            # Initialize detection fields
            object_name = processed_item_name = item_price = processed_item_price = None
            item_member_price = processed_item_member_price = item_initial_price = processed_item_initial_price = None

            # Process detections based on class IDs
            for detection in detected_object_data:
                class_id = detection['class_name']
                ocr_text = detection.get('ocr_text', '')

                if class_id == 'item_name':
                    object_name = ocr_text
                    processed_item_name = process_single_word(ocr_text, trie)
                elif class_id in ['item_price', 'item_member_price', 'item_initial_price']:
                    processed_price = process_price_by_class_id(shop_name, ocr_text, class_id)
                    if class_id == 'item_price':
                        item_price, processed_item_price = ocr_text, processed_price
                    elif class_id == 'item_member_price':
                        item_member_price, processed_item_member_price = ocr_text, processed_price
                    elif class_id == 'item_initial_price':
                        item_initial_price, processed_item_initial_price = ocr_text, processed_price

            # Create detected object data for DynamoDB
            detected_object = {
                ""image_id"": s3_image_path,
                ""item_name"": object_name,
                ""processed_item_name"": processed_item_name,
                ""whole_image_ocr_text"": whole_image_text,
                ""model2_detections"": detected_object_data,
                ""shop_name"": shop_name,
                ""item_price"": item_price,
                ""processed_item_price"": str(processed_item_price),
                ""item_member_price"": item_member_price,
                ""processed_item_member_price"": str(processed_item_member_price),
                ""item_initial_price"": item_initial_price,
                ""processed_item_initial_price"": str(processed_item_initial_price),
                ""valid"": valid == ""True""
            }

            # Save the object to DynamoDB
            save_item_to_dynamodb(""detected_data"", detected_object)
            processed_items.append(detected_object)

    except Exception as e:
        logger.error(f""Error processing images: {e}"")

    logger.info(""Finished processing all detected images"")
    return processed_items


# Function to check if models are already deployed
def check_models_deployed(**kwargs):
    deployed = Variable.get('models_deployed', default_var=False)
    logger.info(f""Current value of models_deployed: {deployed}"")
    if deployed == 'True':
        return ""skip_deploy_models""
    else:
        set_deployed_flag()
        return ""deploy_models""


def set_deployed_flag():
    Variable.update(""models_deployed"", True)

def reset_deployed_flag():
    Variable.set(""models_deployed"", False)

# Define the DAG and tasks
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

dag = DAG('pages_data_pipeline', default_args=default_args, schedule_interval=None, max_active_tasks=1)


def log_params(**context):
    reset_deployed_flag()
    filename = context['dag_run'].conf.get('filename', '')
    shop_name = context['dag_run'].conf.get('shop_name', '')
    logger.info(f""Filename: {filename}, Shop Name: {shop_name}"")
    return filename, shop_name


with (dag):
    # Task to log parameters
    log_task = PythonOperator(
        task_id='log_params',
        python_callable=log_params,
        provide_context=True,
        dag=dag
    )

    # Task to check if models are deployed
    check_deploy_task = BranchPythonOperator(
        task_id='check_deploy_task',
        python_callable=check_models_deployed,
        dag=dag
    )

    # Task to skip deployment if models are already deployed
    skip_deploy_models = PythonOperator(
        task_id='skip_deploy_models',
        python_callable=lambda: time.sleep(60),
        dag=dag
    )

    # Task to deploy models
    deploy_models_task = PythonOperator(
        task_id='deploy_models',
        python_callable=run_ec2_instances,
        dag=dag
    )

    # Task to split the PDF
    split_task = PythonOperator(
        task_id='split_pdf',
        python_callable=split_pdf_to_pages,
        op_kwargs={
            'filename': '{{ dag_run.conf[""filename""] }}',
            'shop_name': '{{ dag_run.conf[""shop_name""] }}',
            'valid': '{{ dag_run.conf[""valid""] }}'
        },
        dag=dag
    )

    # Task to detect items using YOLO
    detect_items_task = PythonOperator(
        task_id='yolo_on_pages',
        python_callable=yolo_on_pages,
        op_kwargs={'page_filenames': '{{ ti.xcom_pull(task_ids=""split_pdf"") }}'},
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to process detected items and save them to DynamoDB
    process_task = PythonOperator(
        task_id='process_detected_items',
        python_callable=process_detected_items_step,
        op_kwargs={
            'detection_data': '{{ ti.xcom_pull(task_ids=""yolo_on_pages"") }}',
            'shop_name': '{{ dag_run.conf[""shop_name""] }}',
            'valid': '{{ dag_run.conf[""valid""] }}'
        },
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to wait for other pipelines
","    wait_for_others_task = ExternalTaskSensor(
        task_id='wait_for_other_pipelines',
        external_dag_id='pages_data_pipeline',  # Assuming other pipelines have the same DAG ID
        external_task_id='process_detected_items',  # Wait for this task in all other DAGs
        mode='poke',  # Can be 'reschedule' to avoid loading the database
        poke_interval=10,  # Check every 30 seconds
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        dag=dag
    )

    # Task to delete endpoints at the end
    delete_endpoints_task = PythonOperator(
        task_id='delete_endpoints',
        python_callable=stop_ec2_instances,
        trigger_rule=TriggerRule.ALL_DONE,  # Only run if all upstream tasks succeed
        dag=dag
    )

    trigger_other_pipeline_task = TriggerDagRunOperator(
        task_id='trigger_other_pipeline',
        trigger_dag_id='check_file_validity_and_update_detected_items',
        dag=dag
    )

    # Task dependencies
    log_task >> split_task >> check_deploy_task
    check_deploy_task >> [skip_deploy_models, deploy_models_task]
    deploy_models_task >> detect_items_task
","    skip_deploy_models >> detect_items_task
    detect_items_task >> process_task >> wait_for_others_task >> delete_endpoints_task >> trigger_other_pipeline_task
","        # When we don't have any jobs available for this pipeline then the operator will start
    wait_for_others_task = PythonOperator(
        task_id='wait_for_others',
        python_callable=lambda: stop_ec2_instances(),
        op_kwargs={},
        trigger_rule=TriggerRule.ALL_SUCCESS,
        dag=dag
    )

    # Delete endpoints that need to be deleted by our AWS Lambda service
    delete_endpoint_task = PythonOperator(
        task_id='delete_endpoint',
        python_callable=lambda_function,
        op_kwargs={}
    )

   "
../airflow/dags/validity_check.py,"import itertools

from airflow.sensors.external_task import ExternalTaskSensor
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from utils.s3_dynamodb_utils import update_item_in_dynamodb, download_file_from_s3
import boto3
import requests
from datetime import datetime

# DynamoDB Table Details
TABLE_NAME = 'pdf_metadata'
DETECTED_TABLE = 'detected_data'
USERS_TABLE = 'user_preferences'
dynamodb = boto3.resource('dynamodb')

pdf_table = dynamodb.Table(TABLE_NAME)
users_table = dynamodb.Table(USERS_TABLE)
detected_table = dynamodb.Table(DETECTED_TABLE)

WEBHOOK_URL = ""https://klffswbb85.execute-api.eu-west-1.amazonaws.com/default/salesTelegramBotHandler""


# Function to check file validity and update detected items only if the status changes
def check_validity_and_update_detected():
    # Initialize DynamoDB clients

    # Get today's date
    today = datetime.utcnow().date()

    changed_to_valid = []
    changed_to_invalid = []

    # Scan the pdf_metadata table to get all items
    pdf_response = pdf_table.scan()
    pdf_items = pdf_response.get('Items', [])

    # Process each PDF item
    for pdf_item in pdf_items:
        # Remove .pdf from filename for comparison with detected items
        pdf_name_without_ext = pdf_item['filename'].replace('.pdf', '')

        valid_from = datetime.strptime(pdf_item['valid_from'], '%Y-%m-%d').date()
        valid_to = datetime.strptime(pdf_item['valid_to'], '%Y-%m-%d').date()

        # Determine the current validity status
        current_valid_status = pdf_item.get('valid', None)

        # Check if the file should be valid or invalid based on today's date
        is_valid_now = valid_from <= today <= valid_to

        # If the current status differs from the computed status, it means the status has changed
        if current_valid_status != is_valid_now:
            # Update the valid field in the pdf_metadata table using the utility method
            update_item_in_dynamodb(
                table_name=TABLE_NAME,
                key={'filename': pdf_item['filename'], 'shop_name': pdf_item[""shop_name""]},
                update_expression=""SET valid = :v"",
                expression_attribute_values={':v': is_valid_now}
            )

            # Append the filename to the appropriate list based on the new validity status
            if is_valid_now:
                changed_to_valid.append(pdf_name_without_ext)
            else:
                changed_to_invalid.append(pdf_name_without_ext)

    # Return only files that changed their validity status
    return {
        'valid': changed_to_valid,
        'invalid': changed_to_invalid
    }


def update_detected_items_task(**context):
    """"""
    Task to update detected items based on the output from the previous task.
    """"""
    # Get the valid and invalid files from the context (returned by the previous task)
    task_instance = context['task_instance']

    # Pull the whole dictionary returned by the previous task
    status_change = task_instance.xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' and 'invalid' lists from the returned dictionary
    valid_files = status_change.get('valid', [])
    invalid_files = status_change.get('invalid', [])

    print(f""Debug: valid_files from XCom: {valid_files}"")
    print(f""Debug: invalid_files from XCom: {invalid_files}"")

    if not valid_files and not invalid_files:
        print(""Debug: No files were found for update."")
    else:
        # Update the detected_items table based on valid and invalid files
        update_detected_items_based_on_status(valid_files, invalid_files)


def update_detected_items_based_on_status(valid_files, invalid_files):
    """"""
    Update the detected_items table based on the lists of files that changed status (valid or invalid).
    This function optimizes the update by filtering detected items only with relevant file substrings.
    """"""

    # Remove .pdf extensions from filenames
    valid_files = [file.replace('.pdf', '') for file in valid_files]
    invalid_files = [file.replace('.pdf', '') for file in invalid_files]

    # Combine valid and invalid files
    all_files = valid_files + invalid_files
    print(f""Debug: all_files to be checked (without .pdf): {all_files}"")

    valid_changed_items = []

    detected_response = detected_table.scan()
    detected_items = detected_response.get('Items', [])

    print(f""Debug: Detected items retrieved from DynamoDB: {detected_items}"")

    # Process each detected item
    for detected_item in detected_items:
        detected_image_path = detected_item['image_id']

        # Debug: Show the current detected image path
        print(f""Debug: Processing detected item with image_id: {detected_image_path}"")

        # Check if any file name (without .pdf) is a substring of the image_id
","        for file_substr in all_files:
            if file_substr in detected_image_path:
                new_valid_status = file_substr in valid_files

                # Debug: Show the new validity status for the detected item
","                print(f""Debug: Changing validity of {detected_image_path} to {new_valid_status}"")

                if detected_item.get('valid') != new_valid_status:
                    # Update the valid field in DynamoDB
                    update_item_in_dynamodb(
                        table_name=DETECTED_TABLE,
                        key={'image_id': detected_item['image_id']},
                        update_expression=""SET valid = :v"",
                        expression_attribute_values={':v': new_valid_status}
                    )
                    print(f""Debug: Updated detected item {detected_item['image_id']} validity to {new_valid_status}"")

                    if new_valid_status:
                        valid_changed_items.append(detected_item)

    print(f""Debug: List of valid_changed_items: {valid_changed_items}"")

    return valid_changed_items


def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    pdf_table = dynamodb.Table(TABLE_NAME)
    response = pdf_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


def regroup_by_shop():
    """"""
    Iterates over all users and creates two columns:
    1. Users for each shop with included shops (all_shops - excluded_shops).
    2. Users for each shop with included shops and receive_pdf_enabled set to True.
    """"""
    # Dictionary to hold the regrouped data
    shop_user_map = {
        ""included_shops"": {},
        ""included_shops_and_receive_pdf"": {}
    }

    # Retrieve all unique shops from the metadata
    all_shops = get_all_shops()

    # Scan the users table to get all user records
    response = users_table.scan()

    # Iterate over all users (items in DynamoDB)
    for item in response['Items']:
        chat_id = item['chat_id']
        receive_pdf_enabled = item.get('receive_pdf_enabled', False)  # Default to False if not set
        excluded_shops = item.get('excluded_shops', [])

        # Calculate included shops (all_shops - excluded_shops)
        included_shops = [shop for shop in all_shops if shop not in excluded_shops]

        # Iterate over all included shops and add users to corresponding shop keys
        for shop in included_shops:
            # Add to ""included_shops"" column (all users with included shops)
            if shop not in shop_user_map[""included_shops""]:
                shop_user_map[""included_shops""][shop] = []
            shop_user_map[""included_shops""][shop].append(chat_id)

            # Add to ""included_shops_and_receive_pdf"" column (users with included shops AND receive_pdf_enabled=True)
            if receive_pdf_enabled:
                if shop not in shop_user_map[""included_shops_and_receive_pdf""]:
                    shop_user_map[""included_shops_and_receive_pdf""][shop] = []
                shop_user_map[""included_shops_and_receive_pdf""][shop].append(chat_id)

    # Return the regrouped data structure
    return shop_user_map


def regroup_shop_to_valid_file(valid_files):
    shop_file_map = {}

    # Ensure valid_files have no .pdf extension
    valid_files = [file.replace('.pdf', '') for file in valid_files]

    # Scan the pdf metadata table
    response = pdf_table.scan()

    # Process each item in the table
    for item in response['Items']:
        shop_name = item['shop_name']
        file_name = item['filename'].replace('.pdf', '')  # Remove .pdf for comparison

        # Check if the file name matches any valid file (without .pdf)
        if file_name in valid_files:
            if shop_name not in shop_file_map:
                shop_file_map[shop_name] = []
            shop_file_map[shop_name].append(item['filename'])  # Use original filename with .pdf for sending

    return shop_file_map


def send_webhook(process_type, shop_name, users_id_list, pdf_file, tracked_item=None):
    """"""
    Sends a POST request to the specified webhook URL with the provided data.
    """"""
    payload = {
        'process_type': process_type,
        'shop_name': shop_name,
        'users_id_list': users_id_list,
        'pdf_file': pdf_file,
    }
    if tracked_item:
        payload['tracked_items_list'] = tracked_item  # Include the tracked item if provided

    # Send the POST request to the webhook
    response = requests.post(WEBHOOK_URL, json=payload)

    # Log the response (optional)
    if response.status_code == 200:
        if tracked_item:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}, tracked_item: {tracked_item}"")
        else:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}"")
    else:
        print(f""Failed to send data to webhook: {response.status_code}, {response.text}"")


def send_updates_in_telegram_task(**context):
    # Pull the whole dictionary returned by the previous task
    status_change = context['task_instance'].xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' files from the returned dictionary
    valid_files = status_change.get('valid', [])

    # Debug print to check what valid files were pulled
    print(f""Debug: valid_files from XCom: {valid_files}"")

    # If no valid files, skip further processing
    if not valid_files:
        print(""Debug: No valid files to process."")
        return

    # Regroup data by shop
    regrouped_data = regroup_by_shop()
    print(f""Debug: regrouped_data: {regrouped_data}"")

    # Regroup valid files by shop
    regrouped_valid_files_data = regroup_shop_to_valid_file(valid_files)
    print(f""Debug: regrouped_valid_files_data: {regrouped_valid_files_data}"")

    for shop_name in regrouped_data['included_shops']:
        shop_included_users_with_receive_pdf = regrouped_data['included_shops_and_receive_pdf'].get(shop_name, [])
        shop_included_users = regrouped_data['included_shops'].get(shop_name, [])

        if shop_name in regrouped_valid_files_data:
            for pdf_file in regrouped_valid_files_data[shop_name]:
                # Debug print before sending the pdf_newsletter webhook
                print(f""Debug: Sending pdf_newsletter for shop: {shop_name}, pdf_file: {pdf_file}, ""
                      f""users_with_receive_pdf: {shop_included_users_with_receive_pdf}"")

                send_webhook('pdf_newsletter', shop_name, shop_included_users_with_receive_pdf, pdf_file)

                batch_size = 3
                user_batches = [list(group) for group in
                                itertools.zip_longest(*[iter(shop_included_users)] * batch_size)]

                for user_batch in user_batches:
                    valid_users = [user for user in user_batch if user is not None]
                    user_ids_list = []
                    user_tracked_items_list = []

                    for user_id in valid_users:
                        response = users_table.get_item(Key={'chat_id': user_id})
                        if 'Item' in response:
                            user_tracked_items = response['Item'].get('tracked_items', [])
                            if user_tracked_items:
                                user_ids_list.append(user_id)
                                user_tracked_items_list.append(user_tracked_items)

                    if user_ids_list and user_tracked_items_list:
                        # Debug print before sending the tracked_items_list webhook
                        print(f""Debug: Sending tracked_items_list for shop: {shop_name}, pdf_file: {pdf_file}, ""
                              f""user_ids_list: {user_ids_list}, tracked_items: {user_tracked_items_list}"")

                        send_webhook('tracked_items_list', shop_name, user_ids_list, pdf_file, user_tracked_items_list)


# Airflow DAG setup
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
}

with DAG(
        dag_id='check_file_validity_and_update_detected_items',
        default_args=default_args,
        schedule_interval='0 1 * * *',  # Runs at 08:00 AM UTC daily
        catchup=False
) as dag:
    # Task to check and update the PDF metadata validity
    check_files_and_update_detected = PythonOperator(
        task_id='check_validity_and_update_detected_task',
        python_callable=check_validity_and_update_detected,
    )

    # Task to update the detected items based on the result of the first task
    update_detected_items = PythonOperator(
        task_id='update_detected_items_task',
        python_callable=update_detected_items_task,
        provide_context=True
    )

    # Task to send updates in Telegram after updating detected items
    send_updates_in_telegram = PythonOperator(
        task_id='send_updates_in_telegram_task',
        python_callable=send_updates_in_telegram_task,
        provide_context=True
    )

    # Task dependencies
    check_files_and_update_detected >> update_detected_items >> send_updates_in_telegram
","        if detected_image_path.startswith(pdf_item['image_id']):
            # The file was detected successfully
            pass
        elif detected_image_path.endswith('.pdf'):
            # The file was detected successfully but with a different file type
            pass
        else:
            # The file was detected successfully but without an extention, so ignore it
            pass

    # Add the new item to the detected_table database
    if valid_files:
        # Create the list of images we need to use when checking the validity of the detected file
        img_urls = list(set([img['url'] for img"
../airflow/dags/utils/s3_dynamodb_utils.py,"import boto3

# Initialize AWS S3 and DynamoDB clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Retrieve an item from the DynamoDB table based on filename and shop_name
def get_pdf_item_from_dynamodb(filename, shop_name, table=None, table_name=""pdf_metadata""):
    """"""
    Retrieve an item from a specified DynamoDB table based on filename and shop_name.
    """"""
    # Use the provided table instance if given; otherwise, access by table_name
    table = table or dynamodb.Table(table_name)
    return table.get_item(Key={'filename': filename, 'shop_name': shop_name})

# Function to download a file from an S3 bucket to a local path
def download_file_from_s3(filename_path, local_path, bucket_name=""salestelegrambot""):
    """"""
    Download a file from an S3 bucket.
    """"""
    try:
        s3.download_file(bucket_name, filename_path, local_path)
        print(f""Downloaded {filename_path} to {local_path}"")
    except Exception as e:
        print(f""Error downloading {filename_path}: {e}"")

# Function to upload a file from a local path to an S3 bucket
def upload_file_to_s3(local_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Upload a file to an S3 bucket.
    """"""
    try:
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/{s3_path}"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")

# Function to save an item to a DynamoDB table
def save_item_to_dynamodb(table_name, item, table=None):
    """"""
    Save an item to a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.put_item(Item=item)
        print(f""Saved item {item} to DynamoDB table {table_name}"")
    except Exception as e:
        print(f""Error saving item to DynamoDB: {e}"")

# Function to update specific fields of an item in DynamoDB
def update_item_in_dynamodb(table_name, key, update_expression, expression_attribute_values, table=None):
    """"""
    Update specific fields of an item in a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.update_item(
            Key=key,
            UpdateExpression=update_expression,
            ExpressionAttributeValues=expression_attribute_values
        )
","        print(f""Updated item in table {table_name} with key {key}"")
    except Exception as e:
","        print(f""Error updating item in DynamoDB: {e}"")
","    except Exception as e:
"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/lambda_function.py,"import ast
import copy
import re
import tempfile
import boto3
import requests
import json
import os
from boto3.dynamodb.conditions import Attr
import logging
from fuzzywuzzy import fuzz
from itertools import islice

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger()


# Function to initialize global constants and resources
def initialize_globals():
    global BUCKET_NAME, TOKEN, API_URL, dynamodb, s3, user_preferences_table, pdf_metadata_table, detected_data_table

    BUCKET_NAME = os.environ.get('BUCKET_NAME')
    TOKEN = os.environ.get('TOKEN')
    API_URL = f""https://api.telegram.org/bot{TOKEN}""

    # Initialize AWS resources
    dynamodb = boto3.resource('dynamodb')
    s3 = boto3.client('s3')

    # DynamoDB table references
    user_preferences_table = dynamodb.Table('user_preferences')
    pdf_metadata_table = dynamodb.Table('pdf_metadata')
    detected_data_table = dynamodb.Table(""detected_data"")


# Call the initialization function
initialize_globals()


# --------------- AWS Handling ---------------
def download_file_from_s3(filename_path, local_path):
    """"""
    Downloads a file from the specified S3 bucket to a local file path.

    :param filename_path: The path of the file in the S3 bucket.
    :param local_path: The local path where the file should be saved.

    :raises ValueError: If the provided filename_path is not a valid string.
    """"""
    # Ensure the S3 file path is a valid string before proceeding with the download
    if not isinstance(filename_path, str) or not filename_path:
        raise ValueError(f""Invalid S3 filename path: {filename_path}"")

    # Log the download operation for debugging purposes
    logger.debug(f""Downloading file from S3 bucket: {filename_path} to local path: {local_path}"")

    # Perform the file download from the specified S3 bucket to the local path
    s3.download_file(BUCKET_NAME, filename_path, local_path)


# --------------- User Preferences Handling ---------------

def get_user_preferences(chat_id):
    """"""
    Retrieves user preferences from DynamoDB for the given chat_id.
    If no preferences are found, it returns a default preference with 'new_user' state.
    """"""
    response = user_preferences_table.get_item(Key={'chat_id': str(chat_id)})
    return response.get('Item', {""state"": ""new_user""})


def save_user_preferences(chat_id, preferences):
    """"""
    Saves or updates user preferences in DynamoDB for the given chat_id.
    """"""
    user_preferences_table.put_item(Item={
        'chat_id': str(chat_id),
        **preferences
    })


def get_user_language(chat_id):
    """"""
    Retrieves the language preference of the user from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('language', None)


def set_user_language(chat_id, language_code):
    """"""
    Sets the language preference for the user and saves it in DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['language'] = language_code
    save_user_preferences(chat_id, preferences)


def save_user_state(chat_id, state):
    """"""
    Saves the user's current interaction state in DynamoDB, allowing for persistence of state across interactions.

    :param chat_id: The chat ID of the user.
    :param state: The interaction state (e.g., menu state, ongoing search, etc.) to be saved.
    """"""
    preferences = get_user_preferences(chat_id)  # Retrieve the current preferences for the user
    preferences['state'] = state  # Set the new state for the user
    save_user_preferences(chat_id, preferences)  # Save the updated preferences (including the state) to DynamoDB


def get_user_state(chat_id):
    """"""
    Retrieves the user's current interaction state from DynamoDB.

    :param chat_id: The chat ID of the user.
    :return: The current state of the user, or None if no state is set.
    """"""
    preferences = get_user_preferences(chat_id)  # Fetch the user preferences from DynamoDB
    return preferences.get('state', None)  # Return the current state, or None if no state is set


# --------------- Shop Selection History Handling ---------------

def save_user_selected_shops_history(chat_id, shops):
    """"""
    Saves the user's selected shop history in DynamoDB.
    Ensures the history only contains the 10 most recent entries.
    """"""
    preferences = get_user_preferences(chat_id)
    history = preferences.setdefault('selected_shops_history', [])

    if shops not in history:
        history.append(copy.deepcopy(shops))
        if len(history) > 10:  # Keep history to last 10 items
            history.pop(0)

    preferences['selected_shops_history'] = history
    save_user_preferences(chat_id, preferences)


def get_user_selected_shops_history(chat_id):
    """"""
    Retrieves the user's selected shops history from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('selected_shops_history', [])


# --------------- Tracked Items Handling ---------------


def get_tracked_items(chat_id):
    """"""
    Retrieves the list of items the user is tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('tracked_items', [])


def add_tracked_item(chat_id, item_name):
    """"""
    Adds an item to the user's tracking list, if it's not already being tracked.
    Returns True if the item is newly added, False if it already exists.
    """"""

    # Preprocess the item name: lowercase and convert Czech characters to English equivalents
    preprocessed_item_name = item_name.lower().translate(czech_to_english_map).strip()

    # Retrieve user preferences
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.setdefault('tracked_items', [])

    # Check if the preprocessed item name is already in the tracked items
    if preprocessed_item_name not in tracked_items:
        # Add the preprocessed item to the tracking list
        tracked_items.append(preprocessed_item_name)
        save_user_preferences(chat_id, preferences)
        return True

    return False


def remove_tracked_item(chat_id, item_name):
    """"""
    Removes an item from the user's tracking list.
    """"""
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.get('tracked_items', [])

    if item_name in tracked_items:
        tracked_items.remove(item_name)
        save_user_preferences(chat_id, preferences)


# --------------- Shop Inclusion and Exclusion Handling ---------------

def exclude_all_shops(chat_id):
    """"""
    Excludes all shops by retrieving all shop names from the pdf_metadata table
    and storing them in the user's preferences.
    """"""
    unique_shops = set()
    exclusive_start_key = None

    while True:
        scan_kwargs = {'ProjectionExpression': 'shop_name'}
        if exclusive_start_key:
            scan_kwargs['ExclusiveStartKey'] = exclusive_start_key

        response = pdf_metadata_table.scan(**scan_kwargs)

        for item in response.get('Items', []):
            unique_shops.add(item['shop_name'])

        exclusive_start_key = response.get('LastEvaluatedKey')
        if not exclusive_start_key:
            break

    preferences = get_user_preferences(chat_id)
    preferences['excluded_shops'] = list(unique_shops)
    save_user_preferences(chat_id, preferences)

    return preferences


def get_excluded_shops(chat_id):
    """"""
    Retrieves the list of excluded shops from the user's preferences.
    """"""
    preferences = get_user_preferences(chat_id)
    return set(preferences.get('excluded_shops', []))


def get_included_shops(chat_id):
    """"""
    Returns the list of shops that are included for tracking by comparing all shops
    to the excluded shops in the user's preferences.
    """"""
    excluded_shops = get_excluded_shops(chat_id)
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    all_shops = set(item['shop_name'] for item in response['Items'])
    included_shops = all_shops - excluded_shops
    return sorted(included_shops) if included_shops else []


def exclude_shop(chat_id, shop_name):
    """"""
    Adds a shop to the user's excluded shops list.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name not in excluded_shops:
        excluded_shops.add(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


def include_shop(chat_id, shop_name):
    """"""
    Removes a shop from the user's excluded shops list, thereby including it in tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name in excluded_shops:
        excluded_shops.remove(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


# --------------- General Shop Management ---------------

def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


# --------------- Sale Sheet and Media Preferences Handling ---------------

def is_pdf_receive_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('receive_pdf_enabled', True)  # Default is True


def set_pdf_receive_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['receive_pdf_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_photo_group_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('photo_group_enabled', True)  # Default is True


def set_photo_group_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['photo_group_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_text_info_enabled(chat_id):
    """"""
    Returns whether text info is enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('text_info_enabled', False)  # Default is False


def set_text_info_enabled(chat_id, enabled):
    """"""
    Enables or disables text info for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['text_info_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


# --------------- Search Handling ---------------

czech_to_english_map = str.maketrans(
    ""áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""acdeeinorstuuyzACDEEINORSTUUYZ""
)


def preprocess_text(text):
    """"""Lowercases, converts Czech characters, and removes spaces for consistent comparison.""""""
    text = text.lower().translate(czech_to_english_map).replace("" "", """").strip()
    return text


def custom_rolling_similarity_score(part, text, tolerance=1):
    """"""Calculate similarity score between part and a substring in text using rolling hash.""""""
    part_length = len(part)

    # Check if part length is greater than text length
    if part_length > len(text):
        return 0  # Return 0 if text is too short for a match

    # Calculate initial hash values for the part and the first substring of the text
    part_hash = sum(ord(c) for c in part)
    substring_hash = sum(ord(text[i]) for i in range(part_length))

    max_score = 0

    for i in range(len(text) - part_length + 1):
        # Check if the hash difference is within tolerance * average char value
        if abs(substring_hash - part_hash) <= tolerance * 10:  # Adjusting tolerance scale for leniency
            candidate = text[i:i + part_length]
            # Calculate actual similarity score based on character matching
            score = sum(1 for x, y in zip(part, candidate) if x == y) / part_length * 100
            max_score = max(max_score, score)

        # Update hash by sliding the window
        if i + part_length < len(text):
            substring_hash += ord(text[i + part_length]) - ord(text[i])

    return max_score


def find_item(item_names, shop_name=None, included_shops=None, pdf_filename=None, similarity_threshold=75, penalty=10):
    """"""
    Searches for an item in the detected_data_table based on the given item_name,
    optional shop_name, and list of included_shops. It uses fuzzy matching for flexible name search.

    :param item_names: The name(s) of the items to search for.
    :param shop_name: (Optional) The specific shop to search in.
    :param included_shops: (Optional) A list of shops to limit the search.
    :param pdf_filename: (Optional) The original PDF filename where the item was extracted.
    :param similarity_threshold: (Optional) The threshold of similarity to consider a match (default is 80).
    :param penalty: (Optional) The penalty to apply when no match is found for any word (default is 10).
    :return: A list of matching items with their prices and other metadata.
    """"""
    results = []
    shop_name = shop_name or """"
    included_shops = included_shops or []

    if isinstance(item_names, str):
        item_names = [item_names]

    # Preprocess input item names: lowercase, convert Czech characters, and split by non-alphanumeric characters
    preprocessed_item_names = [
        [preprocess_text(word) for word in re.split(r'\W+', item_name)]
        for item_name in item_names
    ]

    scan_kwargs = {
        'FilterExpression': Attr('valid').eq(True)
    }

    if shop_name:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').eq(shop_name)
    if included_shops:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').is_in(included_shops)
    if pdf_filename:
        pdf_base_name = pdf_filename.replace("".pdf"", """")
        scan_kwargs['FilterExpression'] &= Attr('image_id').contains(pdf_base_name)

    response = detected_data_table.scan(**scan_kwargs)

    for item in response.get('Items', []):
        item_shop_name = item.get('shop_name', 'Unknown Shop')

        # Preprocess item names for matching
        db_item_name = preprocess_text(item.get('item_name', '') or '')
        processed_item_name = preprocess_text(item.get('processed_item_name', '') or '')

        for preprocessed_item_name_parts in preprocessed_item_names:
            total_score = 0
            match_found = False

            for part in preprocessed_item_name_parts:
                # Calculate similarity scores of the entire part against db_item_name, processed_item_name, and image_text
                db_score = fuzz.partial_ratio(part, db_item_name)
                processed_score = fuzz.partial_ratio(part, processed_item_name)

                # Only check image_text if db_score and processed_score are below the threshold
                if db_score < similarity_threshold and processed_score < similarity_threshold:
                    # Use rolling hash similarity scoring for image_text
                    image_text = preprocess_text(item.get('whole_image_ocr_text', '') or '')
                    image_score = custom_rolling_similarity_score(part, image_text, tolerance=1)
                else:
                    image_score = 0

                # Select the maximum score out of the four scores
                max_score = max(db_score, processed_score, image_score)

                # Apply penalty or update total score based on max_score
                if max_score == 0:
                    total_score -= penalty
                else:
                    total_score += max_score
                    match_found = True

            # Calculate the average score for the item
            avg_score = total_score / len(preprocessed_item_name_parts) if preprocessed_item_name_parts else 0

            # Add item to results if the average score meets the threshold
            if avg_score >= similarity_threshold and match_found:
                price = find_price_for_item(item)
                results.append({
                    'item_name': item.get('item_name', ''),
                    'price': price,
                    'shop_name': item_shop_name,
                    'similarity_score': avg_score,
                    'image_name': item.get('image_id')
                })

    # Sort the results by similarity score in descending order
    results = sorted(results, key=lambda x: x['similarity_score'], reverse=True)

    return results


def find_price_for_item(obj):
    """"""
    Extracts different price types (item_price, item_initial_price, item_member_price)
    from a DynamoDB item and returns them as a formatted string.

    :param obj: The DynamoDB item containing price information.
    :return: A formatted string containing price information or ""Price not found"" if no prices exist.
    """"""
    prices = []

    def try_convert_to_dict(value):
        """"""
        Utility function to safely convert a string into a dictionary if possible.

        :param value: The value to attempt conversion.
        :return: A dictionary if successful, otherwise the original value.
        """"""
        if isinstance(value, str):
            try:
                return ast.literal_eval(value)  # Attempt to evaluate as a Python literal
            except (ValueError, SyntaxError):
                try:
                    return json.loads(value)  # Attempt to parse as JSON
                except (ValueError, TypeError):
                    return value  # Return original value if conversion fails
        return value

    # Try to convert price fields to dictionaries, if applicable
    processed_price = try_convert_to_dict(obj.get('processed_item_price', None))
    initial_price = try_convert_to_dict(obj.get('processed_item_initial_price', None))
    member_price = try_convert_to_dict(obj.get('processed_item_member_price', None))

    # Handle processed_item_price
    if isinstance(processed_price, dict):
        prices.append(f""Price: {processed_price.get('item_price')}\n"")
    elif processed_price:  # If it's a string or number
        prices.append(f""Price: {processed_price}\n"")

    # Handle processed_item_initial_price
    if isinstance(initial_price, dict):
        prices.append(f""Initial price: {initial_price.get('item_initial_price')}\n"")
    elif initial_price:
        prices.append(f""Initial price: {initial_price}\n"")

    # Handle processed_item_member_price
    if isinstance(member_price, dict):
        prices.append(f""Member price: {member_price.get('item_member_price')}\n"")
    elif member_price:
        prices.append(f""Member price: {member_price}\n"")

    # Return the price strings or ""Price not found"" if no prices are available
    return """".join(prices) if prices else ""Price not found""


# --------------- User Interaction Handling ---------------

def get_available_languages():
    """"""
    Returns a dictionary of supported languages and their respective codes.
    """"""
    return {
        'en': 'English',
        'ru': 'Русский',
        'uk': 'Українська',
        'cs': 'Čeština'
    }


# Language selection prompt
def language_selection(chat_id):
    """"""
    Sends a language selection prompt to the user with inline buttons for language choices.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""inline_keyboard"": [
            [{""text"": ""English"", ""callback_data"": ""lang_en""}],
            [{""text"": ""Русский"", ""callback_data"": ""lang_ru""}],
            [{""text"": ""Українська"", ""callback_data"": ""lang_uk""}],
            [{""text"": ""Čeština"", ""callback_data"": ""lang_cs""}],
        ]
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Welcome! Please select your language:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Main menu display
def main_menu(chat_id):
    """"""
    Displays the main menu to the user with various options for tracking and comparing shop items.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""keyboard"": [
            [{""text"": ""🔍 Search for item""}],
            [{""text"": ""🛒 Add shop item to track price""}],
            [{""text"": ""🛍 Compare shopping list over shops""}],
            [{""text"": ""⚙️ Settings""}],
            [{""text"": ""ℹ️ About project""}],
        ],
        ""resize_keyboard"": True
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Main Menu"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Including shop to track at the start
def include_user_tracking_shops(chat_id):
    """"""
    Sends a list of shops for the user to start tracking items in. If no shops are excluded,
    it loads all shops into the excluded list first.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    shops = list(get_excluded_shops(chat_id))

    # If no excluded shops, exclude all shops initially
    if shops == []:
        exclude_all_shops(chat_id)

    # Send the list of shops with options to select from
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop from the list for item search and tracking. You can change this later in 'Settings'."",
        ""reply_markup"": {
            ""keyboard"": [[shop] for shop in get_excluded_shops(chat_id)] + [[""⬅️ Back to main menu""]],
            ""resize_keyboard"": True
        }
    })

    # Save the user's state as selecting shops
    save_user_state(chat_id, '/start_selecting_shops')


# Settings menu display with dynamic labels for photo group and text info
def settings_menu(chat_id):
    """"""
    Displays the settings menu with dynamic labels for photo group and text info toggling options.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""

    # Get the current state of photo groups and text info to display in the button labels
    photo_group_state = ""Enabled"" if is_photo_group_enabled(chat_id) else ""Disabled""
    text_info_state = ""Enabled"" if is_text_info_enabled(chat_id) else ""Disabled""
    receive_pdf_state = ""Enabled"" if is_pdf_receive_enabled(chat_id) else ""Disabled""

    # Create buttons with dynamic labels based on current settings
    buttons = {
        ""keyboard"": [
            [{""text"": ""🚫 Exclude some shops from tracking""}],
            [{""text"": ""✅ Include some shops in tracking""}],
            [{""text"": ""🛑 Remove shop item from tracking price""}],
            [{""text"": f""📄 Turn {'off' if receive_pdf_state == 'Enabled' else 'on'} Receiving New PDFs""}],
            [{""text"": f""📄 Turn {'off' if photo_group_state == 'Enabled' else 'on'} items photo groups""}],
            [{""text"": f""📄 Turn {'off' if text_info_state == 'Enabled' else 'on'} items text info""}],
            [{""text"": ""🌐 Change interface language""}],
            [{""text"": ""⬅️ Back to main menu""}],
        ],
        ""resize_keyboard"": True
    }

    # Send the settings menu
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Select an option from Settings:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Sending images as an album
def send_images_as_album(chat_id, media_group, shop_name):
    """"""
    Sends images in batches of up to 10 (as albums) to the user via Telegram.

    :param chat_id: The Telegram chat ID of the user.
    :param media_group: A list of tuples (S3 image path, local temp path) representing images to send.
    :param shop_name: The name of the shop to include in the caption of the first image in each batch.
    """"""

    def chunks(iterable, size):
        """"""Helper function to divide media_group into chunks of a given size.""""""
        iterator = iter(iterable)
        while True:
            batch = list(islice(iterator, size))
            if not batch:
                break
            yield batch

    for batch_index, batch in enumerate(chunks(media_group, 10)):
        media = []
        files = {}

        # Loop through the current batch of images
        for i, (s3_image_path, temp_path) in enumerate(batch):
            try:
                # Extract the image filename from the S3 path
                image_name = os.path.basename(s3_image_path)

                # Log the S3 path and temporary file for debugging
                logger.debug(f""Downloading image from S3: {s3_image_path} to {temp_path}"")

                # Use a temporary file to handle the downloaded image
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file_path = temp_file.name
                    # Download the image from S3 into the temporary file
                    download_file_from_s3(s3_image_path, temp_file_path)

                    # Read the file content for use in the Telegram API
                    with open(temp_file_path, 'rb') as image_file:
                        image_content = image_file.read()

                    # Prepare the files dictionary (it must have unique keys for each image)
                    files[f""photo{batch_index}_{i}""] = (image_name, image_content)

                    # Prepare the media array with references to the attached photos
                    media.append({
                        ""type"": ""photo"",
                        ""media"": f""attach://photo{batch_index}_{i}"",
                        ""caption"": shop_name if i == 0 else """"  # Add shop name as caption only to the first image
                    })

                    # Remove the temporary file after reading
                    os.remove(temp_file_path)

            except Exception as e:
                logger.error(f""Error processing image {image_name}: {str(e)}"")

        # If no valid media is available, log an error and return
        if not media:
            logger.error(""No valid media to send."")
            continue

        # Send the media group using the Telegram API
        try:
            response = requests.post(f""{API_URL}/sendMediaGroup"", files=files, data={
                ""chat_id"": chat_id,
                ""media"": json.dumps(media)  # Convert the media list to a JSON string
            })

            logger.debug(f""Telegram API response for sendMediaGroup: {response.status_code}, {response.text}"")
        except Exception as e:
            logger.error(f""Error sending media group: {str(e)}"")


def send_single_pdf(
        chat_ids,
        file_source,
        shop_name
):
    """"""
    Sends a single PDF file to multiple users via Telegram using a file path.

    :param chat_ids: A list of Telegram chat IDs of the users.
    :param file_source: The file path of the PDF to send.
    :param shop_name: The name of the shop to include in the caption of the PDF.
    """"""
    try:
        # Download the file from S3 only once
        # Extract the original filename from the S3 path
        original_filename = os.path.basename(file_source)
        logger.debug(f""Original filename extracted: {original_filename}"")

        # Create a temporary file with the original filename and extension
        with tempfile.NamedTemporaryFile(delete=False, suffix="".pdf"") as temp_file:
            temp_file_path = temp_file.name
            logger.debug(f""Temporary file created at {temp_file_path}"")

            # Download the file from S3 (replace this with your actual download logic)
            download_file_from_s3(file_source, temp_file_path)

        # Iterate over each chat_id and send the file to each user
        for chat_id in chat_ids:
            logger.debug(f""Sending PDF to chat_id: {chat_id}"")

            # Prepare the base payload for each user
            data = {
                ""chat_id"": chat_id,
                ""caption"": f""New {shop_name} letak available""
            }

            # Send via multipart/form-data (local file upload)
            with open(temp_file_path, 'rb') as pdf_file:
                logger.debug(f""Opened local file: {temp_file_path} for chat_id: {chat_id}"")

                # Prepare multipart form-data
                files = {
                    ""document"": (original_filename, pdf_file, ""application/pdf"")
                }

                # Send the document via multipart form-data
                logger.debug(f""Sending document via multipart/form-data for chat_id: {chat_id}..."")
                response = requests.post(f""{API_URL}/sendDocument"", files=files, data=data)

                # Log the response from Telegram for debugging purposes
                logger.debug(
                    f""Telegram API response for sendDocument (chat_id: {chat_id}): {response.status_code}, {response.text}"")

        # Clean up the temporary file after sending to all users
        logger.debug(f""Cleaning up temporary file: {temp_file_path}"")
        os.remove(temp_file_path)
        logger.debug(f""Temporary file {temp_file_path} removed successfully"")

    except ValueError as ve:
        logger.error(f""ValueError: {str(ve)}"")
    except Exception as e:
        logger.error(f""Error sending document '{file_source}' to chat IDs {chat_ids}: {str(e)}"")


# --------------- Message Processing ---------------

def handle_start_command(chat_id, state):
","    description = ""Welcome to the Smart Shopping Bot! I will help you track prices, manage sale sheets, and find the best shopping paths.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": description})

    if state == ""new_user"":
        # Set default preferences for new users
        preferences = get_user_preferences(chat_id)
        if 'photo_group_enabled' not in preferences:
            preferences['photo_group_enabled'] = True  # Default to show photo groups
        if 'text_info_enabled' not in preferences:
            preferences['text_info_enabled'] = False  # Default to hide text info
        if 'receive_pdf_enabled' not in preferences:
            preferences['receive_pdf_enabled'] = True  # Default to send pdf after new validation pipeline check
","        save_user_preferences(chat_id, preferences)

        # Guide user through initial steps of setup: language selection or shop inclusion
        if get_user_language(chat_id) is None:
            language_selection(chat_id)
            save_user_state(chat_id, None)
        elif get_included_shops(chat_id) is None:
            include_user_tracking_shops(chat_id)
            save_user_state(chat_id, None)
        else:
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # If user is not new, take them directly to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_shop_selection(chat_id, text):
    if text == ""➕ Add another shop"":
        include_user_tracking_shops(chat_id)
    elif text == ""➡️ Save tracking shop list. Return to the main menu"":
        if not get_included_shops(chat_id):
            # No shops have been included yet, notify the user
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select at least one shop from the list:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(get_excluded_shops(chat_id))] + [
                        [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True}
            })
        else:
            # At least one shop is included, allow returning to the menu
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # Get the list of excluded shops
        shops = list(get_excluded_shops(chat_id))
        # Check if the input text is a valid shop
        if text in shops:
            include_shop(chat_id, text)
            # Ask if the user wants to add more shops or return to the main menu
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Save tracking shop list. Return to the main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, '/start_selecting_shops')
        elif text == ""⬅️ Back to main menu"":
            # Check if any shops are included before allowing return to the main menu
            if not get_included_shops(chat_id):
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please select at least one shop before returning to the menu:"",
                    ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                     ""resize_keyboard"": True}
                })
            else:
                main_menu(chat_id)
                save_user_state(chat_id, None)

        else:
            # The user input is not a valid shop, ask them to select again
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a valid shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_search_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please enter the name of the item you want to search for."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'searching_item')


def handle_add_shop_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please provide the name of the shop item you want to track."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'adding_item')


def handle_settings_button(chat_id):
    settings_menu(chat_id)
    save_user_state(chat_id, 'in_settings')


def handle_compare_shop_list_button(chat_id):
    # Clean preferences after unexpected last user manipulations
    preferences = get_user_preferences(chat_id)
    preferences['selected_shops'] = []
    preferences['item_list'] = []
    save_user_preferences(chat_id, preferences)

    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop or list of shops from your history."",
        ""reply_markup"": {
            ""keyboard"": [[""List of all shops""], [""Lists of shops from history""], [""⬅️ Back to main menu""]],
            ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'selecting_shops')


def handle_about_button(chat_id):
    about_text = ""This bot helps you optimize your shopping by tracking prices, managing sale sheets, and finding the best shopping routes.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": about_text})
    main_menu(chat_id)
    save_user_state(chat_id, None)


def handle_item_adding_searching_state(chat_id, state, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Handle the case where user provides a single item name or multiple item names (split by commas or new lines)
        item_names = [name.strip() for name in text.split("","")]

        # Call the new find_item method with a list of item names
        found_items = find_item(item_names=item_names, included_shops=get_included_shops(chat_id))

        # If adding the item for tracking
        if state == 'adding_item':
            # Loop through each item name and handle them separately for adding to tracking
            for item_name in item_names:
                added = add_tracked_item(chat_id, item_name)

                if added:
                    response = f""'{item_name}' saved for tracking. I will notify you when '{item_name}' has a valid sale.""
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                else:
                    tracked_items = get_tracked_items(chat_id)
                    response = f""'{item_name}' is already in your tracking list. Here is your current list:\n"" + ""\n"".join(
                        tracked_items)
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                    main_menu(chat_id)
                    save_user_state(chat_id, None)
                    return

        # If searching for the item(s)
        if found_items:
            items_by_shop = {}

            # Group found items by shop
            for found_item in found_items:
                shop_name = found_item['shop_name']
                if shop_name not in items_by_shop:
                    items_by_shop[shop_name] = []
                items_by_shop[shop_name].append(found_item)

            photo_group_enabled = is_photo_group_enabled(chat_id)
            text_info_enabled = is_text_info_enabled(chat_id)

            # Iterate over each shop and its associated items
            for shop_name, shop_items in items_by_shop.items():
                # Prepare the initial response message
                response = f""Here is what I found for '{', '.join(item_names)}' in {shop_name}:\n""
                media_group = []

                # Process each found item for the shop
                for found_item in shop_items:
                    if text_info_enabled:
                        response += f""- {found_item['item_name']} at {shop_name}: {found_item['price']}\n""

                    s3_image_dir = found_item.get('image_name')
                    # Collecting images for the media group (album) if photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")

                # Send media group (photos) if enabled
                if media_group and photo_group_enabled:
                    logger.debug(f""Sending media group for {shop_name} with {len(media_group)} images"")
                    send_images_as_album(chat_id, media_group, shop_name)

                # Send text response with item details if text info is enabled
                if text_info_enabled:
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

        else:
            # If no items found, notify the user
            requests.post(f""{API_URL}/sendMessage"",
                          json={""chat_id"": chat_id, ""text"": f""No items found for '{', '.join(item_names)}'.""})

        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_in_settings_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""📄 Turn off Receiving New PDFs"" or text == ""📄 Turn on Receiving New PDFs"":
        current_receive_pdf_state = is_pdf_receive_enabled(chat_id)
        # Safe to toggle photo groups
        set_pdf_receive_enabled(chat_id, not current_receive_pdf_state)
        new_state = ""enabled"" if not current_receive_pdf_state else ""disabled""
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": f""Receiving of new PDFs has been {new_state}.""
        })
        settings_menu(chat_id)
    elif text == ""📄 Turn on items photo groups"" or text == ""📄 Turn off items photo groups"":
        # Toggle the photo group setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_text_info_state and current_photo_group_state:
            # Cannot disable photo groups if text info is already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Text info is already disabled, so photo groups cannot be turned off.""
            })
        else:
            # Safe to toggle photo groups
            set_photo_group_enabled(chat_id, not current_photo_group_state)
            new_state = ""enabled"" if not current_photo_group_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item photo groups are now {new_state}.""
            })
            settings_menu(chat_id)

    elif text == ""📄 Turn on items text info"" or text == ""📄 Turn off items text info"":
        # Toggle the text info setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_photo_group_state and current_text_info_state:
            # Cannot disable text info if photo groups are already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Photo groups are already disabled, so text info cannot be turned off.""
            })
        else:
            # Safe to toggle text info
            set_text_info_enabled(chat_id, not current_text_info_state)
            new_state = ""enabled"" if not current_text_info_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item text info is now {new_state}.""
            })
            settings_menu(chat_id)
    elif text == ""🚫 Exclude some shops from tracking"":
        # Retrieve the included shops that can be excluded
        included_shops = get_included_shops(chat_id)

        # If there are no included shops, inform the user
        if not included_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""No shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of shops that can be excluded
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to exclude from tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in included_shops] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'excluding_shop')
    elif text == ""✅ Include some shops in tracking"":
        # Retrieve the excluded shops that can be included
        excluded_shops = get_excluded_shops(chat_id)

        # If all shops are already included, inform the user
        if not excluded_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""All shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of excluded shops that can be included
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to include in tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(excluded_shops)] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'including_shop')
    elif text == ""🛑 Remove shop item from tracking price"":
        items_to_remove = get_tracked_items(chat_id)
        if items_to_remove:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Select an item to remove from tracking:"",
                ""reply_markup"": {""keyboard"": [[item] for item in items_to_remove] + [[""⬅️ Back to settings""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'removing_item')
        else:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any items being tracked."",
            })
            settings_menu(chat_id)
            save_user_state(chat_id, 'in_settings')


def handle_excluding_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        exclude_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' excluded from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_including_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        include_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' included for tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_removing_item_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        remove_tracked_item(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Item '{text}' removed from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_shop_list_history_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        try:
            index = int(text) - 1
            shop_history = get_user_selected_shops_history(chat_id)
            if 0 <= index < len(shop_history):
                selected_history_list = shop_history[index]
                preferences = get_user_preferences(chat_id)
                preferences['selected_shops'] = selected_history_list
                save_user_preferences(chat_id, preferences)
                # Proceed to item entry
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please provide your shopping list, one per line and send."",
                    ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
                })
                preferences['item_list'] = []
                save_user_preferences(chat_id, preferences)
                save_user_state(chat_id, 'entering_items')
            else:
                raise IndexError
        except (ValueError, IndexError):
            # Handle invalid input
            shop_history = get_user_selected_shops_history(chat_id)
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]
            text_message = ""Invalid selection. Please choose a number from the list:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })


def handle_selecting_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')
    elif text == ""Lists of shops from history"":
        # Get the user's shop history
        shop_history = get_user_selected_shops_history(chat_id)
        if shop_history:
            # Build the keyboard buttons with numbers
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]

            # Corrected prompt and display
            text_message = ""Please select a shop list from your history:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""

            # Send the message with the keyboard
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })

            # Save the user state
            save_user_state(chat_id, 'shop_list_history')
        else:
            shops = get_all_shops()
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any history saved list. Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
    else:
        # Get the list of available shops
        shops = get_all_shops()
        # Check if the input text is a valid shop
        if text in shops:
            # Save the selected shop
            preferences = get_user_preferences(chat_id)
            selected_shops = preferences.get('selected_shops', [])
            if text not in selected_shops:
                selected_shops.append(text)
                preferences['selected_shops'] = selected_shops
                save_user_preferences(chat_id, preferences)
            # Ask if the user wants to add more shops or continue
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Continue with shop list""],
                                 [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

        else:
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_confirming_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➕ Add another shop"":
        # Exclude already selected shops
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        shops = [shop for shop in get_all_shops() if shop not in selected_shops]
        if shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [
                    [""⬅️ Back to main menu""] + [""➡️ Continue with shop list""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            # All shops have been selected
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have selected all available shops."",
                ""reply_markup"": {
                    ""keyboard"": [[""➡️ Continue with shop list""], [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')

    else:
        # Handle unexpected input
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": ""Please select an option from the menu.""
        })


def handle_entering_items_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Add item to the list
        preferences = get_user_preferences(chat_id)
        item_list = preferences.get('item_list', [])
        item_list.extend(text.split('\n'))
        preferences['item_list'] = item_list
        save_user_preferences(chat_id, preferences)
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        item_list = preferences.get('item_list', [])
        response = ""Here are the items found in the selected shops:\n""
        # Retrieve user preferences for photo group and text info settings
        photo_group_enabled = is_photo_group_enabled(chat_id)
        text_info_enabled = is_text_info_enabled(chat_id)
        # List to collect all images for the media group
        for shop in selected_shops:
            if text_info_enabled:
                response += f""\nItems in {shop}:\n""
            media_group = []  # List to collect all images for the media group
            # Call find_item once for all items in the current shop
            found_items = find_item(item_names=item_list, shop_name=shop)

            if found_items:
                for found_item in found_items:
                    # Extract price and image path details from the found item
                    price = found_item.get('price')
                    s3_image_dir = found_item.get('image_name')

                    logger.debug(f""Found item: {found_item}"")
                    logger.debug(f""Price: {price}, Image Path: {s3_image_dir}"")

                    # Include price details in the response if text info is enabled
                    if text_info_enabled:
                        # Add the item price or ""Price not found"" based on availability
                        if price:
                            response += f""- {found_item['item_name']} at {shop}: {price}\n""
                        else:
                            response += f""- {found_item['item_name']} at {shop}: Price not found\n""

                    # Process the image only if the photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        # Extract the filename from the full S3 path
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""

                        # Add the image filename and its corresponding local path to the media group
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")
            else:
                # If none of the items are found in the shop, add a not found message to the response
                for item_name in item_list:
                    response += f""- {item_name}: Not found in {shop}\n""

            # After looping through items, send the images as an album if there are any and photo group is enabled
            logger.debug(f""Media group length: {len(media_group)}. Photo group enabled: {photo_group_enabled}"")
            if media_group and photo_group_enabled:
                logger.debug(f""Sending media group for shop: {shop}"")
                send_images_as_album(chat_id, media_group, shop)
                media_group.clear()  # Clear the media group after sending to avoid duplicate entries
            else:
                logger.debug(f""No images to send or photo group is disabled"")

            # Send the final response with text results if text info is enabled
        if text_info_enabled:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

            # Go back to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)

        # Clear selected shops and item list
        preferences['selected_shops'] = []
        preferences['item_list'] = []
        save_user_preferences(chat_id, preferences)


def process_message(update):
    chat_id = update['message']['chat']['id']
    text = update['message'].get('text')

    # Get current user state
    state = get_user_state(chat_id)

    # Main dispatcher based on text command or state
    if text == ""/start"":
        handle_start_command(chat_id, state)
    elif state == '/start_selecting_shops':
        handle_shop_selection(chat_id, text)
    elif text == ""🔍 Search for item"":
        handle_search_button(chat_id)
    elif text == ""🛒 Add shop item to track price"":
        handle_add_shop_button(chat_id)
    elif text == ""⚙️ Settings"":
        handle_settings_button(chat_id)
    elif text == ""🛍 Compare shopping list over shops"":
        handle_compare_shop_list_button(chat_id)
    elif text == ""ℹ️ About project"":
        handle_about_button(chat_id)
    elif state == 'adding_item' or state == ""searching_item"":
        handle_item_adding_searching_state(chat_id, state, text)
    elif state == 'in_settings':
        handle_in_settings_state(chat_id, text)
    elif state == 'excluding_shop':
        handle_excluding_shop_state(chat_id, text)
    elif state == 'including_shop':
        handle_including_shop_state(chat_id, text)
    elif state == 'removing_item':
        handle_removing_item_state(chat_id, text)
    elif state == ""shop_list_history"":
        handle_shop_list_history_state(chat_id, text)
    elif state == 'selecting_shops':
        handle_selecting_shops_state(chat_id, text)
    elif state == 'confirming_shops':
        handle_confirming_shops_state(chat_id, text)
    elif state == 'entering_items':
        handle_entering_items_state(chat_id, text)
    else:
        if text == ""⬅️ Back to main menu"":
            main_menu(chat_id)
            save_user_state(chat_id, None)
        else:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id,
                                                          ""text"": ""I'm sorry, I didn't understand that. Please choose an option from the menu.""})


# Handle callback queries from inline buttons (e.g., language selection)
def process_callback_query(update):
    """"""
    Processes the callback queries triggered by inline buttons, such as language selection.
    :param update: The update payload containing the callback query details.
    """"""
    query = update['callback_query']
    chat_id = query['message']['chat']['id']  # Extract the chat ID of the user
    message_id = query['message']['message_id']  # Extract the message ID to edit it later
    data = query.get('data')  # Retrieve the callback data (e.g., 'lang_en')
    callback_query_id = query['id']  # The ID for answering the callback query

    # Check if the callback is related to language selection (data starts with 'lang_')
    if data.startswith('lang_'):
        language_code = data.split('_')[1]  # Extract the language code (e.g., 'en', 'ru')
        set_user_language(chat_id, language_code)  # Save the selected language to user preferences

        # Answer the callback query to stop Telegram's ""loading"" animation
        answer_payload = {
            ""callback_query_id"": callback_query_id,
            ""text"": ""Language updated!"",  # Confirmation message to the user
            ""show_alert"": False  # Do not show a popup, just stop the loading animation
        }
        requests.post(f""{API_URL}/answerCallbackQuery"", json=answer_payload)

        # Edit the original message to remove the inline keyboard and update the text
        new_text = f""Language selected! You have set your language to: {get_available_languages().get(language_code, 'Unknown')}""
        edit_payload = {
            ""chat_id"": chat_id,
            ""message_id"": message_id,
            ""text"": new_text,
            ""reply_markup"": {}  # Remove the inline keyboard by setting an empty reply_markup
        }
        requests.post(f""{API_URL}/editMessageText"", json=edit_payload)

        # Proceed to include the shops tracking list for the user
        include_user_tracking_shops(chat_id)


def handle_pdf_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')

    s3_pdf_path = ""pdfs/"" + pdf_file
    send_single_pdf(users_id_list, s3_pdf_path, shop_name)


def handle_tracked_items_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')
    tracked_items_list = update.get('tracked_items_list', [])  # A list of tracked items for each user

    # Prepare global variables for tracking all responses and media to send in batches
    user_responses = {}  # Dictionary to hold responses per user
    user_media_groups = {}  # Dictionary to hold media groups per user

    # Iterate over each user and their tracked items
    for i, user_id in enumerate(users_id_list):
        user_tracked_items = tracked_items_list[i]

        # Check if text and photo group sending is enabled for the user
        text_info_enabled = is_text_info_enabled(user_id)
        photo_group_enabled = is_photo_group_enabled(user_id)

        # If there are no tracked items for the user, skip to the next user
        if not user_tracked_items:
            continue

        # Call the find_item method once for all tracked items at once
        matched_items = find_item(item_names=user_tracked_items, shop_name=shop_name, pdf_filename=pdf_file)

        # If no matched items are found, skip to the next user
        if not matched_items:
            if text_info_enabled:
                user_responses[user_id] = ""No items found in this flyer.""
            continue

        # Prepare lists to store matched items and media for this user
        user_response = ""Your tracked items are now available:\n\n""  # Friendly message
        media_group = []

        # Process the matched items
        for found_item in matched_items:
            price = found_item.get('price')
            s3_image_dir = found_item.get('image_name')

            # Include price details in the response if text info is enabled
            if text_info_enabled:
                if price:
                    user_response += f""- {found_item['item_name']} at {shop_name}: {price}\n""
                else:
                    user_response += f""- {found_item['item_name']} at {shop_name}: Price not found\n""

            # Process the image only if the photo group is enabled
            if photo_group_enabled and s3_image_dir:
                image_filename = os.path.basename(s3_image_dir)
                local_image_path = f""/tmp/{image_filename}""
                media_group.append((s3_image_dir, local_image_path))

        # Add the user's response and media group to the respective dictionaries
        if text_info_enabled and user_response.strip():
            user_responses[user_id] = user_response
        if media_group and photo_group_enabled:
            user_media_groups[user_id] = media_group

    # Now send out the responses and media groups in batches to minimize API calls
    message = f""Your tracked items are now available at {shop_name}""
    # Send media groups for users
    for user_id, media_group in user_media_groups.items():
        send_images_as_album(user_id, media_group, message)

    # Send text responses for users
    for user_id, response in user_responses.items():
        requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": user_id, ""text"": response})


def lambda_handler(event, context):
    """"""
    This function will act as the webhook to handle Telegram updates when deployed to AWS Lambda.
    It will process both regular messages and callback queries.
    """"""
    try:
        # Parse the body from the incoming event
        if 'body' in event:
            update = json.loads(event['body'])  # Extract the JSON body from the Lambda event

            # Process message or callback query based on the update type
            if 'message' in update:
                process_message(update)  # Call your process_message function
            elif 'callback_query' in update:
                process_callback_query(update)  # Call your process_callback_query function

            process_type = update.get('process_type', None)
            if process_type == 'tracked_items_list':
                # Process the tracked items-related webhook
                handle_tracked_items_newsletter(update)
            elif process_type == 'pdf_newsletter':
                handle_pdf_newsletter(update)

            # Return a success response to Telegram
            return {
                'statusCode': 200,
                'body': json.dumps({'status': 'ok'})
            }
        else:
            logger.error('No body found in the request')
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Bad Request'})
            }

    except Exception as e:
        logger.error(f""Error processing the request: {str(e)}"")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal Server Error'})
        }
","_path = f'{MEDIA_GROUP_PATH}/{image_filename}'

                        # Open local temp file to store the photo directory for future reference
                            local_temp_path = os.path.abspath(
                                os.path.join(""/tmp/"", os.pardir),
                                os.pardir,
                                f""/tmp/{image_filename}.jpg"")

                            # Copy image from temporary file to the local temp file for further manipulation
                            shutil.copy2(local_temp_path, local_image_path)
                            local_temp_path = os.path.abspath(
                                os.path.join(""/tmp/"","
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/lambda_function.py,"import ast
import copy
import re
import tempfile
import boto3
import requests
import json
import os
from boto3.dynamodb.conditions import Attr
import logging
from fuzzywuzzy import fuzz
from itertools import islice

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger()


# Function to initialize global constants and resources
def initialize_globals():
    global BUCKET_NAME, TOKEN, API_URL, dynamodb, s3, user_preferences_table, pdf_metadata_table, detected_data_table

    BUCKET_NAME = os.environ.get('BUCKET_NAME')
    TOKEN = os.environ.get('TOKEN')
    API_URL = f""https://api.telegram.org/bot{TOKEN}""

    # Initialize AWS resources
    dynamodb = boto3.resource('dynamodb')
    s3 = boto3.client('s3')

    # DynamoDB table references
    user_preferences_table = dynamodb.Table('user_preferences')
    pdf_metadata_table = dynamodb.Table('pdf_metadata')
    detected_data_table = dynamodb.Table(""detected_data"")


# Call the initialization function
initialize_globals()


# --------------- AWS Handling ---------------
def download_file_from_s3(filename_path, local_path):
    """"""
    Downloads a file from the specified S3 bucket to a local file path.

    :param filename_path: The path of the file in the S3 bucket.
    :param local_path: The local path where the file should be saved.

    :raises ValueError: If the provided filename_path is not a valid string.
    """"""
    # Ensure the S3 file path is a valid string before proceeding with the download
    if not isinstance(filename_path, str) or not filename_path:
        raise ValueError(f""Invalid S3 filename path: {filename_path}"")

    # Log the download operation for debugging purposes
    logger.debug(f""Downloading file from S3 bucket: {filename_path} to local path: {local_path}"")

    # Perform the file download from the specified S3 bucket to the local path
    s3.download_file(BUCKET_NAME, filename_path, local_path)


# --------------- User Preferences Handling ---------------

def get_user_preferences(chat_id):
    """"""
    Retrieves user preferences from DynamoDB for the given chat_id.
    If no preferences are found, it returns a default preference with 'new_user' state.
    """"""
    response = user_preferences_table.get_item(Key={'chat_id': str(chat_id)})
    return response.get('Item', {""state"": ""new_user""})


def save_user_preferences(chat_id, preferences):
    """"""
    Saves or updates user preferences in DynamoDB for the given chat_id.
    """"""
    user_preferences_table.put_item(Item={
        'chat_id': str(chat_id),
        **preferences
    })


def get_user_language(chat_id):
    """"""
    Retrieves the language preference of the user from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('language', None)


def set_user_language(chat_id, language_code):
    """"""
    Sets the language preference for the user and saves it in DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['language'] = language_code
    save_user_preferences(chat_id, preferences)


def save_user_state(chat_id, state):
    """"""
    Saves the user's current interaction state in DynamoDB, allowing for persistence of state across interactions.

    :param chat_id: The chat ID of the user.
    :param state: The interaction state (e.g., menu state, ongoing search, etc.) to be saved.
    """"""
    preferences = get_user_preferences(chat_id)  # Retrieve the current preferences for the user
    preferences['state'] = state  # Set the new state for the user
    save_user_preferences(chat_id, preferences)  # Save the updated preferences (including the state) to DynamoDB


def get_user_state(chat_id):
    """"""
    Retrieves the user's current interaction state from DynamoDB.

    :param chat_id: The chat ID of the user.
    :return: The current state of the user, or None if no state is set.
    """"""
    preferences = get_user_preferences(chat_id)  # Fetch the user preferences from DynamoDB
    return preferences.get('state', None)  # Return the current state, or None if no state is set


# --------------- Shop Selection History Handling ---------------

def save_user_selected_shops_history(chat_id, shops):
    """"""
    Saves the user's selected shop history in DynamoDB.
    Ensures the history only contains the 10 most recent entries.
    """"""
    preferences = get_user_preferences(chat_id)
    history = preferences.setdefault('selected_shops_history', [])

    if shops not in history:
        history.append(copy.deepcopy(shops))
        if len(history) > 10:  # Keep history to last 10 items
            history.pop(0)

    preferences['selected_shops_history'] = history
    save_user_preferences(chat_id, preferences)


def get_user_selected_shops_history(chat_id):
    """"""
    Retrieves the user's selected shops history from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('selected_shops_history', [])


# --------------- Tracked Items Handling ---------------


def get_tracked_items(chat_id):
    """"""
    Retrieves the list of items the user is tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('tracked_items', [])


def add_tracked_item(chat_id, item_name):
    """"""
    Adds an item to the user's tracking list, if it's not already being tracked.
    Returns True if the item is newly added, False if it already exists.
    """"""

    # Preprocess the item name: lowercase and convert Czech characters to English equivalents
    preprocessed_item_name = item_name.lower().translate(czech_to_english_map).strip()

    # Retrieve user preferences
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.setdefault('tracked_items', [])

    # Check if the preprocessed item name is already in the tracked items
    if preprocessed_item_name not in tracked_items:
        # Add the preprocessed item to the tracking list
        tracked_items.append(preprocessed_item_name)
        save_user_preferences(chat_id, preferences)
        return True

    return False


def remove_tracked_item(chat_id, item_name):
    """"""
    Removes an item from the user's tracking list.
    """"""
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.get('tracked_items', [])

    if item_name in tracked_items:
        tracked_items.remove(item_name)
        save_user_preferences(chat_id, preferences)


# --------------- Shop Inclusion and Exclusion Handling ---------------

def exclude_all_shops(chat_id):
    """"""
    Excludes all shops by retrieving all shop names from the pdf_metadata table
    and storing them in the user's preferences.
    """"""
    unique_shops = set()
    exclusive_start_key = None

    while True:
        scan_kwargs = {'ProjectionExpression': 'shop_name'}
        if exclusive_start_key:
            scan_kwargs['ExclusiveStartKey'] = exclusive_start_key

        response = pdf_metadata_table.scan(**scan_kwargs)

        for item in response.get('Items', []):
            unique_shops.add(item['shop_name'])

        exclusive_start_key = response.get('LastEvaluatedKey')
        if not exclusive_start_key:
            break

    preferences = get_user_preferences(chat_id)
    preferences['excluded_shops'] = list(unique_shops)
    save_user_preferences(chat_id, preferences)

    return preferences


def get_excluded_shops(chat_id):
    """"""
    Retrieves the list of excluded shops from the user's preferences.
    """"""
    preferences = get_user_preferences(chat_id)
    return set(preferences.get('excluded_shops', []))


def get_included_shops(chat_id):
    """"""
    Returns the list of shops that are included for tracking by comparing all shops
    to the excluded shops in the user's preferences.
    """"""
    excluded_shops = get_excluded_shops(chat_id)
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    all_shops = set(item['shop_name'] for item in response['Items'])
    included_shops = all_shops - excluded_shops
    return sorted(included_shops) if included_shops else []


def exclude_shop(chat_id, shop_name):
    """"""
    Adds a shop to the user's excluded shops list.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name not in excluded_shops:
        excluded_shops.add(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


def include_shop(chat_id, shop_name):
    """"""
    Removes a shop from the user's excluded shops list, thereby including it in tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name in excluded_shops:
        excluded_shops.remove(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


# --------------- General Shop Management ---------------

def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


# --------------- Sale Sheet and Media Preferences Handling ---------------

def is_pdf_receive_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('receive_pdf_enabled', True)  # Default is True


def set_pdf_receive_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['receive_pdf_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_photo_group_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('photo_group_enabled', True)  # Default is True


def set_photo_group_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['photo_group_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_text_info_enabled(chat_id):
    """"""
    Returns whether text info is enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('text_info_enabled', False)  # Default is False


def set_text_info_enabled(chat_id, enabled):
    """"""
    Enables or disables text info for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['text_info_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


# --------------- Search Handling ---------------

czech_to_english_map = str.maketrans(
    ""áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""acdeeinorstuuyzACDEEINORSTUUYZ""
)


def preprocess_text(text):
    """"""Lowercases, converts Czech characters, and removes spaces for consistent comparison.""""""
    text = text.lower().translate(czech_to_english_map).replace("" "", """").strip()
    return text


def custom_rolling_similarity_score(part, text, tolerance=1):
    """"""Calculate similarity score between part and a substring in text using rolling hash.""""""
    part_length = len(part)

    # Check if part length is greater than text length
    if part_length > len(text):
        return 0  # Return 0 if text is too short for a match

    # Calculate initial hash values for the part and the first substring of the text
    part_hash = sum(ord(c) for c in part)
    substring_hash = sum(ord(text[i]) for i in range(part_length))

    max_score = 0

    for i in range(len(text) - part_length + 1):
        # Check if the hash difference is within tolerance * average char value
        if abs(substring_hash - part_hash) <= tolerance * 10:  # Adjusting tolerance scale for leniency
            candidate = text[i:i + part_length]
            # Calculate actual similarity score based on character matching
            score = sum(1 for x, y in zip(part, candidate) if x == y) / part_length * 100
            max_score = max(max_score, score)

        # Update hash by sliding the window
        if i + part_length < len(text):
            substring_hash += ord(text[i + part_length]) - ord(text[i])

    return max_score


def find_item(item_names, shop_name=None, included_shops=None, pdf_filename=None, similarity_threshold=75, penalty=10):
    """"""
    Searches for an item in the detected_data_table based on the given item_name,
    optional shop_name, and list of included_shops. It uses fuzzy matching for flexible name search.

    :param item_names: The name(s) of the items to search for.
    :param shop_name: (Optional) The specific shop to search in.
    :param included_shops: (Optional) A list of shops to limit the search.
    :param pdf_filename: (Optional) The original PDF filename where the item was extracted.
    :param similarity_threshold: (Optional) The threshold of similarity to consider a match (default is 80).
    :param penalty: (Optional) The penalty to apply when no match is found for any word (default is 10).
    :return: A list of matching items with their prices and other metadata.
    """"""
    results = []
    shop_name = shop_name or """"
    included_shops = included_shops or []

    if isinstance(item_names, str):
        item_names = [item_names]

    # Preprocess input item names: lowercase, convert Czech characters, and split by non-alphanumeric characters
    preprocessed_item_names = [
        [preprocess_text(word) for word in re.split(r'\W+', item_name)]
        for item_name in item_names
    ]

    scan_kwargs = {
        'FilterExpression': Attr('valid').eq(True)
    }

    if shop_name:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').eq(shop_name)
    if included_shops:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').is_in(included_shops)
    if pdf_filename:
        pdf_base_name = pdf_filename.replace("".pdf"", """")
        scan_kwargs['FilterExpression'] &= Attr('image_id').contains(pdf_base_name)

    response = detected_data_table.scan(**scan_kwargs)

    for item in response.get('Items', []):
        item_shop_name = item.get('shop_name', 'Unknown Shop')

        # Preprocess item names for matching
        db_item_name = preprocess_text(item.get('item_name', '') or '')
        processed_item_name = preprocess_text(item.get('processed_item_name', '') or '')

        for preprocessed_item_name_parts in preprocessed_item_names:
            total_score = 0
            match_found = False

            for part in preprocessed_item_name_parts:
                # Calculate similarity scores of the entire part against db_item_name, processed_item_name, and image_text
                db_score = fuzz.partial_ratio(part, db_item_name)
                processed_score = fuzz.partial_ratio(part, processed_item_name)

                # Only check image_text if db_score and processed_score are below the threshold
                if db_score < similarity_threshold and processed_score < similarity_threshold:
                    # Use rolling hash similarity scoring for image_text
                    image_text = preprocess_text(item.get('whole_image_ocr_text', '') or '')
                    image_score = custom_rolling_similarity_score(part, image_text, tolerance=1)
                else:
                    image_score = 0

                # Select the maximum score out of the four scores
                max_score = max(db_score, processed_score, image_score)

                # Apply penalty or update total score based on max_score
                if max_score == 0:
                    total_score -= penalty
                else:
                    total_score += max_score
                    match_found = True

            # Calculate the average score for the item
            avg_score = total_score / len(preprocessed_item_name_parts) if preprocessed_item_name_parts else 0

            # Add item to results if the average score meets the threshold
            if avg_score >= similarity_threshold and match_found:
                price = find_price_for_item(item)
                results.append({
                    'item_name': item.get('item_name', ''),
                    'price': price,
                    'shop_name': item_shop_name,
                    'similarity_score': avg_score,
                    'image_name': item.get('image_id')
                })

    # Sort the results by similarity score in descending order
    results = sorted(results, key=lambda x: x['similarity_score'], reverse=True)

    return results


def find_price_for_item(obj):
    """"""
    Extracts different price types (item_price, item_initial_price, item_member_price)
    from a DynamoDB item and returns them as a formatted string.

    :param obj: The DynamoDB item containing price information.
    :return: A formatted string containing price information or ""Price not found"" if no prices exist.
    """"""
    prices = []

    def try_convert_to_dict(value):
        """"""
        Utility function to safely convert a string into a dictionary if possible.

        :param value: The value to attempt conversion.
        :return: A dictionary if successful, otherwise the original value.
        """"""
        if isinstance(value, str):
            try:
                return ast.literal_eval(value)  # Attempt to evaluate as a Python literal
            except (ValueError, SyntaxError):
                try:
                    return json.loads(value)  # Attempt to parse as JSON
                except (ValueError, TypeError):
                    return value  # Return original value if conversion fails
        return value

    # Try to convert price fields to dictionaries, if applicable
    processed_price = try_convert_to_dict(obj.get('processed_item_price', None))
    initial_price = try_convert_to_dict(obj.get('processed_item_initial_price', None))
    member_price = try_convert_to_dict(obj.get('processed_item_member_price', None))

    # Handle processed_item_price
    if isinstance(processed_price, dict):
        prices.append(f""Price: {processed_price.get('item_price')}\n"")
    elif processed_price:  # If it's a string or number
        prices.append(f""Price: {processed_price}\n"")

    # Handle processed_item_initial_price
    if isinstance(initial_price, dict):
        prices.append(f""Initial price: {initial_price.get('item_initial_price')}\n"")
    elif initial_price:
        prices.append(f""Initial price: {initial_price}\n"")

    # Handle processed_item_member_price
    if isinstance(member_price, dict):
        prices.append(f""Member price: {member_price.get('item_member_price')}\n"")
    elif member_price:
        prices.append(f""Member price: {member_price}\n"")

    # Return the price strings or ""Price not found"" if no prices are available
    return """".join(prices) if prices else ""Price not found""


# --------------- User Interaction Handling ---------------

def get_available_languages():
    """"""
    Returns a dictionary of supported languages and their respective codes.
    """"""
    return {
        'en': 'English',
        'ru': 'Русский',
        'uk': 'Українська',
        'cs': 'Čeština'
    }


# Language selection prompt
def language_selection(chat_id):
    """"""
    Sends a language selection prompt to the user with inline buttons for language choices.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""inline_keyboard"": [
            [{""text"": ""English"", ""callback_data"": ""lang_en""}],
            [{""text"": ""Русский"", ""callback_data"": ""lang_ru""}],
            [{""text"": ""Українська"", ""callback_data"": ""lang_uk""}],
            [{""text"": ""Čeština"", ""callback_data"": ""lang_cs""}],
        ]
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Welcome! Please select your language:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Main menu display
def main_menu(chat_id):
    """"""
    Displays the main menu to the user with various options for tracking and comparing shop items.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""keyboard"": [
            [{""text"": ""🔍 Search for item""}],
            [{""text"": ""🛒 Add shop item to track price""}],
            [{""text"": ""🛍 Compare shopping list over shops""}],
            [{""text"": ""⚙️ Settings""}],
            [{""text"": ""ℹ️ About project""}],
        ],
        ""resize_keyboard"": True
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Main Menu"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Including shop to track at the start
def include_user_tracking_shops(chat_id):
    """"""
    Sends a list of shops for the user to start tracking items in. If no shops are excluded,
    it loads all shops into the excluded list first.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    shops = list(get_excluded_shops(chat_id))

    # If no excluded shops, exclude all shops initially
    if shops == []:
        exclude_all_shops(chat_id)

    # Send the list of shops with options to select from
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop from the list for item search and tracking. You can change this later in 'Settings'."",
        ""reply_markup"": {
            ""keyboard"": [[shop] for shop in get_excluded_shops(chat_id)] + [[""⬅️ Back to main menu""]],
            ""resize_keyboard"": True
        }
    })

    # Save the user's state as selecting shops
    save_user_state(chat_id, '/start_selecting_shops')


# Settings menu display with dynamic labels for photo group and text info
def settings_menu(chat_id):
    """"""
    Displays the settings menu with dynamic labels for photo group and text info toggling options.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""

    # Get the current state of photo groups and text info to display in the button labels
    photo_group_state = ""Enabled"" if is_photo_group_enabled(chat_id) else ""Disabled""
    text_info_state = ""Enabled"" if is_text_info_enabled(chat_id) else ""Disabled""
    receive_pdf_state = ""Enabled"" if is_pdf_receive_enabled(chat_id) else ""Disabled""

    # Create buttons with dynamic labels based on current settings
    buttons = {
        ""keyboard"": [
            [{""text"": ""🚫 Exclude some shops from tracking""}],
            [{""text"": ""✅ Include some shops in tracking""}],
            [{""text"": ""🛑 Remove shop item from tracking price""}],
            [{""text"": f""📄 Turn {'off' if receive_pdf_state == 'Enabled' else 'on'} Receiving New PDFs""}],
            [{""text"": f""📄 Turn {'off' if photo_group_state == 'Enabled' else 'on'} items photo groups""}],
            [{""text"": f""📄 Turn {'off' if text_info_state == 'Enabled' else 'on'} items text info""}],
            [{""text"": ""🌐 Change interface language""}],
            [{""text"": ""⬅️ Back to main menu""}],
        ],
        ""resize_keyboard"": True
    }

    # Send the settings menu
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Select an option from Settings:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Sending images as an album
def send_images_as_album(chat_id, media_group, shop_name):
    """"""
    Sends images in batches of up to 10 (as albums) to the user via Telegram.

    :param chat_id: The Telegram chat ID of the user.
    :param media_group: A list of tuples (S3 image path, local temp path) representing images to send.
    :param shop_name: The name of the shop to include in the caption of the first image in each batch.
    """"""

    def chunks(iterable, size):
        """"""Helper function to divide media_group into chunks of a given size.""""""
        iterator = iter(iterable)
        while True:
            batch = list(islice(iterator, size))
            if not batch:
                break
            yield batch

    for batch_index, batch in enumerate(chunks(media_group, 10)):
        media = []
        files = {}

        # Loop through the current batch of images
        for i, (s3_image_path, temp_path) in enumerate(batch):
            try:
                # Extract the image filename from the S3 path
                image_name = os.path.basename(s3_image_path)

                # Log the S3 path and temporary file for debugging
                logger.debug(f""Downloading image from S3: {s3_image_path} to {temp_path}"")

                # Use a temporary file to handle the downloaded image
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file_path = temp_file.name
                    # Download the image from S3 into the temporary file
                    download_file_from_s3(s3_image_path, temp_file_path)

                    # Read the file content for use in the Telegram API
                    with open(temp_file_path, 'rb') as image_file:
                        image_content = image_file.read()

                    # Prepare the files dictionary (it must have unique keys for each image)
                    files[f""photo{batch_index}_{i}""] = (image_name, image_content)

                    # Prepare the media array with references to the attached photos
                    media.append({
                        ""type"": ""photo"",
                        ""media"": f""attach://photo{batch_index}_{i}"",
                        ""caption"": shop_name if i == 0 else """"  # Add shop name as caption only to the first image
                    })

                    # Remove the temporary file after reading
                    os.remove(temp_file_path)

            except Exception as e:
                logger.error(f""Error processing image {image_name}: {str(e)}"")

        # If no valid media is available, log an error and return
        if not media:
            logger.error(""No valid media to send."")
            continue

        # Send the media group using the Telegram API
        try:
            response = requests.post(f""{API_URL}/sendMediaGroup"", files=files, data={
                ""chat_id"": chat_id,
                ""media"": json.dumps(media)  # Convert the media list to a JSON string
            })

            logger.debug(f""Telegram API response for sendMediaGroup: {response.status_code}, {response.text}"")
        except Exception as e:
            logger.error(f""Error sending media group: {str(e)}"")


def send_single_pdf(
        chat_ids,
        file_source,
        shop_name
):
    """"""
    Sends a single PDF file to multiple users via Telegram using a file path.

    :param chat_ids: A list of Telegram chat IDs of the users.
    :param file_source: The file path of the PDF to send.
    :param shop_name: The name of the shop to include in the caption of the PDF.
    """"""
    try:
        # Download the file from S3 only once
        # Extract the original filename from the S3 path
        original_filename = os.path.basename(file_source)
        logger.debug(f""Original filename extracted: {original_filename}"")

        # Create a temporary file with the original filename and extension
        with tempfile.NamedTemporaryFile(delete=False, suffix="".pdf"") as temp_file:
            temp_file_path = temp_file.name
            logger.debug(f""Temporary file created at {temp_file_path}"")

            # Download the file from S3 (replace this with your actual download logic)
            download_file_from_s3(file_source, temp_file_path)

        # Iterate over each chat_id and send the file to each user
        for chat_id in chat_ids:
            logger.debug(f""Sending PDF to chat_id: {chat_id}"")

            # Prepare the base payload for each user
            data = {
                ""chat_id"": chat_id,
                ""caption"": f""New {shop_name} letak available""
            }

            # Send via multipart/form-data (local file upload)
            with open(temp_file_path, 'rb') as pdf_file:
                logger.debug(f""Opened local file: {temp_file_path} for chat_id: {chat_id}"")

                # Prepare multipart form-data
                files = {
                    ""document"": (original_filename, pdf_file, ""application/pdf"")
                }

                # Send the document via multipart form-data
                logger.debug(f""Sending document via multipart/form-data for chat_id: {chat_id}..."")
                response = requests.post(f""{API_URL}/sendDocument"", files=files, data=data)

                # Log the response from Telegram for debugging purposes
                logger.debug(
                    f""Telegram API response for sendDocument (chat_id: {chat_id}): {response.status_code}, {response.text}"")

        # Clean up the temporary file after sending to all users
        logger.debug(f""Cleaning up temporary file: {temp_file_path}"")
        os.remove(temp_file_path)
        logger.debug(f""Temporary file {temp_file_path} removed successfully"")

    except ValueError as ve:
        logger.error(f""ValueError: {str(ve)}"")
    except Exception as e:
        logger.error(f""Error sending document '{file_source}' to chat IDs {chat_ids}: {str(e)}"")


# --------------- Message Processing ---------------

def handle_start_command(chat_id, state):
    description = ""Welcome to the Smart Shopping Bot! I will help you track prices, manage sale sheets, and find the best shopping paths.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": description})

    if state == ""new_user"":
        # Set default preferences for new users
        preferences = get_user_preferences(chat_id)
        if 'photo_group_enabled' not in preferences:
            preferences['photo_group_enabled'] = True  # Default to show photo groups
        if 'text_info_enabled' not in preferences:
            preferences['text_info_enabled'] = False  # Default to hide text info
        if 'receive_pdf_enabled' not in preferences:
            preferences['receive_pdf_enabled'] = True  # Default to send pdf after new validation pipeline check
        save_user_preferences(chat_id, preferences)

        # Guide user through initial steps of setup: language selection or shop inclusion
        if get_user_language(chat_id) is None:
            language_selection(chat_id)
            save_user_state(chat_id, None)
        elif get_included_shops(chat_id) is None:
            include_user_tracking_shops(chat_id)
            save_user_state(chat_id, None)
        else:
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # If user is not new, take them directly to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_shop_selection(chat_id, text):
    if text == ""➕ Add another shop"":
        include_user_tracking_shops(chat_id)
    elif text == ""➡️ Save tracking shop list. Return to the main menu"":
        if not get_included_shops(chat_id):
            # No shops have been included yet, notify the user
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select at least one shop from the list:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(get_excluded_shops(chat_id))] + [
                        [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True}
            })
        else:
            # At least one shop is included, allow returning to the menu
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # Get the list of excluded shops
        shops = list(get_excluded_shops(chat_id))
        # Check if the input text is a valid shop
        if text in shops:
            include_shop(chat_id, text)
            # Ask if the user wants to add more shops or return to the main menu
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Save tracking shop list. Return to the main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, '/start_selecting_shops')
        elif text == ""⬅️ Back to main menu"":
            # Check if any shops are included before allowing return to the main menu
            if not get_included_shops(chat_id):
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please select at least one shop before returning to the menu:"",
                    ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                     ""resize_keyboard"": True}
                })
            else:
                main_menu(chat_id)
                save_user_state(chat_id, None)

        else:
            # The user input is not a valid shop, ask them to select again
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a valid shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_search_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please enter the name of the item you want to search for."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'searching_item')


def handle_add_shop_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please provide the name of the shop item you want to track."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'adding_item')


def handle_settings_button(chat_id):
    settings_menu(chat_id)
    save_user_state(chat_id, 'in_settings')


def handle_compare_shop_list_button(chat_id):
    # Clean preferences after unexpected last user manipulations
    preferences = get_user_preferences(chat_id)
    preferences['selected_shops'] = []
    preferences['item_list'] = []
    save_user_preferences(chat_id, preferences)

    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop or list of shops from your history."",
        ""reply_markup"": {
            ""keyboard"": [[""List of all shops""], [""Lists of shops from history""], [""⬅️ Back to main menu""]],
            ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'selecting_shops')


def handle_about_button(chat_id):
    about_text = ""This bot helps you optimize your shopping by tracking prices, managing sale sheets, and finding the best shopping routes.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": about_text})
    main_menu(chat_id)
    save_user_state(chat_id, None)


def handle_item_adding_searching_state(chat_id, state, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Handle the case where user provides a single item name or multiple item names (split by commas or new lines)
        item_names = [name.strip() for name in text.split("","")]

        # Call the new find_item method with a list of item names
        found_items = find_item(item_names=item_names, included_shops=get_included_shops(chat_id))

        # If adding the item for tracking
        if state == 'adding_item':
            # Loop through each item name and handle them separately for adding to tracking
            for item_name in item_names:
","                added = add_tracked_item(chat_id, item_name)

                if added:
                    response = f""'{item_name}' saved for tracking. I will notify you when '{item_name}' has a valid sale.""
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                else:
                    tracked_items = get_tracked_items(chat_id)
                    response = f""'{item_name}' is already in your tracking list. Here is your current list:\n"" + ""\n"".join(
                        tracked_items)
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                    main_menu(chat_id)
                    save_user_state(chat_id, None)
                    return

        # If searching for the item(s)
","        if found_items:
            items_by_shop = {}

            # Group found items by shop
            for found_item in found_items:
                shop_name = found_item['shop_name']
                if shop_name not in items_by_shop:
                    items_by_shop[shop_name] = []
                items_by_shop[shop_name].append(found_item)

            photo_group_enabled = is_photo_group_enabled(chat_id)
            text_info_enabled = is_text_info_enabled(chat_id)

            # Iterate over each shop and its associated items
            for shop_name, shop_items in items_by_shop.items():
                # Prepare the initial response message
                response = f""Here is what I found for '{', '.join(item_names)}' in {shop_name}:\n""
                media_group = []

                # Process each found item for the shop
                for found_item in shop_items:
                    if text_info_enabled:
                        response += f""- {found_item['item_name']} at {shop_name}: {found_item['price']}\n""

                    s3_image_dir = found_item.get('image_name')
                    # Collecting images for the media group (album) if photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")

                # Send media group (photos) if enabled
                if media_group and photo_group_enabled:
                    logger.debug(f""Sending media group for {shop_name} with {len(media_group)} images"")
                    send_images_as_album(chat_id, media_group, shop_name)

                # Send text response with item details if text info is enabled
                if text_info_enabled:
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

        else:
            # If no items found, notify the user
            requests.post(f""{API_URL}/sendMessage"",
                          json={""chat_id"": chat_id, ""text"": f""No items found for '{', '.join(item_names)}'.""})

        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_in_settings_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""📄 Turn off Receiving New PDFs"" or text == ""📄 Turn on Receiving New PDFs"":
        current_receive_pdf_state = is_pdf_receive_enabled(chat_id)
        # Safe to toggle photo groups
        set_pdf_receive_enabled(chat_id, not current_receive_pdf_state)
        new_state = ""enabled"" if not current_receive_pdf_state else ""disabled""
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": f""Receiving of new PDFs has been {new_state}.""
        })
        settings_menu(chat_id)
    elif text == ""📄 Turn on items photo groups"" or text == ""📄 Turn off items photo groups"":
        # Toggle the photo group setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_text_info_state and current_photo_group_state:
            # Cannot disable photo groups if text info is already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Text info is already disabled, so photo groups cannot be turned off.""
            })
        else:
            # Safe to toggle photo groups
            set_photo_group_enabled(chat_id, not current_photo_group_state)
            new_state = ""enabled"" if not current_photo_group_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item photo groups are now {new_state}.""
            })
            settings_menu(chat_id)

    elif text == ""📄 Turn on items text info"" or text == ""📄 Turn off items text info"":
        # Toggle the text info setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_photo_group_state and current_text_info_state:
            # Cannot disable text info if photo groups are already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Photo groups are already disabled, so text info cannot be turned off.""
            })
        else:
            # Safe to toggle text info
            set_text_info_enabled(chat_id, not current_text_info_state)
            new_state = ""enabled"" if not current_text_info_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item text info is now {new_state}.""
            })
            settings_menu(chat_id)
    elif text == ""🚫 Exclude some shops from tracking"":
        # Retrieve the included shops that can be excluded
        included_shops = get_included_shops(chat_id)

        # If there are no included shops, inform the user
        if not included_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""No shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of shops that can be excluded
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to exclude from tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in included_shops] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'excluding_shop')
    elif text == ""✅ Include some shops in tracking"":
        # Retrieve the excluded shops that can be included
        excluded_shops = get_excluded_shops(chat_id)

        # If all shops are already included, inform the user
        if not excluded_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""All shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of excluded shops that can be included
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to include in tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(excluded_shops)] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'including_shop')
    elif text == ""🛑 Remove shop item from tracking price"":
        items_to_remove = get_tracked_items(chat_id)
        if items_to_remove:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Select an item to remove from tracking:"",
                ""reply_markup"": {""keyboard"": [[item] for item in items_to_remove] + [[""⬅️ Back to settings""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'removing_item')
        else:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any items being tracked."",
            })
            settings_menu(chat_id)
            save_user_state(chat_id, 'in_settings')


def handle_excluding_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        exclude_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' excluded from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_including_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        include_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' included for tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_removing_item_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        remove_tracked_item(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Item '{text}' removed from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_shop_list_history_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        try:
            index = int(text) - 1
            shop_history = get_user_selected_shops_history(chat_id)
            if 0 <= index < len(shop_history):
                selected_history_list = shop_history[index]
                preferences = get_user_preferences(chat_id)
                preferences['selected_shops'] = selected_history_list
                save_user_preferences(chat_id, preferences)
                # Proceed to item entry
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please provide your shopping list, one per line and send."",
                    ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
                })
                preferences['item_list'] = []
                save_user_preferences(chat_id, preferences)
                save_user_state(chat_id, 'entering_items')
            else:
                raise IndexError
        except (ValueError, IndexError):
            # Handle invalid input
            shop_history = get_user_selected_shops_history(chat_id)
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]
            text_message = ""Invalid selection. Please choose a number from the list:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })


def handle_selecting_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')
    elif text == ""Lists of shops from history"":
        # Get the user's shop history
        shop_history = get_user_selected_shops_history(chat_id)
        if shop_history:
            # Build the keyboard buttons with numbers
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]

            # Corrected prompt and display
            text_message = ""Please select a shop list from your history:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""

            # Send the message with the keyboard
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })

            # Save the user state
            save_user_state(chat_id, 'shop_list_history')
        else:
            shops = get_all_shops()
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any history saved list. Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
    else:
        # Get the list of available shops
        shops = get_all_shops()
        # Check if the input text is a valid shop
        if text in shops:
            # Save the selected shop
            preferences = get_user_preferences(chat_id)
            selected_shops = preferences.get('selected_shops', [])
            if text not in selected_shops:
                selected_shops.append(text)
                preferences['selected_shops'] = selected_shops
                save_user_preferences(chat_id, preferences)
            # Ask if the user wants to add more shops or continue
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Continue with shop list""],
                                 [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

        else:
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_confirming_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➕ Add another shop"":
        # Exclude already selected shops
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        shops = [shop for shop in get_all_shops() if shop not in selected_shops]
        if shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [
                    [""⬅️ Back to main menu""] + [""➡️ Continue with shop list""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            # All shops have been selected
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have selected all available shops."",
                ""reply_markup"": {
                    ""keyboard"": [[""➡️ Continue with shop list""], [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')

    else:
        # Handle unexpected input
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": ""Please select an option from the menu.""
        })


def handle_entering_items_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Add item to the list
        preferences = get_user_preferences(chat_id)
        item_list = preferences.get('item_list', [])
        item_list.extend(text.split('\n'))
        preferences['item_list'] = item_list
        save_user_preferences(chat_id, preferences)
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        item_list = preferences.get('item_list', [])
        response = ""Here are the items found in the selected shops:\n""
        # Retrieve user preferences for photo group and text info settings
        photo_group_enabled = is_photo_group_enabled(chat_id)
        text_info_enabled = is_text_info_enabled(chat_id)
        # List to collect all images for the media group
        for shop in selected_shops:
            if text_info_enabled:
                response += f""\nItems in {shop}:\n""
            media_group = []  # List to collect all images for the media group
            # Call find_item once for all items in the current shop
            found_items = find_item(item_names=item_list, shop_name=shop)

            if found_items:
                for found_item in found_items:
                    # Extract price and image path details from the found item
                    price = found_item.get('price')
                    s3_image_dir = found_item.get('image_name')

                    logger.debug(f""Found item: {found_item}"")
                    logger.debug(f""Price: {price}, Image Path: {s3_image_dir}"")

                    # Include price details in the response if text info is enabled
                    if text_info_enabled:
                        # Add the item price or ""Price not found"" based on availability
                        if price:
                            response += f""- {found_item['item_name']} at {shop}: {price}\n""
                        else:
                            response += f""- {found_item['item_name']} at {shop}: Price not found\n""

                    # Process the image only if the photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        # Extract the filename from the full S3 path
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""

                        # Add the image filename and its corresponding local path to the media group
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")
            else:
                # If none of the items are found in the shop, add a not found message to the response
                for item_name in item_list:
                    response += f""- {item_name}: Not found in {shop}\n""

            # After looping through items, send the images as an album if there are any and photo group is enabled
            logger.debug(f""Media group length: {len(media_group)}. Photo group enabled: {photo_group_enabled}"")
            if media_group and photo_group_enabled:
                logger.debug(f""Sending media group for shop: {shop}"")
                send_images_as_album(chat_id, media_group, shop)
                media_group.clear()  # Clear the media group after sending to avoid duplicate entries
            else:
                logger.debug(f""No images to send or photo group is disabled"")

            # Send the final response with text results if text info is enabled
        if text_info_enabled:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

            # Go back to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)

        # Clear selected shops and item list
        preferences['selected_shops'] = []
        preferences['item_list'] = []
        save_user_preferences(chat_id, preferences)


def process_message(update):
    chat_id = update['message']['chat']['id']
    text = update['message'].get('text')

    # Get current user state
    state = get_user_state(chat_id)

    # Main dispatcher based on text command or state
    if text == ""/start"":
        handle_start_command(chat_id, state)
    elif state == '/start_selecting_shops':
        handle_shop_selection(chat_id, text)
    elif text == ""🔍 Search for item"":
        handle_search_button(chat_id)
    elif text == ""🛒 Add shop item to track price"":
        handle_add_shop_button(chat_id)
    elif text == ""⚙️ Settings"":
        handle_settings_button(chat_id)
    elif text == ""🛍 Compare shopping list over shops"":
        handle_compare_shop_list_button(chat_id)
    elif text == ""ℹ️ About project"":
        handle_about_button(chat_id)
    elif state == 'adding_item' or state == ""searching_item"":
        handle_item_adding_searching_state(chat_id, state, text)
    elif state == 'in_settings':
        handle_in_settings_state(chat_id, text)
    elif state == 'excluding_shop':
        handle_excluding_shop_state(chat_id, text)
    elif state == 'including_shop':
        handle_including_shop_state(chat_id, text)
    elif state == 'removing_item':
        handle_removing_item_state(chat_id, text)
    elif state == ""shop_list_history"":
        handle_shop_list_history_state(chat_id, text)
    elif state == 'selecting_shops':
        handle_selecting_shops_state(chat_id, text)
    elif state == 'confirming_shops':
        handle_confirming_shops_state(chat_id, text)
    elif state == 'entering_items':
        handle_entering_items_state(chat_id, text)
    else:
        if text == ""⬅️ Back to main menu"":
            main_menu(chat_id)
            save_user_state(chat_id, None)
        else:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id,
                                                          ""text"": ""I'm sorry, I didn't understand that. Please choose an option from the menu.""})


# Handle callback queries from inline buttons (e.g., language selection)
def process_callback_query(update):
    """"""
    Processes the callback queries triggered by inline buttons, such as language selection.
    :param update: The update payload containing the callback query details.
    """"""
    query = update['callback_query']
    chat_id = query['message']['chat']['id']  # Extract the chat ID of the user
    message_id = query['message']['message_id']  # Extract the message ID to edit it later
    data = query.get('data')  # Retrieve the callback data (e.g., 'lang_en')
    callback_query_id = query['id']  # The ID for answering the callback query

    # Check if the callback is related to language selection (data starts with 'lang_')
    if data.startswith('lang_'):
        language_code = data.split('_')[1]  # Extract the language code (e.g., 'en', 'ru')
        set_user_language(chat_id, language_code)  # Save the selected language to user preferences

        # Answer the callback query to stop Telegram's ""loading"" animation
        answer_payload = {
            ""callback_query_id"": callback_query_id,
            ""text"": ""Language updated!"",  # Confirmation message to the user
            ""show_alert"": False  # Do not show a popup, just stop the loading animation
        }
        requests.post(f""{API_URL}/answerCallbackQuery"", json=answer_payload)

        # Edit the original message to remove the inline keyboard and update the text
        new_text = f""Language selected! You have set your language to: {get_available_languages().get(language_code, 'Unknown')}""
        edit_payload = {
            ""chat_id"": chat_id,
            ""message_id"": message_id,
            ""text"": new_text,
            ""reply_markup"": {}  # Remove the inline keyboard by setting an empty reply_markup
        }
        requests.post(f""{API_URL}/editMessageText"", json=edit_payload)

        # Proceed to include the shops tracking list for the user
        include_user_tracking_shops(chat_id)


def handle_pdf_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')

    s3_pdf_path = ""pdfs/"" + pdf_file
    send_single_pdf(users_id_list, s3_pdf_path, shop_name)


def handle_tracked_items_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')
    tracked_items_list = update.get('tracked_items_list', [])  # A list of tracked items for each user

    # Prepare global variables for tracking all responses and media to send in batches
    user_responses = {}  # Dictionary to hold responses per user
    user_media_groups = {}  # Dictionary to hold media groups per user

    # Iterate over each user and their tracked items
    for i, user_id in enumerate(users_id_list):
        user_tracked_items = tracked_items_list[i]

        # Check if text and photo group sending is enabled for the user
        text_info_enabled = is_text_info_enabled(user_id)
        photo_group_enabled = is_photo_group_enabled(user_id)

        # If there are no tracked items for the user, skip to the next user
        if not user_tracked_items:
            continue

        # Call the find_item method once for all tracked items at once
        matched_items = find_item(item_names=user_tracked_items, shop_name=shop_name, pdf_filename=pdf_file)

        # If no matched items are found, skip to the next user
        if not matched_items:
            if text_info_enabled:
                user_responses[user_id] = ""No items found in this flyer.""
            continue

        # Prepare lists to store matched items and media for this user
        user_response = ""Your tracked items are now available:\n\n""  # Friendly message
        media_group = []

        # Process the matched items
        for found_item in matched_items:
            price = found_item.get('price')
            s3_image_dir = found_item.get('image_name')

            # Include price details in the response if text info is enabled
            if text_info_enabled:
                if price:
                    user_response += f""- {found_item['item_name']} at {shop_name}: {price}\n""
                else:
                    user_response += f""- {found_item['item_name']} at {shop_name}: Price not found\n""

            # Process the image only if the photo group is enabled
            if photo_group_enabled and s3_image_dir:
                image_filename = os.path.basename(s3_image_dir)
                local_image_path = f""/tmp/{image_filename}""
                media_group.append((s3_image_dir, local_image_path))

        # Add the user's response and media group to the respective dictionaries
        if text_info_enabled and user_response.strip():
            user_responses[user_id] = user_response
        if media_group and photo_group_enabled:
            user_media_groups[user_id] = media_group

    # Now send out the responses and media groups in batches to minimize API calls
    message = f""Your tracked items are now available at {shop_name}""
    # Send media groups for users
    for user_id, media_group in user_media_groups.items():
        send_images_as_album(user_id, media_group, message)

    # Send text responses for users
    for user_id, response in user_responses.items():
        requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": user_id, ""text"": response})


def lambda_handler(event, context):
    """"""
    This function will act as the webhook to handle Telegram updates when deployed to AWS Lambda.
    It will process both regular messages and callback queries.
    """"""
    try:
        # Parse the body from the incoming event
        if 'body' in event:
            update = json.loads(event['body'])  # Extract the JSON body from the Lambda event

            # Process message or callback query based on the update type
            if 'message' in update:
                process_message(update)  # Call your process_message function
            elif 'callback_query' in update:
                process_callback_query(update)  # Call your process_callback_query function

            process_type = update.get('process_type', None)
            if process_type == 'tracked_items_list':
                # Process the tracked items-related webhook
                handle_tracked_items_newsletter(update)
            elif process_type == 'pdf_newsletter':
                handle_pdf_newsletter(update)

            # Return a success response to Telegram
            return {
                'statusCode': 200,
                'body': json.dumps({'status': 'ok'})
            }
        else:
            logger.error('No body found in the request')
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Bad Request'})
            }

    except Exception as e:
        logger.error(f""Error processing the request: {str(e)}"")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal Server Error'})
        }
","')
    elif text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Get the list of excluded shops
        shops = list(get_excluded_shops(chat_id))
        # Check if the input text is a valid shop
        if text in shops:
            include_shop(chat_id, text)
            # Notify the user
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"":"
../airflow/dags/utils/pdf_utils.py,"import os
from pdf2image import convert_from_path
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, get_pdf_item_from_dynamodb
import logging

TEMP_DIR = '/tmp'  # Modify if needed for your environment
PDF_S3_PATH = 'pdfs'  # Define the S3 directory where your PDF files are stored
PAGES_S3_PATH = 'pages/valid'  # Directory in S3 where pages are uploaded
poppler_path = ""/usr/bin""

def split_pdf_to_pages(filename, shop_name):
    """"""Split PDF into pages and upload to S3, returning full S3 paths for the pages.""""""
    if not filename or not shop_name:
","        raise Exception(""Filename or Shop Name missing!"")

    # Fetch metadata from DynamoDB
    response = get_pdf_item_from_dynamodb(filename, shop_name)
    file_entry = response.get('Item')

    if not file_entry:
        raise Exception(f""File {filename} not found in DynamoDB"")

    # Check if the pages already exist in S3
","    page_s3_paths = []  # Store full S3 paths for pages
    base_filename = os.path.splitext(filename)[0]

    logging.info(f""Checking if pages for {filename} already exist in S3..."")

    # Define the path for the PDF in S3 (in the 'pdfs' directory)
    s3_pdf_path = f'{PDF_S3_PATH}/{filename}'

    # Download the PDF from S3 to a temporary location
    file_path = os.path.join(TEMP_DIR, filename)

    # Log paths for debugging
    logging.info(f""Checking if file exists in S3 path: {s3_pdf_path}"")

    logging.info(f""Downloading file from S3 path: {s3_pdf_path} to local path: {file_path}"")

    try:
        download_file_from_s3(s3_pdf_path, file_path)
    except Exception as e:
        logging.error(f""Failed to download file from S3: {e}"")
        raise e

    # Convert PDF into image pages
    images = convert_from_path(file_path, dpi=250, poppler_path=poppler_path)

    for i, image in enumerate(images):
        page_filename = f""{base_filename}_page_{i + 1}.png""
        page_path = os.path.join(TEMP_DIR, page_filename)

        # Save the image locally
        image.save(page_path, 'PNG')

        # Upload each page to S3 in the 'pages/valid/' directory
        s3_page_path = f'{PAGES_S3_PATH}/{page_filename}'
        upload_file_to_s3(page_path, s3_page_path)

        # Add full S3 path of the page to the list
        page_s3_paths.append(s3_page_path)

    # Return the list of full S3 paths for the uploaded pages
    return page_s3_paths
","        raise ValueError(""The input argument is required"")

"
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
","        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
","        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
","    img = cv2.imread(os.path.join(request.files[""image""].filename)))

    try:
        # Perform the actual prediction and return its classification label and confidence score
"
../airflow/dags/validity_check.py,"import itertools

from airflow.sensors.external_task import ExternalTaskSensor
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from utils.s3_dynamodb_utils import update_item_in_dynamodb, download_file_from_s3
import boto3
import requests
from datetime import datetime

# DynamoDB Table Details
TABLE_NAME = 'pdf_metadata'
DETECTED_TABLE = 'detected_data'
USERS_TABLE = 'user_preferences'
dynamodb = boto3.resource('dynamodb')

pdf_table = dynamodb.Table(TABLE_NAME)
users_table = dynamodb.Table(USERS_TABLE)
detected_table = dynamodb.Table(DETECTED_TABLE)

WEBHOOK_URL = ""https://klffswbb85.execute-api.eu-west-1.amazonaws.com/default/salesTelegramBotHandler""


# Function to check file validity and update detected items only if the status changes
def check_validity_and_update_detected():
    # Initialize DynamoDB clients

    # Get today's date
    today = datetime.utcnow().date()

    changed_to_valid = []
    changed_to_invalid = []

    # Scan the pdf_metadata table to get all items
    pdf_response = pdf_table.scan()
    pdf_items = pdf_response.get('Items', [])

    # Process each PDF item
    for pdf_item in pdf_items:
        # Remove .pdf from filename for comparison with detected items
        pdf_name_without_ext = pdf_item['filename'].replace('.pdf', '')

        valid_from = datetime.strptime(pdf_item['valid_from'], '%Y-%m-%d').date()
        valid_to = datetime.strptime(pdf_item['valid_to'], '%Y-%m-%d').date()

        # Determine the current validity status
        current_valid_status = pdf_item.get('valid', None)

        # Check if the file should be valid or invalid based on today's date
        is_valid_now = valid_from <= today <= valid_to

        # If the current status differs from the computed status, it means the status has changed
        if current_valid_status != is_valid_now:
            # Update the valid field in the pdf_metadata table using the utility method
            update_item_in_dynamodb(
                table_name=TABLE_NAME,
                key={'filename': pdf_item['filename'], 'shop_name': pdf_item[""shop_name""]},
                update_expression=""SET valid = :v"",
                expression_attribute_values={':v': is_valid_now}
            )

            # Append the filename to the appropriate list based on the new validity status
            if is_valid_now:
                changed_to_valid.append(pdf_name_without_ext)
            else:
                changed_to_invalid.append(pdf_name_without_ext)

    # Return only files that changed their validity status
    return {
        'valid': changed_to_valid,
        'invalid': changed_to_invalid
    }


def update_detected_items_task(**context):
    """"""
    Task to update detected items based on the output from the previous task.
    """"""
    # Get the valid and invalid files from the context (returned by the previous task)
    task_instance = context['task_instance']

    # Pull the whole dictionary returned by the previous task
    status_change = task_instance.xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' and 'invalid' lists from the returned dictionary
    valid_files = status_change.get('valid', [])
    invalid_files = status_change.get('invalid', [])

    print(f""Debug: valid_files from XCom: {valid_files}"")
    print(f""Debug: invalid_files from XCom: {invalid_files}"")

    if not valid_files and not invalid_files:
        print(""Debug: No files were found for update."")
    else:
        # Update the detected_items table based on valid and invalid files
        update_detected_items_based_on_status(valid_files, invalid_files)


def update_detected_items_based_on_status(valid_files, invalid_files):
    """"""
    Update the detected_items table based on the lists of files that changed status (valid or invalid).
    This function optimizes the update by filtering detected items only with relevant file substrings.
    """"""

    # Remove .pdf extensions from filenames
    valid_files = [file.replace('.pdf', '') for file in valid_files]
    invalid_files = [file.replace('.pdf', '') for file in invalid_files]

    # Combine valid and invalid files
    all_files = valid_files + invalid_files
    print(f""Debug: all_files to be checked (without .pdf): {all_files}"")

    valid_changed_items = []

    detected_response = detected_table.scan()
    detected_items = detected_response.get('Items', [])

    print(f""Debug: Detected items retrieved from DynamoDB: {detected_items}"")

    # Process each detected item
    for detected_item in detected_items:
        detected_image_path = detected_item['image_id']

        # Debug: Show the current detected image path
        print(f""Debug: Processing detected item with image_id: {detected_image_path}"")

        # Check if any file name (without .pdf) is a substring of the image_id
        for file_substr in all_files:
            if file_substr in detected_image_path:
                new_valid_status = file_substr in valid_files

                # Debug: Show the new validity status for the detected item
                print(f""Debug: Changing validity of {detected_image_path} to {new_valid_status}"")

                if detected_item.get('valid') != new_valid_status:
                    # Update the valid field in DynamoDB
                    update_item_in_dynamodb(
                        table_name=DETECTED_TABLE,
                        key={'image_id': detected_item['image_id']},
                        update_expression=""SET valid = :v"",
                        expression_attribute_values={':v': new_valid_status}
                    )
                    print(f""Debug: Updated detected item {detected_item['image_id']} validity to {new_valid_status}"")

                    if new_valid_status:
                        valid_changed_items.append(detected_item)

    print(f""Debug: List of valid_changed_items: {valid_changed_items}"")

    return valid_changed_items


def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    pdf_table = dynamodb.Table(TABLE_NAME)
    response = pdf_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


def regroup_by_shop():
    """"""
    Iterates over all users and creates two columns:
    1. Users for each shop with included shops (all_shops - excluded_shops).
    2. Users for each shop with included shops and receive_pdf_enabled set to True.
    """"""
    # Dictionary to hold the regrouped data
    shop_user_map = {
        ""included_shops"": {},
        ""included_shops_and_receive_pdf"": {}
    }

    # Retrieve all unique shops from the metadata
    all_shops = get_all_shops()

    # Scan the users table to get all user records
    response = users_table.scan()

    # Iterate over all users (items in DynamoDB)
    for item in response['Items']:
        chat_id = item['chat_id']
        receive_pdf_enabled = item.get('receive_pdf_enabled', False)  # Default to False if not set
        excluded_shops = item.get('excluded_shops', [])

        # Calculate included shops (all_shops - excluded_shops)
","        included_shops = [shop for shop in all_shops if shop not in excluded_shops]

        # Iterate over all included shops and add users to corresponding shop keys
        for shop in included_shops:
            # Add to ""included_shops"" column (all users with included shops)
            if shop not in shop_user_map[""included_shops""]:
                shop_user_map[""included_shops""][shop] = []
            shop_user_map[""included_shops""][shop].append(chat_id)

            # Add to ""included_shops_and_receive_pdf"" column (users with included shops AND receive_pdf_enabled=True)
            if receive_pdf_enabled:
                if shop not in shop_user_map[""included_shops_and_receive_pdf""]:
                    shop_user_map[""included_shops_and_receive_pdf""][shop] = []
                shop_user_map[""included_shops_and_receive_pdf""][shop].append(chat_id)

    # Return the regrouped data structure
","    return shop_user_map


def regroup_shop_to_valid_file(valid_files):
    shop_file_map = {}

    # Ensure valid_files have no .pdf extension
    valid_files = [file.replace('.pdf', '') for file in valid_files]

    # Scan the pdf metadata table
    response = pdf_table.scan()

    # Process each item in the table
    for item in response['Items']:
        shop_name = item['shop_name']
        file_name = item['filename'].replace('.pdf', '')  # Remove .pdf for comparison

        # Check if the file name matches any valid file (without .pdf)
        if file_name in valid_files:
            if shop_name not in shop_file_map:
                shop_file_map[shop_name] = []
            shop_file_map[shop_name].append(item['filename'])  # Use original filename with .pdf for sending

    return shop_file_map


def send_webhook(process_type, shop_name, users_id_list, pdf_file, tracked_item=None):
    """"""
    Sends a POST request to the specified webhook URL with the provided data.
    """"""
    payload = {
        'process_type': process_type,
        'shop_name': shop_name,
        'users_id_list': users_id_list,
        'pdf_file': pdf_file,
    }
    if tracked_item:
        payload['tracked_items_list'] = tracked_item  # Include the tracked item if provided

    # Send the POST request to the webhook
    response = requests.post(WEBHOOK_URL, json=payload)

    # Log the response (optional)
    if response.status_code == 200:
        if tracked_item:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}, tracked_item: {tracked_item}"")
        else:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}"")
    else:
        print(f""Failed to send data to webhook: {response.status_code}, {response.text}"")


def send_updates_in_telegram_task(**context):
    # Pull the whole dictionary returned by the previous task
    status_change = context['task_instance'].xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' files from the returned dictionary
    valid_files = status_change.get('valid', [])

    # Debug print to check what valid files were pulled
    print(f""Debug: valid_files from XCom: {valid_files}"")

    # If no valid files, skip further processing
    if not valid_files:
        print(""Debug: No valid files to process."")
        return

    # Regroup data by shop
    regrouped_data = regroup_by_shop()
    print(f""Debug: regrouped_data: {regrouped_data}"")

    # Regroup valid files by shop
    regrouped_valid_files_data = regroup_shop_to_valid_file(valid_files)
    print(f""Debug: regrouped_valid_files_data: {regrouped_valid_files_data}"")

    for shop_name in regrouped_data['included_shops']:
        shop_included_users_with_receive_pdf = regrouped_data['included_shops_and_receive_pdf'].get(shop_name, [])
        shop_included_users = regrouped_data['included_shops'].get(shop_name, [])

        if shop_name in regrouped_valid_files_data:
            for pdf_file in regrouped_valid_files_data[shop_name]:
                # Debug print before sending the pdf_newsletter webhook
                print(f""Debug: Sending pdf_newsletter for shop: {shop_name}, pdf_file: {pdf_file}, ""
                      f""users_with_receive_pdf: {shop_included_users_with_receive_pdf}"")

                send_webhook('pdf_newsletter', shop_name, shop_included_users_with_receive_pdf, pdf_file)

                batch_size = 3
                user_batches = [list(group) for group in
                                itertools.zip_longest(*[iter(shop_included_users)] * batch_size)]

                for user_batch in user_batches:
                    valid_users = [user for user in user_batch if user is not None]
                    user_ids_list = []
                    user_tracked_items_list = []

                    for user_id in valid_users:
                        response = users_table.get_item(Key={'chat_id': user_id})
                        if 'Item' in response:
                            user_tracked_items = response['Item'].get('tracked_items', [])
                            if user_tracked_items:
                                user_ids_list.append(user_id)
                                user_tracked_items_list.append(user_tracked_items)

                    if user_ids_list and user_tracked_items_list:
                        # Debug print before sending the tracked_items_list webhook
                        print(f""Debug: Sending tracked_items_list for shop: {shop_name}, pdf_file: {pdf_file}, ""
                              f""user_ids_list: {user_ids_list}, tracked_items: {user_tracked_items_list}"")

                        send_webhook('tracked_items_list', shop_name, user_ids_list, pdf_file, user_tracked_items_list)


# Airflow DAG setup
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
}

with DAG(
        dag_id='check_file_validity_and_update_detected_items',
        default_args=default_args,
        schedule_interval='0 1 * * *',  # Runs at 08:00 AM UTC daily
        catchup=False
) as dag:
    # Task to check and update the PDF metadata validity
    check_files_and_update_detected = PythonOperator(
        task_id='check_validity_and_update_detected_task',
        python_callable=check_validity_and_update_detected,
    )

    # Task to update the detected items based on the result of the first task
    update_detected_items = PythonOperator(
        task_id='update_detected_items_task',
        python_callable=update_detected_items_task,
        provide_context=True
    )

    # Task to send updates in Telegram after updating detected items
    send_updates_in_telegram = PythonOperator(
        task_id='send_updates_in_telegram_task',
        python_callable=send_updates_in_telegram_task,
        provide_context=True
    )

    # Task dependencies
    check_files_and_update_detected >> update_detected_items >> send_updates_in_telegram
","        # Exclude all shops from the set of shops already in the dataframe
        # Only exclude shops which are not in the df
        included_shops = set(item['shop_name'] for item in item['included_shops']))

        # Add shops for each user, including those who are excluded but aren't included
        # (they don't appear in the df)
        for user_id in set(user['id']):
            if excluded_shops!= user_id:
                # Set the user's included shops for this user (all_shops - excluded_shops)
            # (except for"
../airflow/dags/utils/yolo_ocr_utils.py,"import json
import os
import tempfile

import cv2
import requests  # For sending HTTP requests to the deployed YOLO model
import logging
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, save_item_to_dynamodb

TEMP_DIR = ""/tmp""

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


def got_text_from_image(image_path):
    url = 'http://34.246.217.135:80/extract_text'  # Adjust the URL if necessary
    try:
        with open(image_path, 'rb') as image_file:
            files = {'image': image_file}
            response = requests.post(url, files=files)

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image: {response.status_code} - {response.text}"")
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


def got_text_from_image_box(image_path, box):
    url = 'http://34.246.217.135:80/extract_text_with_box'  # OCR API route
    try:
        logger.info(f""Sending bounding box to OCR API: {box}"")  # Log the bounding box

        with open(image_path, 'rb') as image_file:
            # Prepare the multipart/form-data request
","            files = {'image': image_file}
            # Send JSON data separately from form-data (image)
            json_data = {'box': box}

            response = requests.post(url, files=files, data={'json': json.dumps(json_data)})

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image_box: {response.status_code} - {response.text}"")
    except Exception as e:
        logger.error(f""Exception in got_text_from_image_box: {e}"")
        raise Exception(f""Exception in extract_text_from_image_box: {e}"")


","def run_yolo_on_pages(s3_input_images_filepaths, dynamodb_table_name, model='model1',
                      save_images=False, detection_output_path=None, include_ocr=False, padding=0):
    """"""
    This function runs the YOLO model on a list of images from S3 or local paths, saves the detection details
    to DynamoDB, and returns the detection results as a dictionary. Optionally, it can also save the detected ROI images.

    Args:
        s3_input_images_filepaths (list): List of S3 paths of input image files.
        detection_output_path (str): S3 directory where the detection .txt files should be saved.
        dynamodb_table_name (str): The name of the DynamoDB table to store detection details.
        save_images (bool): Whether to save the detected ROIs as images.
        model (str): The model to be used ('model1' or 'model2').

    Returns:
        dict: A dictionary of predictions with image paths as keys and detections as values.
        list: A list of saved ROI image paths if save_images is True.
    """"""
    logger.info(f""Starting YOLO processing on {len(s3_input_images_filepaths)} images using model: {model}"")

    predictions = {}  # Dictionary to store detections for each image
    s3_saved_images = []  # List to store S3 paths of saved ROI images if save_images is True

    for filepath in s3_input_images_filepaths:
        try:
            logger.info(f""Processing image: {filepath}"")

            # Download the image from S3 to local TMP_DIR
            local_image_path = os.path.join(TEMP_DIR, os.path.basename(filepath))  # Corrected local path
            download_file_from_s3(filepath, local_image_path)
            logger.info(f""Downloaded image from S3 to {local_image_path}"")

            # Run the prediction and detection using the deployed YOLO model
            with open(local_image_path, 'rb') as image_file:
                response = requests.post(
                    f""http://34.246.217.135:80/predict"",  # YOLO model endpoint
                    files={'image': image_file},
                    params={'model': model}
                )

            if response.status_code == 200:
                detections = response.json().get('detections', [])
                logger.info(f""Received {len(detections)} detections for {filepath}"")
            else:
                raise Exception(f""Error from YOLO model: {response.status_code} - {response.text}"")

            img = cv2.imread(local_image_path)  # Load the image for ROI extraction (if needed)

            # Store detections for this image
            predictions[filepath] = []  # Initialize a list to store all detections for this image

            # Initialize a dictionary to store detections grouped by class
            detections_by_class = {}

            height, width = img.shape[:2]  # Get the image dimensions (height, width)

            for i, det in enumerate(detections):
                x1, y1, x2, y2 = det['box']  # Get bounding box coordinates
                class_name = det['class']  # Class name (e.g., 'shop_item')
                confidence = det['confidence']  # Confidence score for detection

                # Calculate width and height of the bounding box
                box_width = x2 - x1
                box_height = y2 - y1

                # Calculate 10% padding for width and height
                padding_w = int(box_width * 0.10)
                padding_h = int(box_height * 0.10)

                # Increase the bounding box by 10% padding on all sides, ensuring it stays within the image boundaries
                x1 = max(0, x1 - padding_w)
                y1 = max(0, y1 - padding_h)
                x2 = min(width, x2 + padding_w)
                y2 = min(height, y2 + padding_h)

                # Build the bounding box information, and add class_name to the detection item
                detection_item = {
                    'class_name': class_name,  # Add the class name here
                    'bounding_box': {
                        'x1': str(x1), 'y1': str(y1), 'x2': str(x2), 'y2': str(y2)
                    },
                    'confidence': str(confidence)
                }

                # Perform OCR if include_ocr is True
                if include_ocr:
                    # Prepare the bounding box for OCR
                    ocr_box = [x1, y1, x2, y2]  # Box format: [x1, y1, x2, y2]

                    # Step 1: Perform OCR directly on the bounding box area of the original image
                    object_text = got_text_from_image_box(local_image_path, ocr_box)

                    # Add OCR text to the detection item
                    detection_item['ocr_text'] = object_text
                    logger.info(f""OCR extracted text for class {class_name} in bounding box: {object_text}"")

                # Append detection item under the corresponding class_name
                if class_name not in detections_by_class:
                    detections_by_class[class_name] = []
                detections_by_class[class_name].append(detection_item)

                # Append the detection to the image's list of detections in the predictions dictionary
                predictions[filepath].append(detection_item)  # Now appending within the loop

            # After processing all detections, prepare the item for DynamoDB
            item_to_save = {
                'image_id': filepath,
                'detections': detections_by_class  # Grouped detections by class
            }

            # Save the detections to DynamoDB
            save_item_to_dynamodb(dynamodb_table_name, item_to_save)
            logger.info(f""Saved all detections for image {filepath} to DynamoDB"")

            # If save_images is True, extract ROI and save as PNG
            if save_images:
                for i, det in enumerate(detections):
                    x1, y1, x2, y2 = det['box']
                    class_name = det['class']
                    roi = img[y1:y2, x1:x2]  # Extract ROI from image
                    roi_filename = f""{os.path.basename(filepath).replace('.png', '')}_det_{i}_{class_name}.png""
                    roi_local_path = os.path.join(TEMP_DIR, roi_filename)

                    # Save the ROI image locally as PNG
                    cv2.imwrite(roi_local_path, roi)
                    logger.info(f""Saved ROI to {roi_local_path}"")

                    # Define the S3 path where the ROI will be uploaded
                    s3_roi_path = f""{detection_output_path}/images/{roi_filename}""

                    # Upload the ROI image to S3
                    upload_file_to_s3(roi_local_path, s3_roi_path)
                    s3_saved_images.append(s3_roi_path)
                    logger.info(f""Uploaded ROI to S3: {s3_roi_path}"")

                    # Clean up the temporary local ROI file after uploading
                    os.remove(roi_local_path)
                    logger.info(f""Deleted local ROI file: {roi_local_path}"")

        except Exception as e:
            logger.error(f""Error processing image {filepath}: {e}"")

    # Return the predictions dictionary and saved image paths if save_images is True
    return predictions, s3_saved_images if save_images else predictions","            formdata = {""image"": image_file, ""box"": box}

            headers = {'Content-Type': 'application/octet-stream'}

            resp = requests.post(url, files=formdata, headers=headers)

        if resp.status_code == 200:
            data = resp.json()
            return data.get(""extracted_text_with_box"", """")
        else:
            raise Exception(f""Error from OCR API: {resp.status_code} - {resp.text}"")


def yolo_process(s3_input_images_"
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
","        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
","                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","            try:
            # Try to authenticate
            auth = (current_user, token)
            r = requests.post(AIRFLOW_URL, headers={""Content-Type"": ""application/x-www-form-urlencoded""),
                             params={'grant_type':'password',
                                    'username': current_user,
                                    'password': <PASSWORD>,
                                  'scope': 'all'}, data=auth)
            
            # Raise exception if we have any errors
            if r.ok:
           "
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
","        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
","        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","        if type(stored_password)!= type(default_password):
            # The stored password type does not match the expected format
            print(f""The stored password type: {type(stored_password)} is different than the expected format: {type(default_password)})"")  # Debug: Log different stored passwords

        # Create an access token
        access_token = create_access_token(username, password)

        # Return the access token
        return jsonify({
          'result': 'Logged in',
          'message': f""Successfully logged in."",
            'access_token': access_token.decode(),
           "
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
","    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


","if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","    # Delete file from S3 bucket (bucket name is optional - will only delete files that are part of this pipeline)
    s3.delete_object(Bucket=BUCKET_NAME, Key=f'data_{filename[:-4]}'))

    # Delete all contents of the file after processing
    return jsonify({""message"": f""Deleted file '{filename}' from bucket {BUCKET_NAME}"",
                        ""done"": True})


if __name__ == ""__main__"":
    "
../airflow/dags/utils/price_processing.py,"import re


# Helper function to parse price strings into floats
def parse_price(price_str):
    # Remove non-numeric characters except for decimal points or commas
    clean_str = re.sub(r'[^0-9.,]', '', price_str)
    clean_str = clean_str.replace(',', '.').replace(""'"", '.')

    try:
        # If it contains a decimal, treat it as a float
        if '.' in clean_str:
            return float(clean_str)
        # Otherwise, treat the last two digits as the decimal part
        elif len(clean_str) > 2:
            return float(clean_str[:-2] + '.' + clean_str[-2:])
        else:
            return float(clean_str)
    except ValueError:
        return None


# EsoMarket Condition
def process_esomarket(price_str):
    price = parse_price(price_str)
    return price if price else None


def process_penny(price_str, price_type):
    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Clean up extracted prices and convert them to floats
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Common cents values like 90 or 99
    common_cents = [90, 99]

    # Handle cases based on the length of parsed prices
    if len(parsed_prices) == 3:
        # Handle cases like ""19 90 25.90 2""
        item_price = float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")
        initial_price = parsed_prices[2]
        return {""item_price"": item_price, ""initial_price"": initial_price}

    if len(parsed_prices) == 2:
        # If the second price is commonly a ""cents"" part like 90 or 99, merge with the first
        if parsed_prices[1] in common_cents:
            return {""item_price"": float(f""{int(parsed_prices[0])}.{int(parsed_prices[1])}"")}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}

    if len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Billa Condition
def process_billa(price_str, price_type):
    # Detect volume keywords: pri koupi, kupte, etc.
    volume_keywords = ['pri', 'koupi', 'kupte', 'ks', 'bodi', 'bodu', 'up te', 'aza']
    volume_detected = any(keyword in price_str.lower() for keyword in volume_keywords)

    # Extract numeric parts from the string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle specific distracted membership or volume words
    if 'bodi' in price_str.lower() or 'bodu' in price_str.lower():
        return {'item_member_price': '75bodi'}

    # Check if there are two prices and handle them
    if len(parsed_prices) == 2:
        # If the second value is an integer <5, treat it as volume, not initial_price
        if parsed_prices[1] < 5 and parsed_prices[1].is_integer():
            return {""item_price"": parsed_prices[0], ""volume"": str(int(parsed_prices[1]))}
        else:
            return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}

    return None


# Define Albert Hypermarket parsing method
def process_albert_hypermarket(price_str, price_type):
    # Clean string by keeping numbers and relevant separators
    clean_str = re.sub(r'[^0-9\s.,\'\-:]', '', price_str)  # Allow special chars like -, :, '

    # Handle specific cases for '-' or ':' as separators for integer prices
    combined_prices = []
    tokens = clean_str.split()

    for token in tokens:
        # Case 1: Numbers ending with ""-"" or "":""
        if token.endswith('-') or token.endswith(':'):
            token = token[:-1]  # Remove the trailing symbol
            combined_prices.append(parse_price(token))
        elif ""'"" in token:
            # Case 2: Handle cases like ""31'90""
            parts = token.split(""'"")
            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
                combined_price = f""{parts[0]}.{parts[1]}""
                combined_prices.append(parse_price(combined_price))
            else:
                combined_prices.append(parse_price(token))
        else:
            combined_prices.append(parse_price(token))

    # Filter out None values
    parsed_prices = [p for p in combined_prices if p is not None]

    # Condition: If the price is less than 5, treat it as invalid (exclude it)
    if parsed_prices and parsed_prices[0] < 5:
        return None

    # Assign prices based on the price_type
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Function to handle Tesco Supermarket OCR strings
def process_tesco_supermarket(price_str, price_type):
    # Handle dates (e.g., ""12.7. - 14.7."") by ignoring them
    date_pattern = r'\d{1,2}\.\d{1,2}\.\s*-\s*\d{1,2}\.\d{1,2}\.'  # Pattern for dates like ""12.7. - 14.7.""
    clean_str = re.sub(date_pattern, '', price_str)

    # Skip strings with percentages or irrelevant text
    if ""%"" in clean_str or ""HOP"" in clean_str:
        return None

    # Extract price values, specifically for club card or ""cena"" keyword
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Logic to differentiate between item prices and initial prices
    if price_type == ""item_member_price"":
        if parsed_prices:
            return {""item_member_price"": parsed_prices[0]}
    elif price_type == ""item_initial_price"":
        if parsed_prices:
            return {""item_initial_price"": parsed_prices[0]}
    else:
        if parsed_prices:
            return {""item_price"": parsed_prices[0]}

    return None


# Lidl Condition
def process_lidl(price_str):
    return parse_price(price_str)


# Kaufland Condition
def process_kaufland(price_str, price_type):
    if re.search(r'(\d+[.,]\d+)\s+(\d+[.,]\d+)', price_str):
        return None  # Skip sequences of more than 2 prices

    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[-1], ""initial_price"": parsed_prices[0]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Flop Top Condition
def process_flop_top(price_str, price_type):
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 2:
        return {""item_price"": parsed_prices[0], ""initial_price"": parsed_prices[1]}
    elif len(parsed_prices) == 1:
        return {""item_price"": parsed_prices[0]}
    return None


# Travel Free Condition
def process_travel_free(price_str, price_type):
    # Removing any € symbols to focus only on numeric data
","    clean_str = price_str.replace(""€"", """").strip()

    # Find all the price values in the string
    prices = re.findall(r'\d+[.,]?\d*', clean_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Ensure prices are sorted correctly (sale price is less than initial price)
    if len(parsed_prices) == 2:
        sale_price = min(parsed_prices)
        initial_price = max(parsed_prices)
        return {""item_price"": sale_price, ""initial_price"": initial_price}

    # If we only have one price, return it as the item price
    elif len(parsed_prices) == 1:
","        return {""item_price"": parsed_prices[0]}

    return None


# CBA Potraviny Condition
def process_cba_potraviny(price_str):
    return parse_price(price_str)


# Bene Condition
def process_bene(price_str):
    return parse_price(price_str)


# CBA Premium Condition
def process_cba_premium(price_str):
    return parse_price(price_str)


# Lidl Shop Condition
def process_lidl_shop(price_str):
    return parse_price(price_str)


# CBA Market Condition
def process_cba_market(price_str):
    return parse_price(price_str)


# Updated Makro Condition with improved packaging detection
def process_makro(price_str, price_type):
    # Extract packaging information (must be at the beginning of the string)
    packaging_pattern = re.match(r'^(\d+-?\d?\s*(BAL|ks|A VICE|AViCE))', price_str)

    # If packaging is found, extract it and continue processing the price
    packaging = None
    if packaging_pattern:
        packaging = packaging_pattern.group()  # Extract the packaging
        price_str = price_str[len(packaging):].strip()  # Remove packaging from the price string

    # Extract all numeric parts (prices) after the packaging
    prices = re.findall(r'\d+[.,]?\d*', price_str)

    # Convert extracted prices to float
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If there are two prices, assign them as item_price and initial_price
    if len(parsed_prices) >= 2:
        return {
            ""item_price"": parsed_prices[0],
            ""initial_price"": parsed_prices[1],
            ""packaging"": packaging
        }
    elif len(parsed_prices) == 1:
        # If there's only one price, treat it as the item price
        return {
            ""item_price"": parsed_prices[0],
            ""packaging"": packaging
        }
    else:
        return None


# Function to process Ratio price strings
def process_ratio(price_str):
    # Extract prices ignoring ""bezDPH"" or ""vcetneDPH"" text
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # If two prices are found, one should be item_price, the other initial_price
    if len(parsed_prices) == 2:
        return {""cena bez dph"": parsed_prices[0], ""item_price"": parsed_prices[1]}
    return None


# Function to process Globus price strings
def process_globus(price_str, price_type):
    # Skip percentage strings or invalid non-numeric inputs
    if ""%"" in price_str or re.search(r'[^\d.,\'\s-]', price_str):
        return None

    # Handle cases like ""14'90"" or ""44'90"" by replacing apostrophe with a decimal point
    price_str = price_str.replace(""'"", ""."")

    # Handle cases like ""17 90"" by joining them into a valid decimal format
    if re.search(r'\d+\s+\d{2}', price_str):
        price_str = price_str.replace("" "", ""."")

    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle item_price and item_member_price based on price_type
    if price_type == ""item_price"":
        # If one price is found, return it as the item price
        if len(parsed_prices) == 1:
            return {""item_price"": parsed_prices[0]}
    elif price_type == ""item_member_price"":
        # If member price is found, return it
        if len(parsed_prices) == 1:
            return {""item_member_price"": parsed_prices[0]}

    return None


# Function to process Tamda Foods price strings
def process_tamda_foods(price_str, price_type):
    # Skip percentage strings and invalid inputs
    if ""%"" in price_str or ""("" in price_str:
        return None

    # Handle cases like ""1290 KC"", ""3490Kc"", and ""5290KC"" (ignoring the ""KC"" part)
    price_str = re.sub(r'[KCkc]+', '', price_str).strip()

    # Extract numeric parts
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    if len(parsed_prices) == 1:
        if price_type == ""item_member_price"":
            return {""item_member_price"": parsed_prices[0]}
        elif price_type == ""item_price"":
            return {""item_price"": parsed_prices[0]}

    return None


# Function to process all types of prices based on class_id
def process_price_by_class_id(shop_name, got_ocr_text, class_id):
    processed_price = None

    # Check class_id for the type of price
    if class_id == ""item_price"":
        price_type = ""item_price""
    elif class_id == ""item_member_price"":
        price_type = ""item_member_price""
    elif class_id == ""item_initial_price"":
        price_type = ""item_initial_price""
    else:
        return None

    # Dispatch based on shop_name and price_type
    if shop_name == ""EsoMarket"":
        processed_price = process_esomarket(got_ocr_text)
    elif shop_name == ""Penny"":
        processed_price = process_penny(got_ocr_text, price_type)
    elif shop_name == ""Billa"":
        processed_price = process_billa(got_ocr_text, price_type)
    elif shop_name in [""Albert Hypermarket"", ""Albert Supermarket""]:
        processed_price = process_albert_hypermarket(got_ocr_text, price_type)
    elif shop_name in [""Tesco Supermarket"", ""Tesco Hypermarket""]:
        processed_price = process_tesco_supermarket(got_ocr_text, price_type)
    elif shop_name == ""Lidl"":
        processed_price = process_lidl(got_ocr_text)
    elif shop_name == ""Kaufland"":
        processed_price = process_kaufland(got_ocr_text, price_type)
    elif shop_name in [""Flop Top"", ""Flop""]:
        processed_price = process_flop_top(got_ocr_text, price_type)
    elif shop_name == ""Travel Free"":
        processed_price = process_travel_free(got_ocr_text, price_type)
    elif shop_name == ""CBA Potraviny"":
        processed_price = process_cba_potraviny(got_ocr_text)
    elif shop_name == ""Bene"":
        processed_price = process_bene(got_ocr_text)
    elif shop_name == ""CBA Premium"":
        processed_price = process_cba_premium(got_ocr_text)
    elif shop_name == ""Lidl Shop"":
        processed_price = process_lidl_shop(got_ocr_text)
    elif shop_name == ""CBA Market"":
        processed_price = process_cba_market(got_ocr_text)
    elif shop_name == ""Makro"":
        processed_price = process_makro(got_ocr_text, price_type)
    elif shop_name == ""Globus"":
        processed_price = process_globus(got_ocr_text, price_type)
    elif shop_name == ""Tamda Foods"":
        processed_price = process_tamda_foods(got_ocr_text, price_type)
    elif shop_name == ""Ratio"":
        processed_price = process_ratio(got_ocr_text)

    return processed_price
","    if '%' in price_str or '(' in price_str:
        return None

    # Handle cases like ""19 90 25.90 2"" by removing the ""£"" sign with decimals
    price_str = price_str.replace('£', '')

    # Extract all numeric parts from the price string
    prices = re.findall(r'\d+[.,]?\d*', price_str)
    parsed_prices = [parse_price(p) for p in prices if parse_price(p) is not None]

    # Handle item_price and item"
../airflow/dags/utils/yolo_ocr_utils.py,"import json
import os
import tempfile

import cv2
import requests  # For sending HTTP requests to the deployed YOLO model
import logging
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, save_item_to_dynamodb

TEMP_DIR = ""/tmp""

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


def got_text_from_image(image_path):
    url = 'http://34.246.217.135:80/extract_text'  # Adjust the URL if necessary
    try:
        with open(image_path, 'rb') as image_file:
            files = {'image': image_file}
","            response = requests.post(url, files=files)

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image: {response.status_code} - {response.text}"")
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


","def got_text_from_image_box(image_path, box):
    url = 'http://34.246.217.135:80/extract_text_with_box'  # OCR API route
    try:
        logger.info(f""Sending bounding box to OCR API: {box}"")  # Log the bounding box

        with open(image_path, 'rb') as image_file:
            # Prepare the multipart/form-data request
            files = {'image': image_file}
            # Send JSON data separately from form-data (image)
            json_data = {'box': box}

            response = requests.post(url, files=files, data={'json': json.dumps(json_data)})

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image_box: {response.status_code} - {response.text}"")
    except Exception as e:
        logger.error(f""Exception in got_text_from_image_box: {e}"")
        raise Exception(f""Exception in extract_text_from_image_box: {e}"")


def run_yolo_on_pages(s3_input_images_filepaths, dynamodb_table_name, model='model1',
                      save_images=False, detection_output_path=None, include_ocr=False, padding=0):
    """"""
    This function runs the YOLO model on a list of images from S3 or local paths, saves the detection details
    to DynamoDB, and returns the detection results as a dictionary. Optionally, it can also save the detected ROI images.

    Args:
        s3_input_images_filepaths (list): List of S3 paths of input image files.
        detection_output_path (str): S3 directory where the detection .txt files should be saved.
        dynamodb_table_name (str): The name of the DynamoDB table to store detection details.
        save_images (bool): Whether to save the detected ROIs as images.
        model (str): The model to be used ('model1' or 'model2').

    Returns:
        dict: A dictionary of predictions with image paths as keys and detections as values.
        list: A list of saved ROI image paths if save_images is True.
    """"""
    logger.info(f""Starting YOLO processing on {len(s3_input_images_filepaths)} images using model: {model}"")

    predictions = {}  # Dictionary to store detections for each image
    s3_saved_images = []  # List to store S3 paths of saved ROI images if save_images is True

    for filepath in s3_input_images_filepaths:
        try:
            logger.info(f""Processing image: {filepath}"")

            # Download the image from S3 to local TMP_DIR
            local_image_path = os.path.join(TEMP_DIR, os.path.basename(filepath))  # Corrected local path
            download_file_from_s3(filepath, local_image_path)
            logger.info(f""Downloaded image from S3 to {local_image_path}"")

            # Run the prediction and detection using the deployed YOLO model
            with open(local_image_path, 'rb') as image_file:
                response = requests.post(
                    f""http://34.246.217.135:80/predict"",  # YOLO model endpoint
                    files={'image': image_file},
                    params={'model': model}
                )

            if response.status_code == 200:
                detections = response.json().get('detections', [])
                logger.info(f""Received {len(detections)} detections for {filepath}"")
            else:
                raise Exception(f""Error from YOLO model: {response.status_code} - {response.text}"")

            img = cv2.imread(local_image_path)  # Load the image for ROI extraction (if needed)

            # Store detections for this image
            predictions[filepath] = []  # Initialize a list to store all detections for this image

            # Initialize a dictionary to store detections grouped by class
            detections_by_class = {}

            height, width = img.shape[:2]  # Get the image dimensions (height, width)

            for i, det in enumerate(detections):
                x1, y1, x2, y2 = det['box']  # Get bounding box coordinates
                class_name = det['class']  # Class name (e.g., 'shop_item')
                confidence = det['confidence']  # Confidence score for detection

                # Calculate width and height of the bounding box
                box_width = x2 - x1
                box_height = y2 - y1

                # Calculate 10% padding for width and height
                padding_w = int(box_width * 0.10)
                padding_h = int(box_height * 0.10)

                # Increase the bounding box by 10% padding on all sides, ensuring it stays within the image boundaries
                x1 = max(0, x1 - padding_w)
                y1 = max(0, y1 - padding_h)
                x2 = min(width, x2 + padding_w)
                y2 = min(height, y2 + padding_h)

                # Build the bounding box information, and add class_name to the detection item
                detection_item = {
                    'class_name': class_name,  # Add the class name here
                    'bounding_box': {
                        'x1': str(x1), 'y1': str(y1), 'x2': str(x2), 'y2': str(y2)
                    },
                    'confidence': str(confidence)
                }

                # Perform OCR if include_ocr is True
                if include_ocr:
                    # Prepare the bounding box for OCR
                    ocr_box = [x1, y1, x2, y2]  # Box format: [x1, y1, x2, y2]

                    # Step 1: Perform OCR directly on the bounding box area of the original image
                    object_text = got_text_from_image_box(local_image_path, ocr_box)

                    # Add OCR text to the detection item
                    detection_item['ocr_text'] = object_text
                    logger.info(f""OCR extracted text for class {class_name} in bounding box: {object_text}"")

                # Append detection item under the corresponding class_name
                if class_name not in detections_by_class:
                    detections_by_class[class_name] = []
                detections_by_class[class_name].append(detection_item)

                # Append the detection to the image's list of detections in the predictions dictionary
                predictions[filepath].append(detection_item)  # Now appending within the loop

            # After processing all detections, prepare the item for DynamoDB
            item_to_save = {
                'image_id': filepath,
                'detections': detections_by_class  # Grouped detections by class
            }

            # Save the detections to DynamoDB
            save_item_to_dynamodb(dynamodb_table_name, item_to_save)
            logger.info(f""Saved all detections for image {filepath} to DynamoDB"")

            # If save_images is True, extract ROI and save as PNG
            if save_images:
                for i, det in enumerate(detections):
                    x1, y1, x2, y2 = det['box']
                    class_name = det['class']
                    roi = img[y1:y2, x1:x2]  # Extract ROI from image
                    roi_filename = f""{os.path.basename(filepath).replace('.png', '')}_det_{i}_{class_name}.png""
                    roi_local_path = os.path.join(TEMP_DIR, roi_filename)

                    # Save the ROI image locally as PNG
                    cv2.imwrite(roi_local_path, roi)
                    logger.info(f""Saved ROI to {roi_local_path}"")

                    # Define the S3 path where the ROI will be uploaded
                    s3_roi_path = f""{detection_output_path}/images/{roi_filename}""

                    # Upload the ROI image to S3
                    upload_file_to_s3(roi_local_path, s3_roi_path)
                    s3_saved_images.append(s3_roi_path)
                    logger.info(f""Uploaded ROI to S3: {s3_roi_path}"")

                    # Clean up the temporary local ROI file after uploading
                    os.remove(roi_local_path)
                    logger.info(f""Deleted local ROI file: {roi_local_path}"")

        except Exception as e:
            logger.error(f""Error processing image {filepath}: {e}"")

    # Return the predictions dictionary and saved image paths if save_images is True
    return predictions, s3_saved_images if save_images else predictions","        response = requests.post(url, files=files)

        if response.status_code == 200:
            content = response.content
        elif response.status_code == 404:
            return None  # No such page exists
        else:
            content = response.text

        return content
    except Exception as e:
        logger.error(f""Exception in got_text_from_image: {e}"")
        return None

"
../airflow/dags/utils/pdf_utils.py,"import os
from pdf2image import convert_from_path
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, get_pdf_item_from_dynamodb
import logging

TEMP_DIR = '/tmp'  # Modify if needed for your environment
PDF_S3_PATH = 'pdfs'  # Define the S3 directory where your PDF files are stored
PAGES_S3_PATH = 'pages/valid'  # Directory in S3 where pages are uploaded
poppler_path = ""/usr/bin""

def split_pdf_to_pages(filename, shop_name):
    """"""Split PDF into pages and upload to S3, returning full S3 paths for the pages.""""""
    if not filename or not shop_name:
        raise Exception(""Filename or Shop Name missing!"")

    # Fetch metadata from DynamoDB
","    response = get_pdf_item_from_dynamodb(filename, shop_name)
    file_entry = response.get('Item')

    if not file_entry:
        raise Exception(f""File {filename} not found in DynamoDB"")

    # Check if the pages already exist in S3
    page_s3_paths = []  # Store full S3 paths for pages
","    base_filename = os.path.splitext(filename)[0]

    logging.info(f""Checking if pages for {filename} already exist in S3..."")

    # Define the path for the PDF in S3 (in the 'pdfs' directory)
    s3_pdf_path = f'{PDF_S3_PATH}/{filename}'

    # Download the PDF from S3 to a temporary location
    file_path = os.path.join(TEMP_DIR, filename)

    # Log paths for debugging
    logging.info(f""Checking if file exists in S3 path: {s3_pdf_path}"")

    logging.info(f""Downloading file from S3 path: {s3_pdf_path} to local path: {file_path}"")

    try:
        download_file_from_s3(s3_pdf_path, file_path)
    except Exception as e:
        logging.error(f""Failed to download file from S3: {e}"")
        raise e

    # Convert PDF into image pages
    images = convert_from_path(file_path, dpi=250, poppler_path=poppler_path)

    for i, image in enumerate(images):
        page_filename = f""{base_filename}_page_{i + 1}.png""
        page_path = os.path.join(TEMP_DIR, page_filename)

        # Save the image locally
        image.save(page_path, 'PNG')

        # Upload each page to S3 in the 'pages/valid/' directory
        s3_page_path = f'{PAGES_S3_PATH}/{page_filename}'
        upload_file_to_s3(page_path, s3_page_path)

        # Add full S3 path of the page to the list
        page_s3_paths.append(s3_page_path)

    # Return the list of full S3 paths for the uploaded pages
    return page_s3_paths
","    dynamodb_table = boto3.resource('dynamodb', region_name='us-east-1')
    table = dynamodb_table['shop']

    response = table.query(TableName=shop_name)['Items']

    items = []
    for item in response:
        items.append({""id"": item['Id'], ""title"": item[""Title""], ""link"": item[""Link""]})

    page_list = [items[0]]
    page_s3_paths = []

"
../airflow/dags/utils/s3_dynamodb_utils.py,"import boto3

# Initialize AWS S3 and DynamoDB clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Retrieve an item from the DynamoDB table based on filename and shop_name
def get_pdf_item_from_dynamodb(filename, shop_name, table=None, table_name=""pdf_metadata""):
    """"""
    Retrieve an item from a specified DynamoDB table based on filename and shop_name.
    """"""
    # Use the provided table instance if given; otherwise, access by table_name
    table = table or dynamodb.Table(table_name)
    return table.get_item(Key={'filename': filename, 'shop_name': shop_name})

# Function to download a file from an S3 bucket to a local path
def download_file_from_s3(filename_path, local_path, bucket_name=""salestelegrambot""):
    """"""
    Download a file from an S3 bucket.
    """"""
    try:
        s3.download_file(bucket_name, filename_path, local_path)
        print(f""Downloaded {filename_path} to {local_path}"")
    except Exception as e:
        print(f""Error downloading {filename_path}: {e}"")

# Function to upload a file from a local path to an S3 bucket
def upload_file_to_s3(local_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Upload a file to an S3 bucket.
    """"""
    try:
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/{s3_path}"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")

# Function to save an item to a DynamoDB table
def save_item_to_dynamodb(table_name, item, table=None):
    """"""
    Save an item to a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.put_item(Item=item)
        print(f""Saved item {item} to DynamoDB table {table_name}"")
    except Exception as e:
        print(f""Error saving item to DynamoDB: {e}"")

# Function to update specific fields of an item in DynamoDB
def update_item_in_dynamodb(table_name, key, update_expression, expression_attribute_values, table=None):
    """"""
    Update specific fields of an item in a DynamoDB table.
    """"""
","    try:
        table = table or dynamodb.Table(table_name)
        table.update_item(
            Key=key,
            UpdateExpression=update_expression,
            ExpressionAttributeValues=expression_attribute_values
        )
","        print(f""Updated item in table {table_name} with key {key}"")
    except Exception as e:
        print(f""Error updating item in DynamoDB: {e}"")
","    try:
        table = table or dynamodb.Table(table_name)
        table[key] = update_expression
        for attribute_value in expression_attribute_values:
            table[""Attributes""][attribute_value['AttributeName']] = attribute_value['Value']
        print(f""Updated items in table {table_name} with key {key}"")
    except Exception as e:
        print(f""Error updating items in DynamoDB: {e}"")

# Function to delete specific fields of an item in DynamoDB
def delete_item_in_dynamodb(table_name, key, table="
../airflow/dags/validity_check.py,"import itertools

from airflow.sensors.external_task import ExternalTaskSensor
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from utils.s3_dynamodb_utils import update_item_in_dynamodb, download_file_from_s3
import boto3
import requests
from datetime import datetime

# DynamoDB Table Details
TABLE_NAME = 'pdf_metadata'
DETECTED_TABLE = 'detected_data'
USERS_TABLE = 'user_preferences'
dynamodb = boto3.resource('dynamodb')

pdf_table = dynamodb.Table(TABLE_NAME)
users_table = dynamodb.Table(USERS_TABLE)
detected_table = dynamodb.Table(DETECTED_TABLE)

WEBHOOK_URL = ""https://klffswbb85.execute-api.eu-west-1.amazonaws.com/default/salesTelegramBotHandler""


# Function to check file validity and update detected items only if the status changes
def check_validity_and_update_detected():
    # Initialize DynamoDB clients

    # Get today's date
    today = datetime.utcnow().date()

    changed_to_valid = []
    changed_to_invalid = []

    # Scan the pdf_metadata table to get all items
    pdf_response = pdf_table.scan()
    pdf_items = pdf_response.get('Items', [])

    # Process each PDF item
    for pdf_item in pdf_items:
        # Remove .pdf from filename for comparison with detected items
        pdf_name_without_ext = pdf_item['filename'].replace('.pdf', '')

        valid_from = datetime.strptime(pdf_item['valid_from'], '%Y-%m-%d').date()
        valid_to = datetime.strptime(pdf_item['valid_to'], '%Y-%m-%d').date()

        # Determine the current validity status
        current_valid_status = pdf_item.get('valid', None)

        # Check if the file should be valid or invalid based on today's date
        is_valid_now = valid_from <= today <= valid_to

        # If the current status differs from the computed status, it means the status has changed
        if current_valid_status != is_valid_now:
            # Update the valid field in the pdf_metadata table using the utility method
            update_item_in_dynamodb(
                table_name=TABLE_NAME,
                key={'filename': pdf_item['filename'], 'shop_name': pdf_item[""shop_name""]},
                update_expression=""SET valid = :v"",
                expression_attribute_values={':v': is_valid_now}
            )

            # Append the filename to the appropriate list based on the new validity status
            if is_valid_now:
                changed_to_valid.append(pdf_name_without_ext)
            else:
                changed_to_invalid.append(pdf_name_without_ext)

    # Return only files that changed their validity status
    return {
        'valid': changed_to_valid,
        'invalid': changed_to_invalid
    }


def update_detected_items_task(**context):
    """"""
    Task to update detected items based on the output from the previous task.
    """"""
    # Get the valid and invalid files from the context (returned by the previous task)
    task_instance = context['task_instance']

    # Pull the whole dictionary returned by the previous task
    status_change = task_instance.xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' and 'invalid' lists from the returned dictionary
    valid_files = status_change.get('valid', [])
    invalid_files = status_change.get('invalid', [])

    print(f""Debug: valid_files from XCom: {valid_files}"")
    print(f""Debug: invalid_files from XCom: {invalid_files}"")

    if not valid_files and not invalid_files:
        print(""Debug: No files were found for update."")
    else:
        # Update the detected_items table based on valid and invalid files
        update_detected_items_based_on_status(valid_files, invalid_files)


def update_detected_items_based_on_status(valid_files, invalid_files):
    """"""
    Update the detected_items table based on the lists of files that changed status (valid or invalid).
    This function optimizes the update by filtering detected items only with relevant file substrings.
    """"""

    # Remove .pdf extensions from filenames
    valid_files = [file.replace('.pdf', '') for file in valid_files]
    invalid_files = [file.replace('.pdf', '') for file in invalid_files]

    # Combine valid and invalid files
    all_files = valid_files + invalid_files
    print(f""Debug: all_files to be checked (without .pdf): {all_files}"")

    valid_changed_items = []

    detected_response = detected_table.scan()
","    detected_items = detected_response.get('Items', [])

    print(f""Debug: Detected items retrieved from DynamoDB: {detected_items}"")

    # Process each detected item
    for detected_item in detected_items:
        detected_image_path = detected_item['image_id']

        # Debug: Show the current detected image path
        print(f""Debug: Processing detected item with image_id: {detected_image_path}"")

        # Check if any file name (without .pdf) is a substring of the image_id
        for file_substr in all_files:
            if file_substr in detected_image_path:
","                new_valid_status = file_substr in valid_files

                # Debug: Show the new validity status for the detected item
                print(f""Debug: Changing validity of {detected_image_path} to {new_valid_status}"")

                if detected_item.get('valid') != new_valid_status:
                    # Update the valid field in DynamoDB
                    update_item_in_dynamodb(
                        table_name=DETECTED_TABLE,
                        key={'image_id': detected_item['image_id']},
                        update_expression=""SET valid = :v"",
                        expression_attribute_values={':v': new_valid_status}
                    )
                    print(f""Debug: Updated detected item {detected_item['image_id']} validity to {new_valid_status}"")

                    if new_valid_status:
                        valid_changed_items.append(detected_item)

    print(f""Debug: List of valid_changed_items: {valid_changed_items}"")

    return valid_changed_items


def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    pdf_table = dynamodb.Table(TABLE_NAME)
    response = pdf_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


def regroup_by_shop():
    """"""
    Iterates over all users and creates two columns:
    1. Users for each shop with included shops (all_shops - excluded_shops).
    2. Users for each shop with included shops and receive_pdf_enabled set to True.
    """"""
    # Dictionary to hold the regrouped data
    shop_user_map = {
        ""included_shops"": {},
        ""included_shops_and_receive_pdf"": {}
    }

    # Retrieve all unique shops from the metadata
    all_shops = get_all_shops()

    # Scan the users table to get all user records
    response = users_table.scan()

    # Iterate over all users (items in DynamoDB)
    for item in response['Items']:
        chat_id = item['chat_id']
        receive_pdf_enabled = item.get('receive_pdf_enabled', False)  # Default to False if not set
        excluded_shops = item.get('excluded_shops', [])

        # Calculate included shops (all_shops - excluded_shops)
        included_shops = [shop for shop in all_shops if shop not in excluded_shops]

        # Iterate over all included shops and add users to corresponding shop keys
        for shop in included_shops:
            # Add to ""included_shops"" column (all users with included shops)
            if shop not in shop_user_map[""included_shops""]:
                shop_user_map[""included_shops""][shop] = []
            shop_user_map[""included_shops""][shop].append(chat_id)

            # Add to ""included_shops_and_receive_pdf"" column (users with included shops AND receive_pdf_enabled=True)
            if receive_pdf_enabled:
                if shop not in shop_user_map[""included_shops_and_receive_pdf""]:
                    shop_user_map[""included_shops_and_receive_pdf""][shop] = []
                shop_user_map[""included_shops_and_receive_pdf""][shop].append(chat_id)

    # Return the regrouped data structure
    return shop_user_map


def regroup_shop_to_valid_file(valid_files):
    shop_file_map = {}

    # Ensure valid_files have no .pdf extension
    valid_files = [file.replace('.pdf', '') for file in valid_files]

    # Scan the pdf metadata table
    response = pdf_table.scan()

    # Process each item in the table
    for item in response['Items']:
        shop_name = item['shop_name']
        file_name = item['filename'].replace('.pdf', '')  # Remove .pdf for comparison

        # Check if the file name matches any valid file (without .pdf)
        if file_name in valid_files:
            if shop_name not in shop_file_map:
                shop_file_map[shop_name] = []
            shop_file_map[shop_name].append(item['filename'])  # Use original filename with .pdf for sending

    return shop_file_map


def send_webhook(process_type, shop_name, users_id_list, pdf_file, tracked_item=None):
    """"""
    Sends a POST request to the specified webhook URL with the provided data.
    """"""
    payload = {
        'process_type': process_type,
        'shop_name': shop_name,
        'users_id_list': users_id_list,
        'pdf_file': pdf_file,
    }
    if tracked_item:
        payload['tracked_items_list'] = tracked_item  # Include the tracked item if provided

    # Send the POST request to the webhook
    response = requests.post(WEBHOOK_URL, json=payload)

    # Log the response (optional)
    if response.status_code == 200:
        if tracked_item:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}, tracked_item: {tracked_item}"")
        else:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}"")
    else:
        print(f""Failed to send data to webhook: {response.status_code}, {response.text}"")


def send_updates_in_telegram_task(**context):
    # Pull the whole dictionary returned by the previous task
    status_change = context['task_instance'].xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' files from the returned dictionary
    valid_files = status_change.get('valid', [])

    # Debug print to check what valid files were pulled
    print(f""Debug: valid_files from XCom: {valid_files}"")

    # If no valid files, skip further processing
    if not valid_files:
        print(""Debug: No valid files to process."")
        return

    # Regroup data by shop
    regrouped_data = regroup_by_shop()
    print(f""Debug: regrouped_data: {regrouped_data}"")

    # Regroup valid files by shop
    regrouped_valid_files_data = regroup_shop_to_valid_file(valid_files)
    print(f""Debug: regrouped_valid_files_data: {regrouped_valid_files_data}"")

    for shop_name in regrouped_data['included_shops']:
        shop_included_users_with_receive_pdf = regrouped_data['included_shops_and_receive_pdf'].get(shop_name, [])
        shop_included_users = regrouped_data['included_shops'].get(shop_name, [])

        if shop_name in regrouped_valid_files_data:
            for pdf_file in regrouped_valid_files_data[shop_name]:
                # Debug print before sending the pdf_newsletter webhook
                print(f""Debug: Sending pdf_newsletter for shop: {shop_name}, pdf_file: {pdf_file}, ""
                      f""users_with_receive_pdf: {shop_included_users_with_receive_pdf}"")

                send_webhook('pdf_newsletter', shop_name, shop_included_users_with_receive_pdf, pdf_file)

                batch_size = 3
                user_batches = [list(group) for group in
                                itertools.zip_longest(*[iter(shop_included_users)] * batch_size)]

                for user_batch in user_batches:
                    valid_users = [user for user in user_batch if user is not None]
                    user_ids_list = []
                    user_tracked_items_list = []

                    for user_id in valid_users:
                        response = users_table.get_item(Key={'chat_id': user_id})
                        if 'Item' in response:
                            user_tracked_items = response['Item'].get('tracked_items', [])
                            if user_tracked_items:
                                user_ids_list.append(user_id)
                                user_tracked_items_list.append(user_tracked_items)

                    if user_ids_list and user_tracked_items_list:
                        # Debug print before sending the tracked_items_list webhook
                        print(f""Debug: Sending tracked_items_list for shop: {shop_name}, pdf_file: {pdf_file}, ""
                              f""user_ids_list: {user_ids_list}, tracked_items: {user_tracked_items_list}"")

                        send_webhook('tracked_items_list', shop_name, user_ids_list, pdf_file, user_tracked_items_list)


# Airflow DAG setup
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
}

with DAG(
        dag_id='check_file_validity_and_update_detected_items',
        default_args=default_args,
        schedule_interval='0 1 * * *',  # Runs at 08:00 AM UTC daily
        catchup=False
) as dag:
    # Task to check and update the PDF metadata validity
    check_files_and_update_detected = PythonOperator(
        task_id='check_validity_and_update_detected_task',
        python_callable=check_validity_and_update_detected,
    )

    # Task to update the detected items based on the result of the first task
    update_detected_items = PythonOperator(
        task_id='update_detected_items_task',
        python_callable=update_detected_items_task,
        provide_context=True
    )

    # Task to send updates in Telegram after updating detected items
    send_updates_in_telegram = PythonOperator(
        task_id='send_updates_in_telegram_task',
        python_callable=send_updates_in_telegram_task,
        provide_context=True
    )

    # Task dependencies
    check_files_and_update_detected >> update_detected_items >> send_updates_in_telegram
","    detected_images = []

    # Loop through every item in the table to find all files that are currently being processed
    for item in detected_response['Items':
        # Find all images for this image id
        if item.get('img_id'):
            img_url = item['img_id']
            if '.jpg' in img_url:
                detected_images.extend([os.path.basename(img_url)])

        # Determine whether the image exists already
        elif '.png' in item['filename':
            detected_images.extend([item['filename']])

    # Create the list of files"
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
","        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

","        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","        stored_password = str(password)
        database_credentials = USERS_TABLE_.query(f""username=='{username}'"").first()  # Lookup the User table
        if not stored_password or not database_credentials:
            raise ValueError
        stored_password = stored_password.decode('utf-8')
        if not isinstance(stored_password, str):
            stored_password = stored_password.encode('utf-8')

        if not isinstance(database_credentials, dict):
            database_credentials = database_credentials.encode('utf-8')

        if (not username and stored_password"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/six.py,"# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""""""Utilities for writing code that runs on Python 2 and 3""""""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.16.0""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith(""java""):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):

            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """"""Add documentation to a function.""""""
    func.__doc__ = doc


def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):

    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = [""__doc__"", ""__name__""]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """"""
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """"""

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + ""."" + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + ""."" + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError(""This loader does not know module "" + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """"""
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """"""
        return hasattr(self.__get_module(fullname), ""__path__"")

    def get_code(self, fullname):
        """"""Return None

        Required, if is_package is implemented""""""
        self.__get_module(fullname)  # eventually raises ImportError
        return None
    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass

_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """"""Lazy loading of moved objects""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""cStringIO"", ""cStringIO"", ""io"", ""StringIO""),
    MovedAttribute(""filter"", ""itertools"", ""builtins"", ""ifilter"", ""filter""),
    MovedAttribute(""filterfalse"", ""itertools"", ""itertools"", ""ifilterfalse"", ""filterfalse""),
    MovedAttribute(""input"", ""__builtin__"", ""builtins"", ""raw_input"", ""input""),
    MovedAttribute(""intern"", ""__builtin__"", ""sys""),
    MovedAttribute(""map"", ""itertools"", ""builtins"", ""imap"", ""map""),
    MovedAttribute(""getcwd"", ""os"", ""os"", ""getcwdu"", ""getcwd""),
    MovedAttribute(""getcwdb"", ""os"", ""os"", ""getcwd"", ""getcwdb""),
    MovedAttribute(""getoutput"", ""commands"", ""subprocess""),
    MovedAttribute(""range"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""reload_module"", ""__builtin__"", ""importlib"" if PY34 else ""imp"", ""reload""),
    MovedAttribute(""reduce"", ""__builtin__"", ""functools""),
    MovedAttribute(""shlex_quote"", ""pipes"", ""shlex"", ""quote""),
    MovedAttribute(""StringIO"", ""StringIO"", ""io""),
    MovedAttribute(""UserDict"", ""UserDict"", ""collections""),
    MovedAttribute(""UserList"", ""UserList"", ""collections""),
    MovedAttribute(""UserString"", ""UserString"", ""collections""),
    MovedAttribute(""xrange"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""zip"", ""itertools"", ""builtins"", ""izip"", ""zip""),
    MovedAttribute(""zip_longest"", ""itertools"", ""itertools"", ""izip_longest"", ""zip_longest""),
    MovedModule(""builtins"", ""__builtin__""),
    MovedModule(""configparser"", ""ConfigParser""),
    MovedModule(""collections_abc"", ""collections"", ""collections.abc"" if sys.version_info >= (3, 3) else ""collections""),
    MovedModule(""copyreg"", ""copy_reg""),
    MovedModule(""dbm_gnu"", ""gdbm"", ""dbm.gnu""),
    MovedModule(""dbm_ndbm"", ""dbm"", ""dbm.ndbm""),
    MovedModule(""_dummy_thread"", ""dummy_thread"", ""_dummy_thread"" if sys.version_info < (3, 9) else ""_thread""),
    MovedModule(""http_cookiejar"", ""cookielib"", ""http.cookiejar""),
    MovedModule(""http_cookies"", ""Cookie"", ""http.cookies""),
    MovedModule(""html_entities"", ""htmlentitydefs"", ""html.entities""),
    MovedModule(""html_parser"", ""HTMLParser"", ""html.parser""),
    MovedModule(""http_client"", ""httplib"", ""http.client""),
    MovedModule(""email_mime_base"", ""email.MIMEBase"", ""email.mime.base""),
    MovedModule(""email_mime_image"", ""email.MIMEImage"", ""email.mime.image""),
    MovedModule(""email_mime_multipart"", ""email.MIMEMultipart"", ""email.mime.multipart""),
    MovedModule(""email_mime_nonmultipart"", ""email.MIMENonMultipart"", ""email.mime.nonmultipart""),
    MovedModule(""email_mime_text"", ""email.MIMEText"", ""email.mime.text""),
    MovedModule(""BaseHTTPServer"", ""BaseHTTPServer"", ""http.server""),
    MovedModule(""CGIHTTPServer"", ""CGIHTTPServer"", ""http.server""),
    MovedModule(""SimpleHTTPServer"", ""SimpleHTTPServer"", ""http.server""),
    MovedModule(""cPickle"", ""cPickle"", ""pickle""),
    MovedModule(""queue"", ""Queue""),
    MovedModule(""reprlib"", ""repr""),
    MovedModule(""socketserver"", ""SocketServer""),
    MovedModule(""_thread"", ""thread"", ""_thread""),
    MovedModule(""tkinter"", ""Tkinter""),
    MovedModule(""tkinter_dialog"", ""Dialog"", ""tkinter.dialog""),
    MovedModule(""tkinter_filedialog"", ""FileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_scrolledtext"", ""ScrolledText"", ""tkinter.scrolledtext""),
    MovedModule(""tkinter_simpledialog"", ""SimpleDialog"", ""tkinter.simpledialog""),
    MovedModule(""tkinter_tix"", ""Tix"", ""tkinter.tix""),
    MovedModule(""tkinter_ttk"", ""ttk"", ""tkinter.ttk""),
    MovedModule(""tkinter_constants"", ""Tkconstants"", ""tkinter.constants""),
    MovedModule(""tkinter_dnd"", ""Tkdnd"", ""tkinter.dnd""),
    MovedModule(""tkinter_colorchooser"", ""tkColorChooser"",
                ""tkinter.colorchooser""),
    MovedModule(""tkinter_commondialog"", ""tkCommonDialog"",
                ""tkinter.commondialog""),
    MovedModule(""tkinter_tkfiledialog"", ""tkFileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_font"", ""tkFont"", ""tkinter.font""),
    MovedModule(""tkinter_messagebox"", ""tkMessageBox"", ""tkinter.messagebox""),
    MovedModule(""tkinter_tksimpledialog"", ""tkSimpleDialog"",
                ""tkinter.simpledialog""),
    MovedModule(""urllib_parse"", __name__ + "".moves.urllib_parse"", ""urllib.parse""),
    MovedModule(""urllib_error"", __name__ + "".moves.urllib_error"", ""urllib.error""),
    MovedModule(""urllib"", __name__ + "".moves.urllib"", __name__ + "".moves.urllib""),
    MovedModule(""urllib_robotparser"", ""robotparser"", ""urllib.robotparser""),
    MovedModule(""xmlrpc_client"", ""xmlrpclib"", ""xmlrpc.client""),
    MovedModule(""xmlrpc_server"", ""SimpleXMLRPCServer"", ""xmlrpc.server""),
]
# Add windows specific modules.
if sys.platform == ""win32"":
    _moved_attributes += [
        MovedModule(""winreg"", ""_winreg""),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
","    if isinstance(attr, MovedModule):
        _importer._add_module(attr, ""moves."" + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + "".moves"")
","_importer._add_module(moves, ""moves"")


class Module_six_moves_urllib_parse(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""SplitResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qs"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qsl"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urldefrag"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urljoin"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""quote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""quote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_to_bytes"", ""urllib"", ""urllib.parse"", ""unquote"", ""unquote_to_bytes""),
    MovedAttribute(""urlencode"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitquery"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splittag"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splituser"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitvalue"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""uses_fragment"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_netloc"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_params"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_query"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_relative"", ""urlparse"", ""urllib.parse""),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(Module_six_moves_urllib_parse(__name__ + "".moves.urllib_parse""),
                      ""moves.urllib_parse"", ""moves.urllib.parse"")


class Module_six_moves_urllib_error(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_error""""""


_urllib_error_moved_attributes = [
    MovedAttribute(""URLError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""HTTPError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""ContentTooShortError"", ""urllib"", ""urllib.error""),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(Module_six_moves_urllib_error(__name__ + "".moves.urllib.error""),
                      ""moves.urllib_error"", ""moves.urllib.error"")


class Module_six_moves_urllib_request(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_request""""""


_urllib_request_moved_attributes = [
    MovedAttribute(""urlopen"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""install_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""build_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""pathname2url"", ""urllib"", ""urllib.request""),
    MovedAttribute(""url2pathname"", ""urllib"", ""urllib.request""),
    MovedAttribute(""getproxies"", ""urllib"", ""urllib.request""),
    MovedAttribute(""Request"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""OpenerDirector"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDefaultErrorHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPRedirectHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPCookieProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""BaseHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgr"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgrWithDefaultRealm"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPSHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FileHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""CacheFTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""UnknownHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPErrorProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""urlretrieve"", ""urllib"", ""urllib.request""),
    MovedAttribute(""urlcleanup"", ""urllib"", ""urllib.request""),
    MovedAttribute(""URLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""FancyURLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""proxy_bypass"", ""urllib"", ""urllib.request""),
    MovedAttribute(""parse_http_list"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""parse_keqv_list"", ""urllib2"", ""urllib.request""),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(Module_six_moves_urllib_request(__name__ + "".moves.urllib.request""),
                      ""moves.urllib_request"", ""moves.urllib.request"")


class Module_six_moves_urllib_response(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_response""""""


_urllib_response_moved_attributes = [
    MovedAttribute(""addbase"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addclosehook"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfo"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfourl"", ""urllib"", ""urllib.response""),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(Module_six_moves_urllib_response(__name__ + "".moves.urllib.response""),
                      ""moves.urllib_response"", ""moves.urllib.response"")


class Module_six_moves_urllib_robotparser(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_robotparser""""""


_urllib_robotparser_moved_attributes = [
    MovedAttribute(""RobotFileParser"", ""robotparser"", ""urllib.robotparser""),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes

_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + "".moves.urllib.robotparser""),
                      ""moves.urllib_robotparser"", ""moves.urllib.robotparser"")


class Module_six_moves_urllib(types.ModuleType):

    """"""Create a six.moves.urllib namespace that resembles the Python 3 namespace""""""
    __path__ = []  # mark as package
    parse = _importer._get_module(""moves.urllib_parse"")
    error = _importer._get_module(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
    response = _importer._get_module(""moves.urllib_response"")
    robotparser = _importer._get_module(""moves.urllib_robotparser"")

    def __dir__(self):
        return ['parse', 'error', 'request', 'response', 'robotparser']

_importer._add_module(Module_six_moves_urllib(__name__ + "".moves.urllib""),
                      ""moves.urllib"")


def add_move(move):
    """"""Add an item to six.moves.""""""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """"""Remove item from six.moves.""""""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError(""no such move, %r"" % (name,))


if PY3:
    _meth_func = ""__func__""
    _meth_self = ""__self__""

    _func_closure = ""__closure__""
    _func_code = ""__code__""
    _func_defaults = ""__defaults__""
    _func_globals = ""__globals__""
else:
    _meth_func = ""im_func""
    _meth_self = ""im_self""

    _func_closure = ""func_closure""
    _func_code = ""func_code""
    _func_defaults = ""func_defaults""
    _func_globals = ""func_globals""


try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any(""__call__"" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """"""Get the function out of a possibly unbound function"""""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:
    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller(""keys"")

    viewvalues = operator.methodcaller(""values"")

    viewitems = operator.methodcaller(""items"")
else:
    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller(""viewkeys"")

    viewvalues = operator.methodcaller(""viewvalues"")

    viewitems = operator.methodcaller(""viewitems"")

_add_doc(iterkeys, ""Return an iterator over the keys of a dictionary."")
_add_doc(itervalues, ""Return an iterator over the values of a dictionary."")
_add_doc(iteritems,
         ""Return an iterator over the (key, value) pairs of a dictionary."")
_add_doc(iterlists,
         ""Return an iterator over the (key, [values]) pairs of a dictionary."")


if PY3:
    def b(s):
        return s.encode(""latin-1"")

    def u(s):
        return s
    unichr = chr
    import struct
    int2byte = struct.Struct("">B"").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = ""assertCountEqual""
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = ""assertRaisesRegexp""
        _assertRegex = ""assertRegexpMatches""
        _assertNotRegex = ""assertNotRegexpMatches""
    else:
        _assertRaisesRegex = ""assertRaisesRegex""
        _assertRegex = ""assertRegex""
        _assertNotRegex = ""assertNotRegex""
else:
    def b(s):
        return s
    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r'\\', r'\\\\'), ""unicode_escape"")
    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])
    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = ""assertItemsEqual""
    _assertRaisesRegex = ""assertRaisesRegexp""
    _assertRegex = ""assertRegexpMatches""
    _assertNotRegex = ""assertNotRegexpMatches""
_add_doc(b, """"""Byte literal"""""")
_add_doc(u, """"""Text literal"""""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, ""exec"")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """"""Execute code in a namespace.""""""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(""""""exec _code_ in _globs_, _locs_"""""")

    exec_(""""""def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""""")


if sys.version_info[:2] > (3,):
    exec_(""""""def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""""")
else:
    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, ""print"", None)
if print_ is None:
    def print_(*args, **kwargs):
        """"""The new-style print function for Python 2.4 and 2.5.""""""
        fp = kwargs.pop(""file"", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError(""sep must be None or a string"")
        end = kwargs.pop(""end"", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError(""end must be None or a string"")
        if kwargs:
            raise TypeError(""invalid keyword arguments to print()"")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode(""\n"")
            space = unicode("" "")
        else:
            newline = ""\n""
            space = "" ""
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get(""file"", sys.stdout)
        flush = kwargs.pop(""flush"", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()

_add_doc(reraise, """"""Reraise an exception."""""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(wrapper, wrapped,
                        assigned=functools.WRAPPER_ASSIGNMENTS,
                        updated=functools.WRAPPER_UPDATES):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper
    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
              updated=functools.WRAPPER_UPDATES):
        return functools.partial(_update_wrapper, wrapped=wrapped,
                                 assigned=assigned, updated=updated)
    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):

        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d['__orig_bases__'] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})


def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get('__slots__')
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        if hasattr(cls, '__qualname__'):
            orig_vars['__qualname__'] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper


def ensure_binary(s, encoding='utf-8', errors='strict'):
    """"""Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """"""
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError(""not expecting type '%s'"" % type(s))


def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    return s


def ensure_text(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError(""not expecting type '%s'"" % type(s))


def python_2_unicode_compatible(klass):
    """"""
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """"""
    if PY2:
        if '__str__' not in klass.__dict__:
            raise ValueError(""@python_2_unicode_compatible cannot be applied ""
                             ""to %s because it doesn't define __str__()."" %
                             klass.__name__)
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get(""__spec__"") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another ""instance"" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (type(importer).__name__ == ""_SixMetaPathImporter"" and
                importer.name == __name__):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)
","(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
    response = _importer._get_module(""moves.urllib_response"")
    delattr(_MovedItems, attr.name)


class _MovedModules(_LazyModule):

    """"""Lazy loading of moved modules from an iterable""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""isatty"", ""bool"", ""True"", ""False""),
    MovedAttribute(""isclosed"", ""bool"", ""True"", ""False""),
   "
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
","        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
","        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
","        best_model_path = './model/best_model'
        chosen_model_path = None

        # Try to load the model
        try:
            print('Loading weights...')
            loaded_state_dict = torch.load(os.path.join(model_dir,'model.bin'), map_location=""cpu""))

            # Check that we have a trained checkpoint
            assert os.path.exists(loaded_state_dict['model']), ""Error loading model""
            model1.load_state_dict(loaded_state_dict['model'], strict=False)
            model2.load_state"
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
","    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
","    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","    token = jwt.encode({
        'exp': datetime.utcnow() + timedelta(days=1)).decode('utf-8'),
      'sub': hashlib.sha256(bytes(username, encoding=""UTF-8"")).hexdigest(),
        'type':'secret',
        'username': username.lower()
    }, algorithm='HS256').decode('utf-8')

"
../airflow/dags/validity_check.py,"import itertools

from airflow.sensors.external_task import ExternalTaskSensor
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from utils.s3_dynamodb_utils import update_item_in_dynamodb, download_file_from_s3
import boto3
import requests
from datetime import datetime

# DynamoDB Table Details
TABLE_NAME = 'pdf_metadata'
DETECTED_TABLE = 'detected_data'
USERS_TABLE = 'user_preferences'
dynamodb = boto3.resource('dynamodb')

pdf_table = dynamodb.Table(TABLE_NAME)
users_table = dynamodb.Table(USERS_TABLE)
detected_table = dynamodb.Table(DETECTED_TABLE)

WEBHOOK_URL = ""https://klffswbb85.execute-api.eu-west-1.amazonaws.com/default/salesTelegramBotHandler""


# Function to check file validity and update detected items only if the status changes
def check_validity_and_update_detected():
    # Initialize DynamoDB clients

    # Get today's date
    today = datetime.utcnow().date()

    changed_to_valid = []
    changed_to_invalid = []

    # Scan the pdf_metadata table to get all items
    pdf_response = pdf_table.scan()
    pdf_items = pdf_response.get('Items', [])

    # Process each PDF item
","    for pdf_item in pdf_items:
        # Remove .pdf from filename for comparison with detected items
        pdf_name_without_ext = pdf_item['filename'].replace('.pdf', '')

        valid_from = datetime.strptime(pdf_item['valid_from'], '%Y-%m-%d').date()
        valid_to = datetime.strptime(pdf_item['valid_to'], '%Y-%m-%d').date()

        # Determine the current validity status
        current_valid_status = pdf_item.get('valid', None)

        # Check if the file should be valid or invalid based on today's date
","        is_valid_now = valid_from <= today <= valid_to

        # If the current status differs from the computed status, it means the status has changed
        if current_valid_status != is_valid_now:
            # Update the valid field in the pdf_metadata table using the utility method
            update_item_in_dynamodb(
                table_name=TABLE_NAME,
                key={'filename': pdf_item['filename'], 'shop_name': pdf_item[""shop_name""]},
                update_expression=""SET valid = :v"",
                expression_attribute_values={':v': is_valid_now}
            )

            # Append the filename to the appropriate list based on the new validity status
            if is_valid_now:
                changed_to_valid.append(pdf_name_without_ext)
            else:
                changed_to_invalid.append(pdf_name_without_ext)

    # Return only files that changed their validity status
    return {
        'valid': changed_to_valid,
        'invalid': changed_to_invalid
    }


def update_detected_items_task(**context):
    """"""
    Task to update detected items based on the output from the previous task.
    """"""
    # Get the valid and invalid files from the context (returned by the previous task)
    task_instance = context['task_instance']

    # Pull the whole dictionary returned by the previous task
    status_change = task_instance.xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' and 'invalid' lists from the returned dictionary
    valid_files = status_change.get('valid', [])
    invalid_files = status_change.get('invalid', [])

    print(f""Debug: valid_files from XCom: {valid_files}"")
    print(f""Debug: invalid_files from XCom: {invalid_files}"")

    if not valid_files and not invalid_files:
        print(""Debug: No files were found for update."")
    else:
        # Update the detected_items table based on valid and invalid files
        update_detected_items_based_on_status(valid_files, invalid_files)


def update_detected_items_based_on_status(valid_files, invalid_files):
    """"""
    Update the detected_items table based on the lists of files that changed status (valid or invalid).
    This function optimizes the update by filtering detected items only with relevant file substrings.
    """"""

    # Remove .pdf extensions from filenames
    valid_files = [file.replace('.pdf', '') for file in valid_files]
    invalid_files = [file.replace('.pdf', '') for file in invalid_files]

    # Combine valid and invalid files
    all_files = valid_files + invalid_files
    print(f""Debug: all_files to be checked (without .pdf): {all_files}"")

    valid_changed_items = []

    detected_response = detected_table.scan()
    detected_items = detected_response.get('Items', [])

    print(f""Debug: Detected items retrieved from DynamoDB: {detected_items}"")

    # Process each detected item
    for detected_item in detected_items:
        detected_image_path = detected_item['image_id']

        # Debug: Show the current detected image path
        print(f""Debug: Processing detected item with image_id: {detected_image_path}"")

        # Check if any file name (without .pdf) is a substring of the image_id
        for file_substr in all_files:
            if file_substr in detected_image_path:
                new_valid_status = file_substr in valid_files

                # Debug: Show the new validity status for the detected item
                print(f""Debug: Changing validity of {detected_image_path} to {new_valid_status}"")

                if detected_item.get('valid') != new_valid_status:
                    # Update the valid field in DynamoDB
                    update_item_in_dynamodb(
                        table_name=DETECTED_TABLE,
                        key={'image_id': detected_item['image_id']},
                        update_expression=""SET valid = :v"",
                        expression_attribute_values={':v': new_valid_status}
                    )
                    print(f""Debug: Updated detected item {detected_item['image_id']} validity to {new_valid_status}"")

                    if new_valid_status:
                        valid_changed_items.append(detected_item)

    print(f""Debug: List of valid_changed_items: {valid_changed_items}"")

    return valid_changed_items


def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    pdf_table = dynamodb.Table(TABLE_NAME)
    response = pdf_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


def regroup_by_shop():
    """"""
    Iterates over all users and creates two columns:
    1. Users for each shop with included shops (all_shops - excluded_shops).
    2. Users for each shop with included shops and receive_pdf_enabled set to True.
    """"""
    # Dictionary to hold the regrouped data
    shop_user_map = {
        ""included_shops"": {},
        ""included_shops_and_receive_pdf"": {}
    }

    # Retrieve all unique shops from the metadata
    all_shops = get_all_shops()

    # Scan the users table to get all user records
    response = users_table.scan()

    # Iterate over all users (items in DynamoDB)
    for item in response['Items']:
        chat_id = item['chat_id']
        receive_pdf_enabled = item.get('receive_pdf_enabled', False)  # Default to False if not set
        excluded_shops = item.get('excluded_shops', [])

        # Calculate included shops (all_shops - excluded_shops)
        included_shops = [shop for shop in all_shops if shop not in excluded_shops]

        # Iterate over all included shops and add users to corresponding shop keys
        for shop in included_shops:
            # Add to ""included_shops"" column (all users with included shops)
            if shop not in shop_user_map[""included_shops""]:
                shop_user_map[""included_shops""][shop] = []
            shop_user_map[""included_shops""][shop].append(chat_id)

            # Add to ""included_shops_and_receive_pdf"" column (users with included shops AND receive_pdf_enabled=True)
            if receive_pdf_enabled:
                if shop not in shop_user_map[""included_shops_and_receive_pdf""]:
                    shop_user_map[""included_shops_and_receive_pdf""][shop] = []
                shop_user_map[""included_shops_and_receive_pdf""][shop].append(chat_id)

    # Return the regrouped data structure
    return shop_user_map


def regroup_shop_to_valid_file(valid_files):
    shop_file_map = {}

    # Ensure valid_files have no .pdf extension
    valid_files = [file.replace('.pdf', '') for file in valid_files]

    # Scan the pdf metadata table
    response = pdf_table.scan()

    # Process each item in the table
    for item in response['Items']:
        shop_name = item['shop_name']
        file_name = item['filename'].replace('.pdf', '')  # Remove .pdf for comparison

        # Check if the file name matches any valid file (without .pdf)
        if file_name in valid_files:
            if shop_name not in shop_file_map:
                shop_file_map[shop_name] = []
            shop_file_map[shop_name].append(item['filename'])  # Use original filename with .pdf for sending

    return shop_file_map


def send_webhook(process_type, shop_name, users_id_list, pdf_file, tracked_item=None):
    """"""
    Sends a POST request to the specified webhook URL with the provided data.
    """"""
    payload = {
        'process_type': process_type,
        'shop_name': shop_name,
        'users_id_list': users_id_list,
        'pdf_file': pdf_file,
    }
    if tracked_item:
        payload['tracked_items_list'] = tracked_item  # Include the tracked item if provided

    # Send the POST request to the webhook
    response = requests.post(WEBHOOK_URL, json=payload)

    # Log the response (optional)
    if response.status_code == 200:
        if tracked_item:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}, tracked_item: {tracked_item}"")
        else:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}"")
    else:
        print(f""Failed to send data to webhook: {response.status_code}, {response.text}"")


def send_updates_in_telegram_task(**context):
    # Pull the whole dictionary returned by the previous task
    status_change = context['task_instance'].xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' files from the returned dictionary
    valid_files = status_change.get('valid', [])

    # Debug print to check what valid files were pulled
    print(f""Debug: valid_files from XCom: {valid_files}"")

    # If no valid files, skip further processing
    if not valid_files:
        print(""Debug: No valid files to process."")
        return

    # Regroup data by shop
    regrouped_data = regroup_by_shop()
    print(f""Debug: regrouped_data: {regrouped_data}"")

    # Regroup valid files by shop
    regrouped_valid_files_data = regroup_shop_to_valid_file(valid_files)
    print(f""Debug: regrouped_valid_files_data: {regrouped_valid_files_data}"")

    for shop_name in regrouped_data['included_shops']:
        shop_included_users_with_receive_pdf = regrouped_data['included_shops_and_receive_pdf'].get(shop_name, [])
        shop_included_users = regrouped_data['included_shops'].get(shop_name, [])

        if shop_name in regrouped_valid_files_data:
            for pdf_file in regrouped_valid_files_data[shop_name]:
                # Debug print before sending the pdf_newsletter webhook
                print(f""Debug: Sending pdf_newsletter for shop: {shop_name}, pdf_file: {pdf_file}, ""
                      f""users_with_receive_pdf: {shop_included_users_with_receive_pdf}"")

                send_webhook('pdf_newsletter', shop_name, shop_included_users_with_receive_pdf, pdf_file)

                batch_size = 3
                user_batches = [list(group) for group in
                                itertools.zip_longest(*[iter(shop_included_users)] * batch_size)]

                for user_batch in user_batches:
                    valid_users = [user for user in user_batch if user is not None]
                    user_ids_list = []
                    user_tracked_items_list = []

                    for user_id in valid_users:
                        response = users_table.get_item(Key={'chat_id': user_id})
                        if 'Item' in response:
                            user_tracked_items = response['Item'].get('tracked_items', [])
                            if user_tracked_items:
                                user_ids_list.append(user_id)
                                user_tracked_items_list.append(user_tracked_items)

                    if user_ids_list and user_tracked_items_list:
                        # Debug print before sending the tracked_items_list webhook
                        print(f""Debug: Sending tracked_items_list for shop: {shop_name}, pdf_file: {pdf_file}, ""
                              f""user_ids_list: {user_ids_list}, tracked_items: {user_tracked_items_list}"")

                        send_webhook('tracked_items_list', shop_name, user_ids_list, pdf_file, user_tracked_items_list)


# Airflow DAG setup
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
}

with DAG(
        dag_id='check_file_validity_and_update_detected_items',
        default_args=default_args,
        schedule_interval='0 1 * * *',  # Runs at 08:00 AM UTC daily
        catchup=False
) as dag:
    # Task to check and update the PDF metadata validity
    check_files_and_update_detected = PythonOperator(
        task_id='check_validity_and_update_detected_task',
        python_callable=check_validity_and_update_detected,
    )

    # Task to update the detected items based on the result of the first task
    update_detected_items = PythonOperator(
        task_id='update_detected_items_task',
        python_callable=update_detected_items_task,
        provide_context=True
    )

    # Task to send updates in Telegram after updating detected items
    send_updates_in_telegram = PythonOperator(
        task_id='send_updates_in_telegram_task',
        python_callable=send_updates_in_telegram_task,
        provide_context=True
    )

    # Task dependencies
    check_files_and_update_detected >> update_detected_items >> send_updates_in_telegram
","    for pdf_item in pdf_items:
        # Ignore non-existing files
        if '.pdf' in pdf_item['filename':
            continue

        # Get all the photos in the pdf item
        photo_filenames = ['photo/' + str(i) + '.jpg' for i in range(64)}

        # Filter out photos that are part of the detected images
        photos_not_in_detected_images = [img for img in photo_filenames if os.path.isfile(os.path.join('.', img))]]

        # Determine if there is an existing pdf item without its associated file pattern"
../PycharmProjects/sales_telegram_bot/backend/sales_telegram_bot_admin_backend/app.py,"import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import boto3
from botocore.exceptions import NoCredentialsError, ClientError
from airflow.api.client.local_client import Client
import bcrypt
import jwt
import json
from datetime import datetime, timedelta
from functools import wraps

# AWS and Airflow configurations
dynamodb = boto3.resource('dynamodb')
PDF_TABLE_NAME = 'pdf_metadata'
USERS_TABLE_NAME = 'admin_page_users'
BUCKET_NAME = 'salestelegrambot'
AWS_REGION = 'eu-west-1'
AIRFLOW_URL = 'http://localhost:8080/api/v1'
AIRFLOW_DAG_ID = 'pages_data_pipeline'
users_table = dynamodb.Table(USERS_TABLE_NAME)
SECRET_KEY = os.getenv('SECRET_KEY')

s3 = boto3.client('s3', region_name=AWS_REGION)

# Flask app setup
app = Flask(__name__)
CORS(app, resources={r""/*"": {""origins"": ""http://52.17.229.28""}})

# List of shops
shops = [
    {""name"": ""Albert Supermarket""}, {""name"": ""Albert Hypermarket""}, {""name"": ""CBA Premium""},
    {""name"": ""CBA Potraviny""}, {""name"": ""CBA Market""}, {""name"": ""Flop""}, {""name"": ""Flop Top""},
    {""name"": ""Kaufland""}, {""name"": ""Makro""}, {""name"": ""Ratio""}, {""name"": ""Tesco Hypermarket""},
    {""name"": ""Tesco Supermarket""}, {""name"": ""Bene""}, {""name"": ""EsoMarket""}, {""name"": ""Globus""},
    {""name"": ""Tamda Foods""}, {""name"": ""Prodejny Zeman""}, {""name"": ""Billa""}, {""name"": ""Lidl""},
    {""name"": ""Lidl Shop""}, {""name"": ""Penny""}, {""name"": ""Travel Free""}, {""name"": ""Zeman""}
]

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None

        # Check if the token is passed in the Authorization header
        if 'Authorization' in request.headers:
            try:
                # Extract token from ""Bearer <token>""
                token = request.headers['Authorization'].split("" "")[1]  

            except Exception as e:
                # Debug: If there was an issue extracting the token, print the error
                print(f""Error extracting token: {str(e)}"")
        else:
            # Debug: If Authorization header is missing
            print(""No Authorization header found"")

        # If no token is present, return an error
        if not token:
            print(""Token is missing!"")  # Debug: Log missing token
            return jsonify({'message': 'Token is missing!'}), 401

        try:
            # Decode the token and verify it
            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = data['username']  # Retrieve the user information
            print(f""Token decoded successfully. Current user: {current_user}"")  # Debug: Log successful decode

        except jwt.ExpiredSignatureError:
            # Debug: Token has expired
            print(""Token has expired!"")
            return jsonify({'message': 'Token has expired!'}), 401

        except jwt.InvalidTokenError as e:
            # Debug: Token is invalid
            print(f""Invalid token: {str(e)}"")
            return jsonify({'message': 'Token is invalid!'}), 401

        # If token is valid, proceed to the wrapped function
        return f(current_user, *args, **kwargs)

    return decorated

def load_pdf_data():
    """"""Load PDF metadata from DynamoDB.""""""
    try:
        response = dynamodb.Table(PDF_TABLE_NAME).scan()
        return response.get('Items', [])
    except Exception as e:
        logging.error(f""Error loading data from DynamoDB: {e}"")
        return []


def save_pdf_data(data):
    """"""Save PDF metadata to DynamoDB.""""""
    table = dynamodb.Table(PDF_TABLE_NAME)
    try:
        for entry in data:
            table.put_item(Item=entry)
    except Exception as e:
        logging.error(f""Error saving data to DynamoDB: {e}"")


def get_unique_filename(filepath):
    """"""Generate a unique filename if the file already exists.""""""
    base, ext = os.path.splitext(filepath)
    counter = 1
    new_filepath = filepath
    while os.path.exists(new_filepath):
        new_filepath = f""{base}_{counter}{ext}""
        counter += 1
    return new_filepath


def hash_password(password):
    """"""Hash the password using bcrypt.""""""
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt)


# def initialize_admin_user():
#     """"""Create a default admin user in DynamoDB if no users exist.""""""
#     try:
#         # Check if the 'users' table is empty
#         response = users_table.scan()
#         if not response['Items']:
#             # No users found, insert default admin credentials
#             hashed_password = hash_password(DEFAULT_PASSWORD)
#             users_table.put_item(
#                 Item={
#                     'username': DEFAULT_USERNAME,
#                     'password': hashed_password.decode('utf-8'),
#                     'role': 'admin'  # Setting the role to admin
#                 }
#             )
#             print(""Default admin user created."")
#         else:
#             print(""Admin user already exists."")
#     except Exception as e:
#         print(f""Error initializing admin user: {e}"")


def check_password(stored_password, provided_password):
	return bcrypt.checkpw(provided_password.encode('utf-8'), stored_password.encode('utf-8'))


def generate_token(username):
    """"""Generate JWT token for the user.""""""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=1)  # Token expires in 1 hour
    }
    token = jwt.encode(payload, SECRET_KEY, algorithm='HS256')
    return token


@app.route('/login', methods=['POST'])
def login():
    data = request.json
    username = data.get('username')
    password = data.get('password')

    try:
        # Retrieve the user from DynamoDB
        response = users_table.get_item(Key={'username': username})
        user = response.get('Item')

        if not user:
            return jsonify({""error"": ""Invalid username""}), 401

","        stored_password = user.get('password')

        # Debug: Print the types of variables
        print(f""Stored password type: {type(stored_password)}"")  # Should be str
        print(f""Provided password type: {type(password)}"")        # Should be str

        # Check if either value is None or invalid
        if not stored_password or not isinstance(stored_password, str):
            return jsonify({""error"": ""Stored password is invalid""}), 500

","        if not check_password(stored_password, password):
            return jsonify({""error"": ""Invalid password""}), 401

        # Generate JWT token upon successful login
        token = generate_token(username)
        return jsonify({""message"": ""Login successful"", ""token"": token}), 200

    except Exception as e:
        # Print the full traceback of the error for better debugging
        import traceback
        print(traceback.format_exc())
        return jsonify({""error"": f""Error during login: {e}""}), 500

# Endpoints
@app.route('/shops', methods=['GET'])
@token_required
def get_shops(current_user):
    return jsonify(shops)


@app.route('/pdfs', methods=['GET'])
@token_required
def get_pdfs(current_user):
    pdf_data = load_pdf_data()
    return jsonify(pdf_data)


@app.route('/upload', methods=['POST'])
@token_required
def upload_file(current_user):
    shop_name = request.form.get('shop_name')
    valid_from = request.form.get('valid_from')
    valid_to = request.form.get('valid_to')
    file = request.files.get('file')
    file_url = request.form.get('file_url')

    if not shop_name or not valid_from or not valid_to:
        return jsonify({""error"": ""Missing required fields""}), 400

    try:
        if file:
            # Handling file upload from form data
            filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""

        elif file_url:
            # Handling file download from a provided URL and uploading to S3
            response = requests.get(file_url, stream=True)
            if response.status_code == 200:
                filename = file_url.split('/')[-1].split('?')[0]  # Extract filename from URL
                s3.upload_fileobj(response.raw, BUCKET_NAME, f'pdfs/{filename}')
                s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{filename}""
            else:
                return jsonify({""error"": f""Failed to download file from {file_url}""}), 400
        else:
            return jsonify({""error"": ""Either file or file_url must be provided""}), 400

        # Save metadata in DynamoDB or your preferred storage
        pdf_entry = {
            ""shop_name"": shop_name,
            ""filename"": filename,
            ""s3_url"": s3_url,
            ""valid_from"": valid_from,
            ""valid_to"": valid_to,
            ""upload_date"": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            ""page_split"": False,
            ""used"": False,
            ""valid"": False
        }
        pdf_data = load_pdf_data()
        pdf_data.append(pdf_entry)
        save_pdf_data(pdf_data)

        return jsonify({""message"": ""File uploaded successfully"", ""filename"": filename, ""s3_url"": s3_url}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500


@app.route('/update/<filename>', methods=['POST'])
@token_required
def update_file(current_user, filename):
    try:
        # Load the current PDF data from DynamoDB
        pdf_data = load_pdf_data()

        # Find the entry for the specified filename
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Get the incoming data from the request
        shop_name = request.form.get('shop_name', file_entry['shop_name'])  # Default to existing value
        valid_from = request.form.get('valid_from', file_entry['valid_from'])
        valid_to = request.form.get('valid_to', file_entry['valid_to'])
        file = request.files.get('file')

        # Update file if a new one is provided
        if file:
            # Remove old file from S3
            s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

            # Upload new file to S3
            new_filename = file.filename
            s3.upload_fileobj(file, BUCKET_NAME, f'pdfs/{new_filename}')
            s3_url = f""https://{BUCKET_NAME}.s3.amazonaws.com/pdfs/{new_filename}""
            file_entry['filename'] = new_filename
            file_entry['s3_url'] = s3_url

        # Update the metadata
        file_entry['shop_name'] = shop_name
        file_entry['valid_from'] = valid_from
        file_entry['valid_to'] = valid_to
        file_entry['valid'] = False

        # Save updated entry back to DynamoDB
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} updated successfully"", ""valid"": False}), 200

    except Exception as e:
        return jsonify({""error"": f""Error updating file: {e}""}), 500


@app.route('/trigger_pipeline/<filename>', methods=['POST'])
@token_required
def trigger_pipeline(current_user, filename):
    pdf_data = load_pdf_data()
    file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

    if not file_entry:
        return jsonify({""error"": ""File not found""}), 404

    payload = {'filename': filename, 'shop_name': file_entry['shop_name'], 'valid': file_entry['valid']}
    app.logger.debug(f""Triggering Airflow DAG with payload: {json.dumps(payload)}"")

    try:
        client = Client(None, None)
        client.trigger_dag(dag_id=AIRFLOW_DAG_ID, run_id=None, conf=payload)

        file_entry['used'] = True
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""Pipeline triggered for {filename}""}), 200
    except Exception as e:
        app.logger.error(f""Failed to trigger Airflow DAG: {e}"")
        return jsonify({""error"": ""Failed to trigger Airflow DAG""}), 500


@app.route('/delete/<filename>', methods=['DELETE'])
@token_required
def delete_file(current_user, filename):
    try:
        # Load PDF data
        pdf_data = load_pdf_data()

        # Find the entry in the DynamoDB table
        file_entry = next((entry for entry in pdf_data if entry['filename'] == filename), None)

        if not file_entry:
            return jsonify({""error"": ""File not found""}), 404

        # Delete the file from S3
        s3.delete_object(Bucket=BUCKET_NAME, Key=f'pdfs/{filename}')

        # Remove the entry from DynamoDB
        table = dynamodb.Table(PDF_TABLE_NAME)
        table.delete_item(
            Key={
                'filename': filename,
                'shop_name': file_entry['shop_name']
            }
        )

        # Remove the file entry from the local pdf_data if applicable
        pdf_data = [entry for entry in pdf_data if entry['filename'] != filename]
        save_pdf_data(pdf_data)

        return jsonify({""message"": f""File {filename} deleted successfully""}), 200

    except NoCredentialsError:
        return jsonify({""error"": ""AWS credentials not available""}), 500
    except Exception as e:
        return jsonify({""error"": f""Error deleting file: {e}""}), 500


if __name__ == '__main__':
    from werkzeug.middleware.proxy_fix import ProxyFix
    app.wsgi_app = ProxyFix(app.wsgi_app)
    app.run(host='0.0.0.0')
","    except NoCredentialsError:
        return jsonify({""error"": 'Username does not exist'}), 401
    except Exception as e:
        app.logger.error(f'Error generating access token: {e}')
        return jsonify({""error"": f'Error generating access token: {e}'})


@app.route('/', methods=[""GET""]))
@token_required
def index():
    return jsonify(SHOP_LIST), 200


@app.route(""/<int:current_user>/<string:shop_name>"", methods=[""GET""])
@token_required
def show"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/six.py,"# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""""""Utilities for writing code that runs on Python 2 and 3""""""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.16.0""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith(""java""):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):

            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """"""Add documentation to a function.""""""
    func.__doc__ = doc


def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):

    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = [""__doc__"", ""__name__""]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """"""
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """"""

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + ""."" + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + ""."" + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError(""This loader does not know module "" + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """"""
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """"""
        return hasattr(self.__get_module(fullname), ""__path__"")

    def get_code(self, fullname):
        """"""Return None

        Required, if is_package is implemented""""""
        self.__get_module(fullname)  # eventually raises ImportError
        return None
    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass

_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """"""Lazy loading of moved objects""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""cStringIO"", ""cStringIO"", ""io"", ""StringIO""),
    MovedAttribute(""filter"", ""itertools"", ""builtins"", ""ifilter"", ""filter""),
    MovedAttribute(""filterfalse"", ""itertools"", ""itertools"", ""ifilterfalse"", ""filterfalse""),
    MovedAttribute(""input"", ""__builtin__"", ""builtins"", ""raw_input"", ""input""),
    MovedAttribute(""intern"", ""__builtin__"", ""sys""),
    MovedAttribute(""map"", ""itertools"", ""builtins"", ""imap"", ""map""),
    MovedAttribute(""getcwd"", ""os"", ""os"", ""getcwdu"", ""getcwd""),
    MovedAttribute(""getcwdb"", ""os"", ""os"", ""getcwd"", ""getcwdb""),
    MovedAttribute(""getoutput"", ""commands"", ""subprocess""),
    MovedAttribute(""range"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""reload_module"", ""__builtin__"", ""importlib"" if PY34 else ""imp"", ""reload""),
    MovedAttribute(""reduce"", ""__builtin__"", ""functools""),
    MovedAttribute(""shlex_quote"", ""pipes"", ""shlex"", ""quote""),
    MovedAttribute(""StringIO"", ""StringIO"", ""io""),
    MovedAttribute(""UserDict"", ""UserDict"", ""collections""),
    MovedAttribute(""UserList"", ""UserList"", ""collections""),
    MovedAttribute(""UserString"", ""UserString"", ""collections""),
    MovedAttribute(""xrange"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""zip"", ""itertools"", ""builtins"", ""izip"", ""zip""),
    MovedAttribute(""zip_longest"", ""itertools"", ""itertools"", ""izip_longest"", ""zip_longest""),
    MovedModule(""builtins"", ""__builtin__""),
    MovedModule(""configparser"", ""ConfigParser""),
    MovedModule(""collections_abc"", ""collections"", ""collections.abc"" if sys.version_info >= (3, 3) else ""collections""),
    MovedModule(""copyreg"", ""copy_reg""),
    MovedModule(""dbm_gnu"", ""gdbm"", ""dbm.gnu""),
    MovedModule(""dbm_ndbm"", ""dbm"", ""dbm.ndbm""),
    MovedModule(""_dummy_thread"", ""dummy_thread"", ""_dummy_thread"" if sys.version_info < (3, 9) else ""_thread""),
    MovedModule(""http_cookiejar"", ""cookielib"", ""http.cookiejar""),
    MovedModule(""http_cookies"", ""Cookie"", ""http.cookies""),
    MovedModule(""html_entities"", ""htmlentitydefs"", ""html.entities""),
    MovedModule(""html_parser"", ""HTMLParser"", ""html.parser""),
    MovedModule(""http_client"", ""httplib"", ""http.client""),
    MovedModule(""email_mime_base"", ""email.MIMEBase"", ""email.mime.base""),
    MovedModule(""email_mime_image"", ""email.MIMEImage"", ""email.mime.image""),
    MovedModule(""email_mime_multipart"", ""email.MIMEMultipart"", ""email.mime.multipart""),
    MovedModule(""email_mime_nonmultipart"", ""email.MIMENonMultipart"", ""email.mime.nonmultipart""),
    MovedModule(""email_mime_text"", ""email.MIMEText"", ""email.mime.text""),
    MovedModule(""BaseHTTPServer"", ""BaseHTTPServer"", ""http.server""),
    MovedModule(""CGIHTTPServer"", ""CGIHTTPServer"", ""http.server""),
    MovedModule(""SimpleHTTPServer"", ""SimpleHTTPServer"", ""http.server""),
    MovedModule(""cPickle"", ""cPickle"", ""pickle""),
    MovedModule(""queue"", ""Queue""),
    MovedModule(""reprlib"", ""repr""),
    MovedModule(""socketserver"", ""SocketServer""),
    MovedModule(""_thread"", ""thread"", ""_thread""),
    MovedModule(""tkinter"", ""Tkinter""),
    MovedModule(""tkinter_dialog"", ""Dialog"", ""tkinter.dialog""),
    MovedModule(""tkinter_filedialog"", ""FileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_scrolledtext"", ""ScrolledText"", ""tkinter.scrolledtext""),
    MovedModule(""tkinter_simpledialog"", ""SimpleDialog"", ""tkinter.simpledialog""),
    MovedModule(""tkinter_tix"", ""Tix"", ""tkinter.tix""),
    MovedModule(""tkinter_ttk"", ""ttk"", ""tkinter.ttk""),
    MovedModule(""tkinter_constants"", ""Tkconstants"", ""tkinter.constants""),
    MovedModule(""tkinter_dnd"", ""Tkdnd"", ""tkinter.dnd""),
    MovedModule(""tkinter_colorchooser"", ""tkColorChooser"",
                ""tkinter.colorchooser""),
    MovedModule(""tkinter_commondialog"", ""tkCommonDialog"",
                ""tkinter.commondialog""),
    MovedModule(""tkinter_tkfiledialog"", ""tkFileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_font"", ""tkFont"", ""tkinter.font""),
    MovedModule(""tkinter_messagebox"", ""tkMessageBox"", ""tkinter.messagebox""),
    MovedModule(""tkinter_tksimpledialog"", ""tkSimpleDialog"",
                ""tkinter.simpledialog""),
    MovedModule(""urllib_parse"", __name__ + "".moves.urllib_parse"", ""urllib.parse""),
    MovedModule(""urllib_error"", __name__ + "".moves.urllib_error"", ""urllib.error""),
    MovedModule(""urllib"", __name__ + "".moves.urllib"", __name__ + "".moves.urllib""),
    MovedModule(""urllib_robotparser"", ""robotparser"", ""urllib.robotparser""),
    MovedModule(""xmlrpc_client"", ""xmlrpclib"", ""xmlrpc.client""),
    MovedModule(""xmlrpc_server"", ""SimpleXMLRPCServer"", ""xmlrpc.server""),
]
# Add windows specific modules.
if sys.platform == ""win32"":
    _moved_attributes += [
        MovedModule(""winreg"", ""_winreg""),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
    if isinstance(attr, MovedModule):
        _importer._add_module(attr, ""moves."" + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + "".moves"")
_importer._add_module(moves, ""moves"")


class Module_six_moves_urllib_parse(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


","_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""SplitResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qs"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qsl"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urldefrag"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urljoin"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""quote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""quote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_to_bytes"", ""urllib"", ""urllib.parse"", ""unquote"", ""unquote_to_bytes""),
    MovedAttribute(""urlencode"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitquery"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splittag"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splituser"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitvalue"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""uses_fragment"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_netloc"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_params"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_query"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_relative"", ""urlparse"", ""urllib.parse""),
]
for attr in _urllib_parse_moved_attributes:
","    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(Module_six_moves_urllib_parse(__name__ + "".moves.urllib_parse""),
                      ""moves.urllib_parse"", ""moves.urllib.parse"")


class Module_six_moves_urllib_error(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_error""""""


_urllib_error_moved_attributes = [
    MovedAttribute(""URLError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""HTTPError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""ContentTooShortError"", ""urllib"", ""urllib.error""),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(Module_six_moves_urllib_error(__name__ + "".moves.urllib.error""),
                      ""moves.urllib_error"", ""moves.urllib.error"")


class Module_six_moves_urllib_request(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_request""""""


_urllib_request_moved_attributes = [
    MovedAttribute(""urlopen"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""install_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""build_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""pathname2url"", ""urllib"", ""urllib.request""),
    MovedAttribute(""url2pathname"", ""urllib"", ""urllib.request""),
    MovedAttribute(""getproxies"", ""urllib"", ""urllib.request""),
    MovedAttribute(""Request"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""OpenerDirector"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDefaultErrorHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPRedirectHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPCookieProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""BaseHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgr"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgrWithDefaultRealm"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPSHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FileHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""CacheFTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""UnknownHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPErrorProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""urlretrieve"", ""urllib"", ""urllib.request""),
    MovedAttribute(""urlcleanup"", ""urllib"", ""urllib.request""),
    MovedAttribute(""URLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""FancyURLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""proxy_bypass"", ""urllib"", ""urllib.request""),
    MovedAttribute(""parse_http_list"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""parse_keqv_list"", ""urllib2"", ""urllib.request""),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(Module_six_moves_urllib_request(__name__ + "".moves.urllib.request""),
                      ""moves.urllib_request"", ""moves.urllib.request"")


class Module_six_moves_urllib_response(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_response""""""


_urllib_response_moved_attributes = [
    MovedAttribute(""addbase"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addclosehook"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfo"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfourl"", ""urllib"", ""urllib.response""),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(Module_six_moves_urllib_response(__name__ + "".moves.urllib.response""),
                      ""moves.urllib_response"", ""moves.urllib.response"")


class Module_six_moves_urllib_robotparser(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_robotparser""""""


_urllib_robotparser_moved_attributes = [
    MovedAttribute(""RobotFileParser"", ""robotparser"", ""urllib.robotparser""),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes

_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + "".moves.urllib.robotparser""),
                      ""moves.urllib_robotparser"", ""moves.urllib.robotparser"")


class Module_six_moves_urllib(types.ModuleType):

    """"""Create a six.moves.urllib namespace that resembles the Python 3 namespace""""""
    __path__ = []  # mark as package
    parse = _importer._get_module(""moves.urllib_parse"")
    error = _importer._get_module(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
    response = _importer._get_module(""moves.urllib_response"")
    robotparser = _importer._get_module(""moves.urllib_robotparser"")

    def __dir__(self):
        return ['parse', 'error', 'request', 'response', 'robotparser']

_importer._add_module(Module_six_moves_urllib(__name__ + "".moves.urllib""),
                      ""moves.urllib"")


def add_move(move):
    """"""Add an item to six.moves.""""""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """"""Remove item from six.moves.""""""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError(""no such move, %r"" % (name,))


if PY3:
    _meth_func = ""__func__""
    _meth_self = ""__self__""

    _func_closure = ""__closure__""
    _func_code = ""__code__""
    _func_defaults = ""__defaults__""
    _func_globals = ""__globals__""
else:
    _meth_func = ""im_func""
    _meth_self = ""im_self""

    _func_closure = ""func_closure""
    _func_code = ""func_code""
    _func_defaults = ""func_defaults""
    _func_globals = ""func_globals""


try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any(""__call__"" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """"""Get the function out of a possibly unbound function"""""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:
    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller(""keys"")

    viewvalues = operator.methodcaller(""values"")

    viewitems = operator.methodcaller(""items"")
else:
    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller(""viewkeys"")

    viewvalues = operator.methodcaller(""viewvalues"")

    viewitems = operator.methodcaller(""viewitems"")

_add_doc(iterkeys, ""Return an iterator over the keys of a dictionary."")
_add_doc(itervalues, ""Return an iterator over the values of a dictionary."")
_add_doc(iteritems,
         ""Return an iterator over the (key, value) pairs of a dictionary."")
_add_doc(iterlists,
         ""Return an iterator over the (key, [values]) pairs of a dictionary."")


if PY3:
    def b(s):
        return s.encode(""latin-1"")

    def u(s):
        return s
    unichr = chr
    import struct
    int2byte = struct.Struct("">B"").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = ""assertCountEqual""
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = ""assertRaisesRegexp""
        _assertRegex = ""assertRegexpMatches""
        _assertNotRegex = ""assertNotRegexpMatches""
    else:
        _assertRaisesRegex = ""assertRaisesRegex""
        _assertRegex = ""assertRegex""
        _assertNotRegex = ""assertNotRegex""
else:
    def b(s):
        return s
    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r'\\', r'\\\\'), ""unicode_escape"")
    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])
    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = ""assertItemsEqual""
    _assertRaisesRegex = ""assertRaisesRegexp""
    _assertRegex = ""assertRegexpMatches""
    _assertNotRegex = ""assertNotRegexpMatches""
_add_doc(b, """"""Byte literal"""""")
_add_doc(u, """"""Text literal"""""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, ""exec"")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """"""Execute code in a namespace.""""""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(""""""exec _code_ in _globs_, _locs_"""""")

    exec_(""""""def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""""")


if sys.version_info[:2] > (3,):
    exec_(""""""def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""""")
else:
    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, ""print"", None)
if print_ is None:
    def print_(*args, **kwargs):
        """"""The new-style print function for Python 2.4 and 2.5.""""""
        fp = kwargs.pop(""file"", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError(""sep must be None or a string"")
        end = kwargs.pop(""end"", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError(""end must be None or a string"")
        if kwargs:
            raise TypeError(""invalid keyword arguments to print()"")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode(""\n"")
            space = unicode("" "")
        else:
            newline = ""\n""
            space = "" ""
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get(""file"", sys.stdout)
        flush = kwargs.pop(""flush"", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()

_add_doc(reraise, """"""Reraise an exception."""""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(wrapper, wrapped,
                        assigned=functools.WRAPPER_ASSIGNMENTS,
                        updated=functools.WRAPPER_UPDATES):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper
    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
              updated=functools.WRAPPER_UPDATES):
        return functools.partial(_update_wrapper, wrapped=wrapped,
                                 assigned=assigned, updated=updated)
    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):

        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d['__orig_bases__'] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})


def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get('__slots__')
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        if hasattr(cls, '__qualname__'):
            orig_vars['__qualname__'] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper


def ensure_binary(s, encoding='utf-8', errors='strict'):
    """"""Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """"""
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError(""not expecting type '%s'"" % type(s))


def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    return s


def ensure_text(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError(""not expecting type '%s'"" % type(s))


def python_2_unicode_compatible(klass):
    """"""
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """"""
    if PY2:
        if '__str__' not in klass.__dict__:
            raise ValueError(""@python_2_unicode_compatible cannot be applied ""
                             ""to %s because it doesn't define __str__()."" %
                             klass.__name__)
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get(""__spec__"") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another ""instance"" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (type(importer).__name__ == ""_SixMetaPathImporter"" and
                importer.name == __name__):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)
",")

    Iterator = types.Iterator
    CreateBoundMethod = types.FunctionType


class _UnresolvedMethodContextManager:
    def __getattr__(self, name):
        try:
            return getattr(moves, name, _UnresolvedMethodContextManager)()
        except AttributeError:
            return getattr(moves, '__call__' + name, _UnresolvedMethodContextManager)(name)

_UnresolvedMethodContextManager.__doc__ = getattr(moves, ""__doc__"", """")
setattr(moves, ""__doc__"", _UnresolvedMethodContextManager.__doc__.replace(""\n\n"", ""\n""))
delattr("
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/six.py,"# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""""""Utilities for writing code that runs on Python 2 and 3""""""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.16.0""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith(""java""):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):

            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """"""Add documentation to a function.""""""
    func.__doc__ = doc


def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):

    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = [""__doc__"", ""__name__""]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """"""
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """"""

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + ""."" + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + ""."" + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError(""This loader does not know module "" + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """"""
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """"""
        return hasattr(self.__get_module(fullname), ""__path__"")

    def get_code(self, fullname):
        """"""Return None

        Required, if is_package is implemented""""""
        self.__get_module(fullname)  # eventually raises ImportError
        return None
    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass

_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """"""Lazy loading of moved objects""""""
    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute(""cStringIO"", ""cStringIO"", ""io"", ""StringIO""),
    MovedAttribute(""filter"", ""itertools"", ""builtins"", ""ifilter"", ""filter""),
    MovedAttribute(""filterfalse"", ""itertools"", ""itertools"", ""ifilterfalse"", ""filterfalse""),
    MovedAttribute(""input"", ""__builtin__"", ""builtins"", ""raw_input"", ""input""),
    MovedAttribute(""intern"", ""__builtin__"", ""sys""),
    MovedAttribute(""map"", ""itertools"", ""builtins"", ""imap"", ""map""),
    MovedAttribute(""getcwd"", ""os"", ""os"", ""getcwdu"", ""getcwd""),
    MovedAttribute(""getcwdb"", ""os"", ""os"", ""getcwd"", ""getcwdb""),
    MovedAttribute(""getoutput"", ""commands"", ""subprocess""),
    MovedAttribute(""range"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""reload_module"", ""__builtin__"", ""importlib"" if PY34 else ""imp"", ""reload""),
    MovedAttribute(""reduce"", ""__builtin__"", ""functools""),
    MovedAttribute(""shlex_quote"", ""pipes"", ""shlex"", ""quote""),
    MovedAttribute(""StringIO"", ""StringIO"", ""io""),
    MovedAttribute(""UserDict"", ""UserDict"", ""collections""),
    MovedAttribute(""UserList"", ""UserList"", ""collections""),
    MovedAttribute(""UserString"", ""UserString"", ""collections""),
    MovedAttribute(""xrange"", ""__builtin__"", ""builtins"", ""xrange"", ""range""),
    MovedAttribute(""zip"", ""itertools"", ""builtins"", ""izip"", ""zip""),
    MovedAttribute(""zip_longest"", ""itertools"", ""itertools"", ""izip_longest"", ""zip_longest""),
    MovedModule(""builtins"", ""__builtin__""),
    MovedModule(""configparser"", ""ConfigParser""),
    MovedModule(""collections_abc"", ""collections"", ""collections.abc"" if sys.version_info >= (3, 3) else ""collections""),
    MovedModule(""copyreg"", ""copy_reg""),
    MovedModule(""dbm_gnu"", ""gdbm"", ""dbm.gnu""),
    MovedModule(""dbm_ndbm"", ""dbm"", ""dbm.ndbm""),
    MovedModule(""_dummy_thread"", ""dummy_thread"", ""_dummy_thread"" if sys.version_info < (3, 9) else ""_thread""),
    MovedModule(""http_cookiejar"", ""cookielib"", ""http.cookiejar""),
    MovedModule(""http_cookies"", ""Cookie"", ""http.cookies""),
    MovedModule(""html_entities"", ""htmlentitydefs"", ""html.entities""),
    MovedModule(""html_parser"", ""HTMLParser"", ""html.parser""),
    MovedModule(""http_client"", ""httplib"", ""http.client""),
    MovedModule(""email_mime_base"", ""email.MIMEBase"", ""email.mime.base""),
    MovedModule(""email_mime_image"", ""email.MIMEImage"", ""email.mime.image""),
    MovedModule(""email_mime_multipart"", ""email.MIMEMultipart"", ""email.mime.multipart""),
    MovedModule(""email_mime_nonmultipart"", ""email.MIMENonMultipart"", ""email.mime.nonmultipart""),
    MovedModule(""email_mime_text"", ""email.MIMEText"", ""email.mime.text""),
    MovedModule(""BaseHTTPServer"", ""BaseHTTPServer"", ""http.server""),
    MovedModule(""CGIHTTPServer"", ""CGIHTTPServer"", ""http.server""),
    MovedModule(""SimpleHTTPServer"", ""SimpleHTTPServer"", ""http.server""),
    MovedModule(""cPickle"", ""cPickle"", ""pickle""),
    MovedModule(""queue"", ""Queue""),
    MovedModule(""reprlib"", ""repr""),
    MovedModule(""socketserver"", ""SocketServer""),
    MovedModule(""_thread"", ""thread"", ""_thread""),
    MovedModule(""tkinter"", ""Tkinter""),
    MovedModule(""tkinter_dialog"", ""Dialog"", ""tkinter.dialog""),
    MovedModule(""tkinter_filedialog"", ""FileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_scrolledtext"", ""ScrolledText"", ""tkinter.scrolledtext""),
    MovedModule(""tkinter_simpledialog"", ""SimpleDialog"", ""tkinter.simpledialog""),
    MovedModule(""tkinter_tix"", ""Tix"", ""tkinter.tix""),
    MovedModule(""tkinter_ttk"", ""ttk"", ""tkinter.ttk""),
    MovedModule(""tkinter_constants"", ""Tkconstants"", ""tkinter.constants""),
    MovedModule(""tkinter_dnd"", ""Tkdnd"", ""tkinter.dnd""),
    MovedModule(""tkinter_colorchooser"", ""tkColorChooser"",
                ""tkinter.colorchooser""),
    MovedModule(""tkinter_commondialog"", ""tkCommonDialog"",
                ""tkinter.commondialog""),
    MovedModule(""tkinter_tkfiledialog"", ""tkFileDialog"", ""tkinter.filedialog""),
    MovedModule(""tkinter_font"", ""tkFont"", ""tkinter.font""),
    MovedModule(""tkinter_messagebox"", ""tkMessageBox"", ""tkinter.messagebox""),
    MovedModule(""tkinter_tksimpledialog"", ""tkSimpleDialog"",
                ""tkinter.simpledialog""),
    MovedModule(""urllib_parse"", __name__ + "".moves.urllib_parse"", ""urllib.parse""),
    MovedModule(""urllib_error"", __name__ + "".moves.urllib_error"", ""urllib.error""),
    MovedModule(""urllib"", __name__ + "".moves.urllib"", __name__ + "".moves.urllib""),
    MovedModule(""urllib_robotparser"", ""robotparser"", ""urllib.robotparser""),
    MovedModule(""xmlrpc_client"", ""xmlrpclib"", ""xmlrpc.client""),
    MovedModule(""xmlrpc_server"", ""SimpleXMLRPCServer"", ""xmlrpc.server""),
]
# Add windows specific modules.
if sys.platform == ""win32"":
    _moved_attributes += [
        MovedModule(""winreg"", ""_winreg""),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
    if isinstance(attr, MovedModule):
        _importer._add_module(attr, ""moves."" + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + "".moves"")
_importer._add_module(moves, ""moves"")


class Module_six_moves_urllib_parse(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""SplitResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qs"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""parse_qsl"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urldefrag"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urljoin"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunparse"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""urlunsplit"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""quote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""quote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_plus"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""unquote_to_bytes"", ""urllib"", ""urllib.parse"", ""unquote"", ""unquote_to_bytes""),
    MovedAttribute(""urlencode"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitquery"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splittag"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splituser"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""splitvalue"", ""urllib"", ""urllib.parse""),
    MovedAttribute(""uses_fragment"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_netloc"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_params"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_query"", ""urlparse"", ""urllib.parse""),
    MovedAttribute(""uses_relative"", ""urlparse"", ""urllib.parse""),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(Module_six_moves_urllib_parse(__name__ + "".moves.urllib_parse""),
                      ""moves.urllib_parse"", ""moves.urllib.parse"")


class Module_six_moves_urllib_error(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_error""""""


_urllib_error_moved_attributes = [
    MovedAttribute(""URLError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""HTTPError"", ""urllib2"", ""urllib.error""),
    MovedAttribute(""ContentTooShortError"", ""urllib"", ""urllib.error""),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(Module_six_moves_urllib_error(__name__ + "".moves.urllib.error""),
                      ""moves.urllib_error"", ""moves.urllib.error"")


class Module_six_moves_urllib_request(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_request""""""


_urllib_request_moved_attributes = [
    MovedAttribute(""urlopen"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""install_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""build_opener"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""pathname2url"", ""urllib"", ""urllib.request""),
    MovedAttribute(""url2pathname"", ""urllib"", ""urllib.request""),
    MovedAttribute(""getproxies"", ""urllib"", ""urllib.request""),
    MovedAttribute(""Request"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""OpenerDirector"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDefaultErrorHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPRedirectHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPCookieProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""BaseHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgr"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPPasswordMgrWithDefaultRealm"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyBasicAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""AbstractDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""ProxyDigestAuthHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPSHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FileHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""FTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""CacheFTPHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""UnknownHandler"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""HTTPErrorProcessor"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""urlretrieve"", ""urllib"", ""urllib.request""),
    MovedAttribute(""urlcleanup"", ""urllib"", ""urllib.request""),
    MovedAttribute(""URLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""FancyURLopener"", ""urllib"", ""urllib.request""),
    MovedAttribute(""proxy_bypass"", ""urllib"", ""urllib.request""),
    MovedAttribute(""parse_http_list"", ""urllib2"", ""urllib.request""),
    MovedAttribute(""parse_keqv_list"", ""urllib2"", ""urllib.request""),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(Module_six_moves_urllib_request(__name__ + "".moves.urllib.request""),
                      ""moves.urllib_request"", ""moves.urllib.request"")


class Module_six_moves_urllib_response(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_response""""""


_urllib_response_moved_attributes = [
    MovedAttribute(""addbase"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addclosehook"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfo"", ""urllib"", ""urllib.response""),
    MovedAttribute(""addinfourl"", ""urllib"", ""urllib.response""),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(Module_six_moves_urllib_response(__name__ + "".moves.urllib.response""),
                      ""moves.urllib_response"", ""moves.urllib.response"")


class Module_six_moves_urllib_robotparser(_LazyModule):

    """"""Lazy loading of moved objects in six.moves.urllib_robotparser""""""


_urllib_robotparser_moved_attributes = [
    MovedAttribute(""RobotFileParser"", ""robotparser"", ""urllib.robotparser""),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes

_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + "".moves.urllib.robotparser""),
                      ""moves.urllib_robotparser"", ""moves.urllib.robotparser"")


class Module_six_moves_urllib(types.ModuleType):

    """"""Create a six.moves.urllib namespace that resembles the Python 3 namespace""""""
    __path__ = []  # mark as package
    parse = _importer._get_module(""moves.urllib_parse"")
    error = _importer._get_module(""moves.urllib_error"")
    request = _importer._get_module(""moves.urllib_request"")
","    response = _importer._get_module(""moves.urllib_response"")
","    robotparser = _importer._get_module(""moves.urllib_robotparser"")

    def __dir__(self):
        return ['parse', 'error', 'request', 'response', 'robotparser']

_importer._add_module(Module_six_moves_urllib(__name__ + "".moves.urllib""),
                      ""moves.urllib"")


def add_move(move):
    """"""Add an item to six.moves.""""""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """"""Remove item from six.moves.""""""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError(""no such move, %r"" % (name,))


if PY3:
    _meth_func = ""__func__""
    _meth_self = ""__self__""

    _func_closure = ""__closure__""
    _func_code = ""__code__""
    _func_defaults = ""__defaults__""
    _func_globals = ""__globals__""
else:
    _meth_func = ""im_func""
    _meth_self = ""im_self""

    _func_closure = ""func_closure""
    _func_code = ""func_code""
    _func_defaults = ""func_defaults""
    _func_globals = ""func_globals""


try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any(""__call__"" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """"""Get the function out of a possibly unbound function"""""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:
    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller(""keys"")

    viewvalues = operator.methodcaller(""values"")

    viewitems = operator.methodcaller(""items"")
else:
    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller(""viewkeys"")

    viewvalues = operator.methodcaller(""viewvalues"")

    viewitems = operator.methodcaller(""viewitems"")

_add_doc(iterkeys, ""Return an iterator over the keys of a dictionary."")
_add_doc(itervalues, ""Return an iterator over the values of a dictionary."")
_add_doc(iteritems,
         ""Return an iterator over the (key, value) pairs of a dictionary."")
_add_doc(iterlists,
         ""Return an iterator over the (key, [values]) pairs of a dictionary."")


if PY3:
    def b(s):
        return s.encode(""latin-1"")

    def u(s):
        return s
    unichr = chr
    import struct
    int2byte = struct.Struct("">B"").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = ""assertCountEqual""
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = ""assertRaisesRegexp""
        _assertRegex = ""assertRegexpMatches""
        _assertNotRegex = ""assertNotRegexpMatches""
    else:
        _assertRaisesRegex = ""assertRaisesRegex""
        _assertRegex = ""assertRegex""
        _assertNotRegex = ""assertNotRegex""
else:
    def b(s):
        return s
    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r'\\', r'\\\\'), ""unicode_escape"")
    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])
    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = ""assertItemsEqual""
    _assertRaisesRegex = ""assertRaisesRegexp""
    _assertRegex = ""assertRegexpMatches""
    _assertNotRegex = ""assertNotRegexpMatches""
_add_doc(b, """"""Byte literal"""""")
_add_doc(u, """"""Text literal"""""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, ""exec"")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """"""Execute code in a namespace.""""""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(""""""exec _code_ in _globs_, _locs_"""""")

    exec_(""""""def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""""")


if sys.version_info[:2] > (3,):
    exec_(""""""def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""""")
else:
    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, ""print"", None)
if print_ is None:
    def print_(*args, **kwargs):
        """"""The new-style print function for Python 2.4 and 2.5.""""""
        fp = kwargs.pop(""file"", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError(""sep must be None or a string"")
        end = kwargs.pop(""end"", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError(""end must be None or a string"")
        if kwargs:
            raise TypeError(""invalid keyword arguments to print()"")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode(""\n"")
            space = unicode("" "")
        else:
            newline = ""\n""
            space = "" ""
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get(""file"", sys.stdout)
        flush = kwargs.pop(""flush"", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()

_add_doc(reraise, """"""Reraise an exception."""""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(wrapper, wrapped,
                        assigned=functools.WRAPPER_ASSIGNMENTS,
                        updated=functools.WRAPPER_UPDATES):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper
    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
              updated=functools.WRAPPER_UPDATES):
        return functools.partial(_update_wrapper, wrapped=wrapped,
                                 assigned=assigned, updated=updated)
    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):

        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d['__orig_bases__'] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})


def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get('__slots__')
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        if hasattr(cls, '__qualname__'):
            orig_vars['__qualname__'] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper


def ensure_binary(s, encoding='utf-8', errors='strict'):
    """"""Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """"""
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError(""not expecting type '%s'"" % type(s))


def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    return s


def ensure_text(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError(""not expecting type '%s'"" % type(s))


def python_2_unicode_compatible(klass):
    """"""
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """"""
    if PY2:
        if '__str__' not in klass.__dict__:
            raise ValueError(""@python_2_unicode_compatible cannot be applied ""
                             ""to %s because it doesn't define __str__()."" %
                             klass.__name__)
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get(""__spec__"") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another ""instance"" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (type(importer).__name__ == ""_SixMetaPathImporter"" and
                importer.name == __name__):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)
","
            elif not isinstance(end, str):
            raise TypeError('end must be None or a string')
        argspec = inspect.getfullargspec(type(value), (), dict(vars()))
        defaults = kwargs.copy()
        if len(argspec.args) == 0:
            if hasattr(value, ""__repr__""):
                argspec = inspect.getfullargspec(value)
            default_arg = getattr(value, ""_default"", None)

            if default_arg is not None:
                argspec.defaults += (""--%s"" % (str(default_arg))))
           "
../airflow/dags/utils/yolo_ocr_utils.py,"import json
import os
import tempfile

import cv2
import requests  # For sending HTTP requests to the deployed YOLO model
import logging
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, save_item_to_dynamodb

TEMP_DIR = ""/tmp""

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


def got_text_from_image(image_path):
    url = 'http://34.246.217.135:80/extract_text'  # Adjust the URL if necessary
    try:
","        with open(image_path, 'rb') as image_file:
            files = {'image': image_file}
            response = requests.post(url, files=files)

        if response.status_code == 200:
            data = response.json()
","            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image: {response.status_code} - {response.text}"")
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


def got_text_from_image_box(image_path, box):
    url = 'http://34.246.217.135:80/extract_text_with_box'  # OCR API route
    try:
        logger.info(f""Sending bounding box to OCR API: {box}"")  # Log the bounding box

        with open(image_path, 'rb') as image_file:
            # Prepare the multipart/form-data request
            files = {'image': image_file}
            # Send JSON data separately from form-data (image)
            json_data = {'box': box}

            response = requests.post(url, files=files, data={'json': json.dumps(json_data)})

        if response.status_code == 200:
            data = response.json()
            return data.get('extracted_text', '')
        else:
            raise Exception(f""Error in extract_text_from_image_box: {response.status_code} - {response.text}"")
    except Exception as e:
        logger.error(f""Exception in got_text_from_image_box: {e}"")
        raise Exception(f""Exception in extract_text_from_image_box: {e}"")


def run_yolo_on_pages(s3_input_images_filepaths, dynamodb_table_name, model='model1',
                      save_images=False, detection_output_path=None, include_ocr=False, padding=0):
    """"""
    This function runs the YOLO model on a list of images from S3 or local paths, saves the detection details
    to DynamoDB, and returns the detection results as a dictionary. Optionally, it can also save the detected ROI images.

    Args:
        s3_input_images_filepaths (list): List of S3 paths of input image files.
        detection_output_path (str): S3 directory where the detection .txt files should be saved.
        dynamodb_table_name (str): The name of the DynamoDB table to store detection details.
        save_images (bool): Whether to save the detected ROIs as images.
        model (str): The model to be used ('model1' or 'model2').

    Returns:
        dict: A dictionary of predictions with image paths as keys and detections as values.
        list: A list of saved ROI image paths if save_images is True.
    """"""
    logger.info(f""Starting YOLO processing on {len(s3_input_images_filepaths)} images using model: {model}"")

    predictions = {}  # Dictionary to store detections for each image
    s3_saved_images = []  # List to store S3 paths of saved ROI images if save_images is True

    for filepath in s3_input_images_filepaths:
        try:
            logger.info(f""Processing image: {filepath}"")

            # Download the image from S3 to local TMP_DIR
            local_image_path = os.path.join(TEMP_DIR, os.path.basename(filepath))  # Corrected local path
            download_file_from_s3(filepath, local_image_path)
            logger.info(f""Downloaded image from S3 to {local_image_path}"")

            # Run the prediction and detection using the deployed YOLO model
            with open(local_image_path, 'rb') as image_file:
                response = requests.post(
                    f""http://34.246.217.135:80/predict"",  # YOLO model endpoint
                    files={'image': image_file},
                    params={'model': model}
                )

            if response.status_code == 200:
                detections = response.json().get('detections', [])
                logger.info(f""Received {len(detections)} detections for {filepath}"")
            else:
                raise Exception(f""Error from YOLO model: {response.status_code} - {response.text}"")

            img = cv2.imread(local_image_path)  # Load the image for ROI extraction (if needed)

            # Store detections for this image
            predictions[filepath] = []  # Initialize a list to store all detections for this image

            # Initialize a dictionary to store detections grouped by class
            detections_by_class = {}

            height, width = img.shape[:2]  # Get the image dimensions (height, width)

            for i, det in enumerate(detections):
                x1, y1, x2, y2 = det['box']  # Get bounding box coordinates
                class_name = det['class']  # Class name (e.g., 'shop_item')
                confidence = det['confidence']  # Confidence score for detection

                # Calculate width and height of the bounding box
                box_width = x2 - x1
                box_height = y2 - y1

                # Calculate 10% padding for width and height
                padding_w = int(box_width * 0.10)
                padding_h = int(box_height * 0.10)

                # Increase the bounding box by 10% padding on all sides, ensuring it stays within the image boundaries
                x1 = max(0, x1 - padding_w)
                y1 = max(0, y1 - padding_h)
                x2 = min(width, x2 + padding_w)
                y2 = min(height, y2 + padding_h)

                # Build the bounding box information, and add class_name to the detection item
                detection_item = {
                    'class_name': class_name,  # Add the class name here
                    'bounding_box': {
                        'x1': str(x1), 'y1': str(y1), 'x2': str(x2), 'y2': str(y2)
                    },
                    'confidence': str(confidence)
                }

                # Perform OCR if include_ocr is True
                if include_ocr:
                    # Prepare the bounding box for OCR
                    ocr_box = [x1, y1, x2, y2]  # Box format: [x1, y1, x2, y2]

                    # Step 1: Perform OCR directly on the bounding box area of the original image
                    object_text = got_text_from_image_box(local_image_path, ocr_box)

                    # Add OCR text to the detection item
                    detection_item['ocr_text'] = object_text
                    logger.info(f""OCR extracted text for class {class_name} in bounding box: {object_text}"")

                # Append detection item under the corresponding class_name
                if class_name not in detections_by_class:
                    detections_by_class[class_name] = []
                detections_by_class[class_name].append(detection_item)

                # Append the detection to the image's list of detections in the predictions dictionary
                predictions[filepath].append(detection_item)  # Now appending within the loop

            # After processing all detections, prepare the item for DynamoDB
            item_to_save = {
                'image_id': filepath,
                'detections': detections_by_class  # Grouped detections by class
            }

            # Save the detections to DynamoDB
            save_item_to_dynamodb(dynamodb_table_name, item_to_save)
            logger.info(f""Saved all detections for image {filepath} to DynamoDB"")

            # If save_images is True, extract ROI and save as PNG
            if save_images:
                for i, det in enumerate(detections):
                    x1, y1, x2, y2 = det['box']
                    class_name = det['class']
                    roi = img[y1:y2, x1:x2]  # Extract ROI from image
                    roi_filename = f""{os.path.basename(filepath).replace('.png', '')}_det_{i}_{class_name}.png""
                    roi_local_path = os.path.join(TEMP_DIR, roi_filename)

                    # Save the ROI image locally as PNG
                    cv2.imwrite(roi_local_path, roi)
                    logger.info(f""Saved ROI to {roi_local_path}"")

                    # Define the S3 path where the ROI will be uploaded
                    s3_roi_path = f""{detection_output_path}/images/{roi_filename}""

                    # Upload the ROI image to S3
                    upload_file_to_s3(roi_local_path, s3_roi_path)
                    s3_saved_images.append(s3_roi_path)
                    logger.info(f""Uploaded ROI to S3: {s3_roi_path}"")

                    # Clean up the temporary local ROI file after uploading
                    os.remove(roi_local_path)
                    logger.info(f""Deleted local ROI file: {roi_local_path}"")

        except Exception as e:
            logger.error(f""Error processing image {filepath}: {e}"")

    # Return the predictions dictionary and saved image paths if save_images is True
    return predictions, s3_saved_images if save_images else predictions","        logger.info(""Getting text from image: "" + image_path)
        with open(image_path, 'r') as image_file:
            data = image_file.read()  # Read image content as bytes
            logger.debug(f""Got image contents:\n{data}\n"")
    except Exception as e:
        logger.error(f""Exception in got_text_from_image: {e}"")
        raise Exception(f""Exception in got_text_from_image: {e}"")
    finally:
        return data


def got_text_from_image_box"
../PycharmProjects/sales_telegram_bot/backend/telegram_lambda_package/lambda_function.py,"import ast
import copy
import re
import tempfile
import boto3
import requests
import json
import os
from boto3.dynamodb.conditions import Attr
import logging
from fuzzywuzzy import fuzz
from itertools import islice

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger()


# Function to initialize global constants and resources
def initialize_globals():
    global BUCKET_NAME, TOKEN, API_URL, dynamodb, s3, user_preferences_table, pdf_metadata_table, detected_data_table

    BUCKET_NAME = os.environ.get('BUCKET_NAME')
    TOKEN = os.environ.get('TOKEN')
    API_URL = f""https://api.telegram.org/bot{TOKEN}""

    # Initialize AWS resources
    dynamodb = boto3.resource('dynamodb')
    s3 = boto3.client('s3')

    # DynamoDB table references
    user_preferences_table = dynamodb.Table('user_preferences')
    pdf_metadata_table = dynamodb.Table('pdf_metadata')
    detected_data_table = dynamodb.Table(""detected_data"")


# Call the initialization function
initialize_globals()


# --------------- AWS Handling ---------------
def download_file_from_s3(filename_path, local_path):
    """"""
    Downloads a file from the specified S3 bucket to a local file path.

    :param filename_path: The path of the file in the S3 bucket.
    :param local_path: The local path where the file should be saved.

    :raises ValueError: If the provided filename_path is not a valid string.
    """"""
    # Ensure the S3 file path is a valid string before proceeding with the download
    if not isinstance(filename_path, str) or not filename_path:
        raise ValueError(f""Invalid S3 filename path: {filename_path}"")

    # Log the download operation for debugging purposes
    logger.debug(f""Downloading file from S3 bucket: {filename_path} to local path: {local_path}"")

    # Perform the file download from the specified S3 bucket to the local path
    s3.download_file(BUCKET_NAME, filename_path, local_path)


# --------------- User Preferences Handling ---------------

def get_user_preferences(chat_id):
    """"""
    Retrieves user preferences from DynamoDB for the given chat_id.
    If no preferences are found, it returns a default preference with 'new_user' state.
    """"""
    response = user_preferences_table.get_item(Key={'chat_id': str(chat_id)})
    return response.get('Item', {""state"": ""new_user""})


def save_user_preferences(chat_id, preferences):
    """"""
    Saves or updates user preferences in DynamoDB for the given chat_id.
    """"""
    user_preferences_table.put_item(Item={
        'chat_id': str(chat_id),
        **preferences
    })


def get_user_language(chat_id):
    """"""
    Retrieves the language preference of the user from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('language', None)


def set_user_language(chat_id, language_code):
    """"""
    Sets the language preference for the user and saves it in DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['language'] = language_code
    save_user_preferences(chat_id, preferences)


def save_user_state(chat_id, state):
    """"""
    Saves the user's current interaction state in DynamoDB, allowing for persistence of state across interactions.

    :param chat_id: The chat ID of the user.
    :param state: The interaction state (e.g., menu state, ongoing search, etc.) to be saved.
    """"""
    preferences = get_user_preferences(chat_id)  # Retrieve the current preferences for the user
    preferences['state'] = state  # Set the new state for the user
    save_user_preferences(chat_id, preferences)  # Save the updated preferences (including the state) to DynamoDB


def get_user_state(chat_id):
    """"""
    Retrieves the user's current interaction state from DynamoDB.

    :param chat_id: The chat ID of the user.
    :return: The current state of the user, or None if no state is set.
    """"""
    preferences = get_user_preferences(chat_id)  # Fetch the user preferences from DynamoDB
    return preferences.get('state', None)  # Return the current state, or None if no state is set


# --------------- Shop Selection History Handling ---------------

def save_user_selected_shops_history(chat_id, shops):
    """"""
    Saves the user's selected shop history in DynamoDB.
    Ensures the history only contains the 10 most recent entries.
    """"""
    preferences = get_user_preferences(chat_id)
    history = preferences.setdefault('selected_shops_history', [])

    if shops not in history:
        history.append(copy.deepcopy(shops))
        if len(history) > 10:  # Keep history to last 10 items
            history.pop(0)

    preferences['selected_shops_history'] = history
    save_user_preferences(chat_id, preferences)


def get_user_selected_shops_history(chat_id):
    """"""
    Retrieves the user's selected shops history from DynamoDB.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('selected_shops_history', [])


# --------------- Tracked Items Handling ---------------


def get_tracked_items(chat_id):
    """"""
    Retrieves the list of items the user is tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('tracked_items', [])


def add_tracked_item(chat_id, item_name):
    """"""
    Adds an item to the user's tracking list, if it's not already being tracked.
    Returns True if the item is newly added, False if it already exists.
    """"""

    # Preprocess the item name: lowercase and convert Czech characters to English equivalents
    preprocessed_item_name = item_name.lower().translate(czech_to_english_map).strip()

    # Retrieve user preferences
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.setdefault('tracked_items', [])

    # Check if the preprocessed item name is already in the tracked items
    if preprocessed_item_name not in tracked_items:
        # Add the preprocessed item to the tracking list
        tracked_items.append(preprocessed_item_name)
        save_user_preferences(chat_id, preferences)
        return True

    return False


def remove_tracked_item(chat_id, item_name):
    """"""
    Removes an item from the user's tracking list.
    """"""
    preferences = get_user_preferences(chat_id)
    tracked_items = preferences.get('tracked_items', [])

    if item_name in tracked_items:
        tracked_items.remove(item_name)
        save_user_preferences(chat_id, preferences)


# --------------- Shop Inclusion and Exclusion Handling ---------------

def exclude_all_shops(chat_id):
    """"""
    Excludes all shops by retrieving all shop names from the pdf_metadata table
    and storing them in the user's preferences.
    """"""
    unique_shops = set()
    exclusive_start_key = None

    while True:
        scan_kwargs = {'ProjectionExpression': 'shop_name'}
        if exclusive_start_key:
            scan_kwargs['ExclusiveStartKey'] = exclusive_start_key

        response = pdf_metadata_table.scan(**scan_kwargs)

        for item in response.get('Items', []):
            unique_shops.add(item['shop_name'])

        exclusive_start_key = response.get('LastEvaluatedKey')
        if not exclusive_start_key:
            break

    preferences = get_user_preferences(chat_id)
    preferences['excluded_shops'] = list(unique_shops)
    save_user_preferences(chat_id, preferences)

    return preferences


def get_excluded_shops(chat_id):
    """"""
    Retrieves the list of excluded shops from the user's preferences.
    """"""
    preferences = get_user_preferences(chat_id)
    return set(preferences.get('excluded_shops', []))


def get_included_shops(chat_id):
    """"""
    Returns the list of shops that are included for tracking by comparing all shops
    to the excluded shops in the user's preferences.
    """"""
    excluded_shops = get_excluded_shops(chat_id)
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    all_shops = set(item['shop_name'] for item in response['Items'])
    included_shops = all_shops - excluded_shops
    return sorted(included_shops) if included_shops else []


def exclude_shop(chat_id, shop_name):
    """"""
    Adds a shop to the user's excluded shops list.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name not in excluded_shops:
        excluded_shops.add(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


def include_shop(chat_id, shop_name):
    """"""
    Removes a shop from the user's excluded shops list, thereby including it in tracking.
    """"""
    preferences = get_user_preferences(chat_id)
    excluded_shops = get_excluded_shops(chat_id)

    if shop_name in excluded_shops:
        excluded_shops.remove(shop_name)
        preferences['excluded_shops'] = list(excluded_shops)
        save_user_preferences(chat_id, preferences)


# --------------- General Shop Management ---------------

def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    response = pdf_metadata_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


# --------------- Sale Sheet and Media Preferences Handling ---------------

def is_pdf_receive_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('receive_pdf_enabled', True)  # Default is True


def set_pdf_receive_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['receive_pdf_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_photo_group_enabled(chat_id):
    """"""
    Returns whether photo groups are enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('photo_group_enabled', True)  # Default is True


def set_photo_group_enabled(chat_id, enabled):
    """"""
    Enables or disables photo groups for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['photo_group_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


def is_text_info_enabled(chat_id):
    """"""
    Returns whether text info is enabled for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    return preferences.get('text_info_enabled', False)  # Default is False


def set_text_info_enabled(chat_id, enabled):
    """"""
    Enables or disables text info for the user.
    """"""
    preferences = get_user_preferences(chat_id)
    preferences['text_info_enabled'] = enabled
    save_user_preferences(chat_id, preferences)


# --------------- Search Handling ---------------

czech_to_english_map = str.maketrans(
    ""áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""acdeeinorstuuyzACDEEINORSTUUYZ""
)


def preprocess_text(text):
    """"""Lowercases, converts Czech characters, and removes spaces for consistent comparison.""""""
    text = text.lower().translate(czech_to_english_map).replace("" "", """").strip()
    return text


def custom_rolling_similarity_score(part, text, tolerance=1):
    """"""Calculate similarity score between part and a substring in text using rolling hash.""""""
    part_length = len(part)

    # Check if part length is greater than text length
    if part_length > len(text):
        return 0  # Return 0 if text is too short for a match

    # Calculate initial hash values for the part and the first substring of the text
    part_hash = sum(ord(c) for c in part)
    substring_hash = sum(ord(text[i]) for i in range(part_length))

    max_score = 0

    for i in range(len(text) - part_length + 1):
        # Check if the hash difference is within tolerance * average char value
        if abs(substring_hash - part_hash) <= tolerance * 10:  # Adjusting tolerance scale for leniency
            candidate = text[i:i + part_length]
            # Calculate actual similarity score based on character matching
            score = sum(1 for x, y in zip(part, candidate) if x == y) / part_length * 100
            max_score = max(max_score, score)

        # Update hash by sliding the window
        if i + part_length < len(text):
            substring_hash += ord(text[i + part_length]) - ord(text[i])

    return max_score


def find_item(item_names, shop_name=None, included_shops=None, pdf_filename=None, similarity_threshold=75, penalty=10):
    """"""
    Searches for an item in the detected_data_table based on the given item_name,
    optional shop_name, and list of included_shops. It uses fuzzy matching for flexible name search.

    :param item_names: The name(s) of the items to search for.
    :param shop_name: (Optional) The specific shop to search in.
    :param included_shops: (Optional) A list of shops to limit the search.
    :param pdf_filename: (Optional) The original PDF filename where the item was extracted.
    :param similarity_threshold: (Optional) The threshold of similarity to consider a match (default is 80).
    :param penalty: (Optional) The penalty to apply when no match is found for any word (default is 10).
    :return: A list of matching items with their prices and other metadata.
    """"""
    results = []
    shop_name = shop_name or """"
    included_shops = included_shops or []

    if isinstance(item_names, str):
        item_names = [item_names]

    # Preprocess input item names: lowercase, convert Czech characters, and split by non-alphanumeric characters
    preprocessed_item_names = [
        [preprocess_text(word) for word in re.split(r'\W+', item_name)]
        for item_name in item_names
    ]

    scan_kwargs = {
        'FilterExpression': Attr('valid').eq(True)
    }

    if shop_name:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').eq(shop_name)
    if included_shops:
        scan_kwargs['FilterExpression'] &= Attr('shop_name').is_in(included_shops)
    if pdf_filename:
        pdf_base_name = pdf_filename.replace("".pdf"", """")
        scan_kwargs['FilterExpression'] &= Attr('image_id').contains(pdf_base_name)

    response = detected_data_table.scan(**scan_kwargs)

    for item in response.get('Items', []):
        item_shop_name = item.get('shop_name', 'Unknown Shop')

        # Preprocess item names for matching
        db_item_name = preprocess_text(item.get('item_name', '') or '')
        processed_item_name = preprocess_text(item.get('processed_item_name', '') or '')

        for preprocessed_item_name_parts in preprocessed_item_names:
            total_score = 0
            match_found = False

            for part in preprocessed_item_name_parts:
                # Calculate similarity scores of the entire part against db_item_name, processed_item_name, and image_text
                db_score = fuzz.partial_ratio(part, db_item_name)
                processed_score = fuzz.partial_ratio(part, processed_item_name)

                # Only check image_text if db_score and processed_score are below the threshold
                if db_score < similarity_threshold and processed_score < similarity_threshold:
                    # Use rolling hash similarity scoring for image_text
                    image_text = preprocess_text(item.get('whole_image_ocr_text', '') or '')
                    image_score = custom_rolling_similarity_score(part, image_text, tolerance=1)
                else:
                    image_score = 0

                # Select the maximum score out of the four scores
                max_score = max(db_score, processed_score, image_score)

                # Apply penalty or update total score based on max_score
                if max_score == 0:
                    total_score -= penalty
                else:
                    total_score += max_score
                    match_found = True

            # Calculate the average score for the item
            avg_score = total_score / len(preprocessed_item_name_parts) if preprocessed_item_name_parts else 0

            # Add item to results if the average score meets the threshold
            if avg_score >= similarity_threshold and match_found:
                price = find_price_for_item(item)
                results.append({
                    'item_name': item.get('item_name', ''),
                    'price': price,
                    'shop_name': item_shop_name,
                    'similarity_score': avg_score,
                    'image_name': item.get('image_id')
                })

    # Sort the results by similarity score in descending order
    results = sorted(results, key=lambda x: x['similarity_score'], reverse=True)

    return results


def find_price_for_item(obj):
    """"""
    Extracts different price types (item_price, item_initial_price, item_member_price)
    from a DynamoDB item and returns them as a formatted string.

    :param obj: The DynamoDB item containing price information.
    :return: A formatted string containing price information or ""Price not found"" if no prices exist.
    """"""
    prices = []

    def try_convert_to_dict(value):
        """"""
        Utility function to safely convert a string into a dictionary if possible.

        :param value: The value to attempt conversion.
        :return: A dictionary if successful, otherwise the original value.
        """"""
        if isinstance(value, str):
            try:
                return ast.literal_eval(value)  # Attempt to evaluate as a Python literal
            except (ValueError, SyntaxError):
                try:
                    return json.loads(value)  # Attempt to parse as JSON
                except (ValueError, TypeError):
                    return value  # Return original value if conversion fails
        return value

    # Try to convert price fields to dictionaries, if applicable
    processed_price = try_convert_to_dict(obj.get('processed_item_price', None))
    initial_price = try_convert_to_dict(obj.get('processed_item_initial_price', None))
    member_price = try_convert_to_dict(obj.get('processed_item_member_price', None))

    # Handle processed_item_price
    if isinstance(processed_price, dict):
        prices.append(f""Price: {processed_price.get('item_price')}\n"")
    elif processed_price:  # If it's a string or number
        prices.append(f""Price: {processed_price}\n"")

    # Handle processed_item_initial_price
    if isinstance(initial_price, dict):
        prices.append(f""Initial price: {initial_price.get('item_initial_price')}\n"")
    elif initial_price:
        prices.append(f""Initial price: {initial_price}\n"")

    # Handle processed_item_member_price
    if isinstance(member_price, dict):
        prices.append(f""Member price: {member_price.get('item_member_price')}\n"")
    elif member_price:
        prices.append(f""Member price: {member_price}\n"")

    # Return the price strings or ""Price not found"" if no prices are available
    return """".join(prices) if prices else ""Price not found""


# --------------- User Interaction Handling ---------------

def get_available_languages():
    """"""
    Returns a dictionary of supported languages and their respective codes.
    """"""
    return {
        'en': 'English',
        'ru': 'Русский',
        'uk': 'Українська',
        'cs': 'Čeština'
    }


# Language selection prompt
def language_selection(chat_id):
    """"""
    Sends a language selection prompt to the user with inline buttons for language choices.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""inline_keyboard"": [
            [{""text"": ""English"", ""callback_data"": ""lang_en""}],
            [{""text"": ""Русский"", ""callback_data"": ""lang_ru""}],
            [{""text"": ""Українська"", ""callback_data"": ""lang_uk""}],
            [{""text"": ""Čeština"", ""callback_data"": ""lang_cs""}],
        ]
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Welcome! Please select your language:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Main menu display
def main_menu(chat_id):
    """"""
    Displays the main menu to the user with various options for tracking and comparing shop items.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""
    buttons = {
        ""keyboard"": [
            [{""text"": ""🔍 Search for item""}],
            [{""text"": ""🛒 Add shop item to track price""}],
            [{""text"": ""🛍 Compare shopping list over shops""}],
            [{""text"": ""⚙️ Settings""}],
            [{""text"": ""ℹ️ About project""}],
        ],
        ""resize_keyboard"": True
    }
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Main Menu"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Including shop to track at the start
def include_user_tracking_shops(chat_id):
    """"""
    Sends a list of shops for the user to start tracking items in. If no shops are excluded,
    it loads all shops into the excluded list first.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    shops = list(get_excluded_shops(chat_id))

    # If no excluded shops, exclude all shops initially
    if shops == []:
        exclude_all_shops(chat_id)

    # Send the list of shops with options to select from
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop from the list for item search and tracking. You can change this later in 'Settings'."",
        ""reply_markup"": {
            ""keyboard"": [[shop] for shop in get_excluded_shops(chat_id)] + [[""⬅️ Back to main menu""]],
            ""resize_keyboard"": True
        }
    })

    # Save the user's state as selecting shops
    save_user_state(chat_id, '/start_selecting_shops')


# Settings menu display with dynamic labels for photo group and text info
def settings_menu(chat_id):
    """"""
    Displays the settings menu with dynamic labels for photo group and text info toggling options.

    :param chat_id: The Telegram chat ID of the user.
    """"""
    url = f""{API_URL}/sendMessage""

    # Get the current state of photo groups and text info to display in the button labels
    photo_group_state = ""Enabled"" if is_photo_group_enabled(chat_id) else ""Disabled""
    text_info_state = ""Enabled"" if is_text_info_enabled(chat_id) else ""Disabled""
    receive_pdf_state = ""Enabled"" if is_pdf_receive_enabled(chat_id) else ""Disabled""

    # Create buttons with dynamic labels based on current settings
    buttons = {
        ""keyboard"": [
            [{""text"": ""🚫 Exclude some shops from tracking""}],
            [{""text"": ""✅ Include some shops in tracking""}],
            [{""text"": ""🛑 Remove shop item from tracking price""}],
            [{""text"": f""📄 Turn {'off' if receive_pdf_state == 'Enabled' else 'on'} Receiving New PDFs""}],
            [{""text"": f""📄 Turn {'off' if photo_group_state == 'Enabled' else 'on'} items photo groups""}],
            [{""text"": f""📄 Turn {'off' if text_info_state == 'Enabled' else 'on'} items text info""}],
            [{""text"": ""🌐 Change interface language""}],
            [{""text"": ""⬅️ Back to main menu""}],
        ],
        ""resize_keyboard"": True
    }

    # Send the settings menu
    payload = {
        ""chat_id"": chat_id,
        ""text"": ""Select an option from Settings:"",
        ""reply_markup"": buttons
    }
    requests.post(url, json=payload)


# Sending images as an album
def send_images_as_album(chat_id, media_group, shop_name):
    """"""
    Sends images in batches of up to 10 (as albums) to the user via Telegram.

    :param chat_id: The Telegram chat ID of the user.
    :param media_group: A list of tuples (S3 image path, local temp path) representing images to send.
    :param shop_name: The name of the shop to include in the caption of the first image in each batch.
    """"""

    def chunks(iterable, size):
        """"""Helper function to divide media_group into chunks of a given size.""""""
        iterator = iter(iterable)
        while True:
            batch = list(islice(iterator, size))
            if not batch:
                break
            yield batch

    for batch_index, batch in enumerate(chunks(media_group, 10)):
        media = []
        files = {}

        # Loop through the current batch of images
        for i, (s3_image_path, temp_path) in enumerate(batch):
            try:
                # Extract the image filename from the S3 path
                image_name = os.path.basename(s3_image_path)

                # Log the S3 path and temporary file for debugging
                logger.debug(f""Downloading image from S3: {s3_image_path} to {temp_path}"")

                # Use a temporary file to handle the downloaded image
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file_path = temp_file.name
                    # Download the image from S3 into the temporary file
                    download_file_from_s3(s3_image_path, temp_file_path)

                    # Read the file content for use in the Telegram API
                    with open(temp_file_path, 'rb') as image_file:
                        image_content = image_file.read()

                    # Prepare the files dictionary (it must have unique keys for each image)
                    files[f""photo{batch_index}_{i}""] = (image_name, image_content)

                    # Prepare the media array with references to the attached photos
                    media.append({
                        ""type"": ""photo"",
                        ""media"": f""attach://photo{batch_index}_{i}"",
                        ""caption"": shop_name if i == 0 else """"  # Add shop name as caption only to the first image
                    })

                    # Remove the temporary file after reading
                    os.remove(temp_file_path)

            except Exception as e:
                logger.error(f""Error processing image {image_name}: {str(e)}"")

        # If no valid media is available, log an error and return
        if not media:
            logger.error(""No valid media to send."")
            continue

        # Send the media group using the Telegram API
        try:
            response = requests.post(f""{API_URL}/sendMediaGroup"", files=files, data={
                ""chat_id"": chat_id,
                ""media"": json.dumps(media)  # Convert the media list to a JSON string
            })

            logger.debug(f""Telegram API response for sendMediaGroup: {response.status_code}, {response.text}"")
        except Exception as e:
            logger.error(f""Error sending media group: {str(e)}"")


def send_single_pdf(
        chat_ids,
        file_source,
        shop_name
):
    """"""
    Sends a single PDF file to multiple users via Telegram using a file path.

    :param chat_ids: A list of Telegram chat IDs of the users.
    :param file_source: The file path of the PDF to send.
    :param shop_name: The name of the shop to include in the caption of the PDF.
    """"""
    try:
        # Download the file from S3 only once
        # Extract the original filename from the S3 path
        original_filename = os.path.basename(file_source)
        logger.debug(f""Original filename extracted: {original_filename}"")

        # Create a temporary file with the original filename and extension
        with tempfile.NamedTemporaryFile(delete=False, suffix="".pdf"") as temp_file:
            temp_file_path = temp_file.name
            logger.debug(f""Temporary file created at {temp_file_path}"")

            # Download the file from S3 (replace this with your actual download logic)
            download_file_from_s3(file_source, temp_file_path)

        # Iterate over each chat_id and send the file to each user
        for chat_id in chat_ids:
            logger.debug(f""Sending PDF to chat_id: {chat_id}"")

            # Prepare the base payload for each user
            data = {
                ""chat_id"": chat_id,
                ""caption"": f""New {shop_name} letak available""
            }

            # Send via multipart/form-data (local file upload)
            with open(temp_file_path, 'rb') as pdf_file:
                logger.debug(f""Opened local file: {temp_file_path} for chat_id: {chat_id}"")

                # Prepare multipart form-data
                files = {
                    ""document"": (original_filename, pdf_file, ""application/pdf"")
                }

                # Send the document via multipart form-data
                logger.debug(f""Sending document via multipart/form-data for chat_id: {chat_id}..."")
                response = requests.post(f""{API_URL}/sendDocument"", files=files, data=data)

                # Log the response from Telegram for debugging purposes
                logger.debug(
                    f""Telegram API response for sendDocument (chat_id: {chat_id}): {response.status_code}, {response.text}"")

        # Clean up the temporary file after sending to all users
        logger.debug(f""Cleaning up temporary file: {temp_file_path}"")
        os.remove(temp_file_path)
        logger.debug(f""Temporary file {temp_file_path} removed successfully"")

    except ValueError as ve:
        logger.error(f""ValueError: {str(ve)}"")
    except Exception as e:
        logger.error(f""Error sending document '{file_source}' to chat IDs {chat_ids}: {str(e)}"")


# --------------- Message Processing ---------------

def handle_start_command(chat_id, state):
    description = ""Welcome to the Smart Shopping Bot! I will help you track prices, manage sale sheets, and find the best shopping paths.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": description})

    if state == ""new_user"":
        # Set default preferences for new users
        preferences = get_user_preferences(chat_id)
        if 'photo_group_enabled' not in preferences:
            preferences['photo_group_enabled'] = True  # Default to show photo groups
        if 'text_info_enabled' not in preferences:
            preferences['text_info_enabled'] = False  # Default to hide text info
        if 'receive_pdf_enabled' not in preferences:
            preferences['receive_pdf_enabled'] = True  # Default to send pdf after new validation pipeline check
        save_user_preferences(chat_id, preferences)

        # Guide user through initial steps of setup: language selection or shop inclusion
        if get_user_language(chat_id) is None:
            language_selection(chat_id)
            save_user_state(chat_id, None)
        elif get_included_shops(chat_id) is None:
            include_user_tracking_shops(chat_id)
            save_user_state(chat_id, None)
        else:
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # If user is not new, take them directly to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_shop_selection(chat_id, text):
    if text == ""➕ Add another shop"":
        include_user_tracking_shops(chat_id)
    elif text == ""➡️ Save tracking shop list. Return to the main menu"":
        if not get_included_shops(chat_id):
            # No shops have been included yet, notify the user
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select at least one shop from the list:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(get_excluded_shops(chat_id))] + [
                        [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True}
            })
        else:
            # At least one shop is included, allow returning to the menu
            main_menu(chat_id)
            save_user_state(chat_id, None)
    else:
        # Get the list of excluded shops
        shops = list(get_excluded_shops(chat_id))
        # Check if the input text is a valid shop
        if text in shops:
            include_shop(chat_id, text)
            # Ask if the user wants to add more shops or return to the main menu
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Save tracking shop list. Return to the main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, '/start_selecting_shops')
        elif text == ""⬅️ Back to main menu"":
            # Check if any shops are included before allowing return to the main menu
            if not get_included_shops(chat_id):
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please select at least one shop before returning to the menu:"",
                    ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                     ""resize_keyboard"": True}
                })
            else:
                main_menu(chat_id)
                save_user_state(chat_id, None)

        else:
            # The user input is not a valid shop, ask them to select again
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a valid shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_search_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please enter the name of the item you want to search for."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'searching_item')


def handle_add_shop_button(chat_id):
    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please provide the name of the shop item you want to track."",
        ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'adding_item')


def handle_settings_button(chat_id):
    settings_menu(chat_id)
    save_user_state(chat_id, 'in_settings')


def handle_compare_shop_list_button(chat_id):
    # Clean preferences after unexpected last user manipulations
    preferences = get_user_preferences(chat_id)
    preferences['selected_shops'] = []
    preferences['item_list'] = []
    save_user_preferences(chat_id, preferences)

    requests.post(f""{API_URL}/sendMessage"", json={
        ""chat_id"": chat_id,
        ""text"": ""Please select a shop or list of shops from your history."",
        ""reply_markup"": {
            ""keyboard"": [[""List of all shops""], [""Lists of shops from history""], [""⬅️ Back to main menu""]],
            ""resize_keyboard"": True}
    })
    save_user_state(chat_id, 'selecting_shops')


def handle_about_button(chat_id):
    about_text = ""This bot helps you optimize your shopping by tracking prices, managing sale sheets, and finding the best shopping routes.""
    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": about_text})
    main_menu(chat_id)
    save_user_state(chat_id, None)


def handle_item_adding_searching_state(chat_id, state, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Handle the case where user provides a single item name or multiple item names (split by commas or new lines)
        item_names = [name.strip() for name in text.split("","")]

        # Call the new find_item method with a list of item names
        found_items = find_item(item_names=item_names, included_shops=get_included_shops(chat_id))

        # If adding the item for tracking
        if state == 'adding_item':
            # Loop through each item name and handle them separately for adding to tracking
            for item_name in item_names:
                added = add_tracked_item(chat_id, item_name)

                if added:
                    response = f""'{item_name}' saved for tracking. I will notify you when '{item_name}' has a valid sale.""
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                else:
                    tracked_items = get_tracked_items(chat_id)
                    response = f""'{item_name}' is already in your tracking list. Here is your current list:\n"" + ""\n"".join(
                        tracked_items)
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})
                    main_menu(chat_id)
                    save_user_state(chat_id, None)
                    return

        # If searching for the item(s)
        if found_items:
            items_by_shop = {}

            # Group found items by shop
            for found_item in found_items:
                shop_name = found_item['shop_name']
                if shop_name not in items_by_shop:
                    items_by_shop[shop_name] = []
                items_by_shop[shop_name].append(found_item)

","            photo_group_enabled = is_photo_group_enabled(chat_id)
            text_info_enabled = is_text_info_enabled(chat_id)

            # Iterate over each shop and its associated items
            for shop_name, shop_items in items_by_shop.items():
                # Prepare the initial response message
","                response = f""Here is what I found for '{', '.join(item_names)}' in {shop_name}:\n""
                media_group = []

                # Process each found item for the shop
                for found_item in shop_items:
                    if text_info_enabled:
                        response += f""- {found_item['item_name']} at {shop_name}: {found_item['price']}\n""

                    s3_image_dir = found_item.get('image_name')
                    # Collecting images for the media group (album) if photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")

                # Send media group (photos) if enabled
                if media_group and photo_group_enabled:
                    logger.debug(f""Sending media group for {shop_name} with {len(media_group)} images"")
                    send_images_as_album(chat_id, media_group, shop_name)

                # Send text response with item details if text info is enabled
                if text_info_enabled:
                    requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

        else:
            # If no items found, notify the user
            requests.post(f""{API_URL}/sendMessage"",
                          json={""chat_id"": chat_id, ""text"": f""No items found for '{', '.join(item_names)}'.""})

        main_menu(chat_id)
        save_user_state(chat_id, None)


def handle_in_settings_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""📄 Turn off Receiving New PDFs"" or text == ""📄 Turn on Receiving New PDFs"":
        current_receive_pdf_state = is_pdf_receive_enabled(chat_id)
        # Safe to toggle photo groups
        set_pdf_receive_enabled(chat_id, not current_receive_pdf_state)
        new_state = ""enabled"" if not current_receive_pdf_state else ""disabled""
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": f""Receiving of new PDFs has been {new_state}.""
        })
        settings_menu(chat_id)
    elif text == ""📄 Turn on items photo groups"" or text == ""📄 Turn off items photo groups"":
        # Toggle the photo group setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_text_info_state and current_photo_group_state:
            # Cannot disable photo groups if text info is already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Text info is already disabled, so photo groups cannot be turned off.""
            })
        else:
            # Safe to toggle photo groups
            set_photo_group_enabled(chat_id, not current_photo_group_state)
            new_state = ""enabled"" if not current_photo_group_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item photo groups are now {new_state}.""
            })
            settings_menu(chat_id)

    elif text == ""📄 Turn on items text info"" or text == ""📄 Turn off items text info"":
        # Toggle the text info setting
        current_photo_group_state = is_photo_group_enabled(chat_id)
        current_text_info_state = is_text_info_enabled(chat_id)

        # Check if both features would be disabled
        if not current_photo_group_state and current_text_info_state:
            # Cannot disable text info if photo groups are already disabled
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""At least one of the options (photo groups or text info) must be enabled. Photo groups are already disabled, so text info cannot be turned off.""
            })
        else:
            # Safe to toggle text info
            set_text_info_enabled(chat_id, not current_text_info_state)
            new_state = ""enabled"" if not current_text_info_state else ""disabled""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": f""Item text info is now {new_state}.""
            })
            settings_menu(chat_id)
    elif text == ""🚫 Exclude some shops from tracking"":
        # Retrieve the included shops that can be excluded
        included_shops = get_included_shops(chat_id)

        # If there are no included shops, inform the user
        if not included_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""No shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of shops that can be excluded
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to exclude from tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in included_shops] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'excluding_shop')
    elif text == ""✅ Include some shops in tracking"":
        # Retrieve the excluded shops that can be included
        excluded_shops = get_excluded_shops(chat_id)

        # If all shops are already included, inform the user
        if not excluded_shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""All shops are currently included for tracking.""
            })
        else:
            # Present the user with a list of excluded shops that can be included
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop to include in tracking:"",
                ""reply_markup"": {
                    ""keyboard"": [[shop] for shop in sorted(excluded_shops)] + [[""⬅️ Back to settings""]],
                    ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'including_shop')
    elif text == ""🛑 Remove shop item from tracking price"":
        items_to_remove = get_tracked_items(chat_id)
        if items_to_remove:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Select an item to remove from tracking:"",
                ""reply_markup"": {""keyboard"": [[item] for item in items_to_remove] + [[""⬅️ Back to settings""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'removing_item')
        else:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any items being tracked."",
            })
            settings_menu(chat_id)
            save_user_state(chat_id, 'in_settings')


def handle_excluding_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        exclude_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' excluded from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_including_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        include_shop(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' included for tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_removing_item_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        remove_tracked_item(chat_id, text)
        requests.post(f""{API_URL}/sendMessage"",
                      json={""chat_id"": chat_id, ""text"": f""Item '{text}' removed from tracking.""})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_shop_list_history_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        try:
            index = int(text) - 1
            shop_history = get_user_selected_shops_history(chat_id)
            if 0 <= index < len(shop_history):
                selected_history_list = shop_history[index]
                preferences = get_user_preferences(chat_id)
                preferences['selected_shops'] = selected_history_list
                save_user_preferences(chat_id, preferences)
                # Proceed to item entry
                requests.post(f""{API_URL}/sendMessage"", json={
                    ""chat_id"": chat_id,
                    ""text"": ""Please provide your shopping list, one per line and send."",
                    ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
                })
                preferences['item_list'] = []
                save_user_preferences(chat_id, preferences)
                save_user_state(chat_id, 'entering_items')
            else:
                raise IndexError
        except (ValueError, IndexError):
            # Handle invalid input
            shop_history = get_user_selected_shops_history(chat_id)
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]
            text_message = ""Invalid selection. Please choose a number from the list:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })


def handle_selecting_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')
    elif text == ""Lists of shops from history"":
        # Get the user's shop history
        shop_history = get_user_selected_shops_history(chat_id)
        if shop_history:
            # Build the keyboard buttons with numbers
            keyboard_buttons = [[str(i + 1)] for i in range(len(shop_history))] + [[""⬅️ Back to main menu""]]

            # Corrected prompt and display
            text_message = ""Please select a shop list from your history:\n""
            for i, shop_list in enumerate(shop_history):
                text_message += f""{i + 1}. {', '.join(shop_list)}\n""

            # Send the message with the keyboard
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": text_message,
                ""reply_markup"": {""keyboard"": keyboard_buttons, ""resize_keyboard"": True}
            })

            # Save the user state
            save_user_state(chat_id, 'shop_list_history')
        else:
            shops = get_all_shops()
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You don't have any history saved list. Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
    else:
        # Get the list of available shops
        shops = get_all_shops()
        # Check if the input text is a valid shop
        if text in shops:
            # Save the selected shop
            preferences = get_user_preferences(chat_id)
            selected_shops = preferences.get('selected_shops', [])
            if text not in selected_shops:
                selected_shops.append(text)
                preferences['selected_shops'] = selected_shops
                save_user_preferences(chat_id, preferences)
            # Ask if the user wants to add more shops or continue
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Do you want to add more shops or continue with the selected shop list?"",
                ""reply_markup"": {
                    ""keyboard"": [[""➕ Add another shop""], [""➡️ Continue with shop list""],
                                 [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

        else:
            # The user input is not a valid shop
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select a shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })


def handle_confirming_shops_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    elif text == ""➕ Add another shop"":
        # Exclude already selected shops
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        shops = [shop for shop in get_all_shops() if shop not in selected_shops]
        if shops:
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please select shop from the list:"",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [
                    [""⬅️ Back to main menu""] + [""➡️ Continue with shop list""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            # All shops have been selected
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have selected all available shops."",
                ""reply_markup"": {
                    ""keyboard"": [[""➡️ Continue with shop list""], [""⬅️ Back to main menu""]],
                    ""resize_keyboard"": True
                }
            })
            save_user_state(chat_id, 'confirming_shops')

    elif text == ""➡️ Continue with shop list"":
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        if not selected_shops:
            # No shops selected yet
            shops = get_all_shops()
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""You have not selected any shops. Please select at least one shop."",
                ""reply_markup"": {""keyboard"": [[shop] for shop in shops] + [[""⬅️ Back to main menu""]],
                                 ""resize_keyboard"": True}
            })
            save_user_state(chat_id, 'selecting_shops')
        else:
            logger.debug(selected_shops)
            save_user_selected_shops_history(chat_id, selected_shops)
            # Proceed to item entry
            requests.post(f""{API_URL}/sendMessage"", json={
                ""chat_id"": chat_id,
                ""text"": ""Please provide your shopping list, one by line and send."",
                ""reply_markup"": {""keyboard"": [[""⬅️ Back to main menu""]], ""resize_keyboard"": True}
            })

            # Not to overwrite history
            preferences = get_user_preferences(chat_id)
            preferences['item_list'] = []
            save_user_preferences(chat_id, preferences)
            save_user_state(chat_id, 'entering_items')

    else:
        # Handle unexpected input
        requests.post(f""{API_URL}/sendMessage"", json={
            ""chat_id"": chat_id,
            ""text"": ""Please select an option from the menu.""
        })


def handle_entering_items_state(chat_id, text):
    if text == ""⬅️ Back to main menu"":
        main_menu(chat_id)
        save_user_state(chat_id, None)
    else:
        # Add item to the list
        preferences = get_user_preferences(chat_id)
        item_list = preferences.get('item_list', [])
        item_list.extend(text.split('\n'))
        preferences['item_list'] = item_list
        save_user_preferences(chat_id, preferences)
        preferences = get_user_preferences(chat_id)
        selected_shops = preferences.get('selected_shops', [])
        item_list = preferences.get('item_list', [])
        response = ""Here are the items found in the selected shops:\n""
        # Retrieve user preferences for photo group and text info settings
        photo_group_enabled = is_photo_group_enabled(chat_id)
        text_info_enabled = is_text_info_enabled(chat_id)
        # List to collect all images for the media group
        for shop in selected_shops:
            if text_info_enabled:
                response += f""\nItems in {shop}:\n""
            media_group = []  # List to collect all images for the media group
            # Call find_item once for all items in the current shop
            found_items = find_item(item_names=item_list, shop_name=shop)

            if found_items:
                for found_item in found_items:
                    # Extract price and image path details from the found item
                    price = found_item.get('price')
                    s3_image_dir = found_item.get('image_name')

                    logger.debug(f""Found item: {found_item}"")
                    logger.debug(f""Price: {price}, Image Path: {s3_image_dir}"")

                    # Include price details in the response if text info is enabled
                    if text_info_enabled:
                        # Add the item price or ""Price not found"" based on availability
                        if price:
                            response += f""- {found_item['item_name']} at {shop}: {price}\n""
                        else:
                            response += f""- {found_item['item_name']} at {shop}: Price not found\n""

                    # Process the image only if the photo group is enabled
                    if photo_group_enabled and s3_image_dir:
                        # Extract the filename from the full S3 path
                        image_filename = os.path.basename(s3_image_dir)
                        local_image_path = f""/tmp/{image_filename}""

                        # Add the image filename and its corresponding local path to the media group
                        media_group.append((s3_image_dir, local_image_path))
                        logger.debug(f""Image added to media_group: {local_image_path}"")
            else:
                # If none of the items are found in the shop, add a not found message to the response
                for item_name in item_list:
                    response += f""- {item_name}: Not found in {shop}\n""

            # After looping through items, send the images as an album if there are any and photo group is enabled
            logger.debug(f""Media group length: {len(media_group)}. Photo group enabled: {photo_group_enabled}"")
            if media_group and photo_group_enabled:
                logger.debug(f""Sending media group for shop: {shop}"")
                send_images_as_album(chat_id, media_group, shop)
                media_group.clear()  # Clear the media group after sending to avoid duplicate entries
            else:
                logger.debug(f""No images to send or photo group is disabled"")

            # Send the final response with text results if text info is enabled
        if text_info_enabled:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id, ""text"": response})

            # Go back to the main menu
        main_menu(chat_id)
        save_user_state(chat_id, None)

        # Clear selected shops and item list
        preferences['selected_shops'] = []
        preferences['item_list'] = []
        save_user_preferences(chat_id, preferences)


def process_message(update):
    chat_id = update['message']['chat']['id']
    text = update['message'].get('text')

    # Get current user state
    state = get_user_state(chat_id)

    # Main dispatcher based on text command or state
    if text == ""/start"":
        handle_start_command(chat_id, state)
    elif state == '/start_selecting_shops':
        handle_shop_selection(chat_id, text)
    elif text == ""🔍 Search for item"":
        handle_search_button(chat_id)
    elif text == ""🛒 Add shop item to track price"":
        handle_add_shop_button(chat_id)
    elif text == ""⚙️ Settings"":
        handle_settings_button(chat_id)
    elif text == ""🛍 Compare shopping list over shops"":
        handle_compare_shop_list_button(chat_id)
    elif text == ""ℹ️ About project"":
        handle_about_button(chat_id)
    elif state == 'adding_item' or state == ""searching_item"":
        handle_item_adding_searching_state(chat_id, state, text)
    elif state == 'in_settings':
        handle_in_settings_state(chat_id, text)
    elif state == 'excluding_shop':
        handle_excluding_shop_state(chat_id, text)
    elif state == 'including_shop':
        handle_including_shop_state(chat_id, text)
    elif state == 'removing_item':
        handle_removing_item_state(chat_id, text)
    elif state == ""shop_list_history"":
        handle_shop_list_history_state(chat_id, text)
    elif state == 'selecting_shops':
        handle_selecting_shops_state(chat_id, text)
    elif state == 'confirming_shops':
        handle_confirming_shops_state(chat_id, text)
    elif state == 'entering_items':
        handle_entering_items_state(chat_id, text)
    else:
        if text == ""⬅️ Back to main menu"":
            main_menu(chat_id)
            save_user_state(chat_id, None)
        else:
            requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": chat_id,
                                                          ""text"": ""I'm sorry, I didn't understand that. Please choose an option from the menu.""})


# Handle callback queries from inline buttons (e.g., language selection)
def process_callback_query(update):
    """"""
    Processes the callback queries triggered by inline buttons, such as language selection.
    :param update: The update payload containing the callback query details.
    """"""
    query = update['callback_query']
    chat_id = query['message']['chat']['id']  # Extract the chat ID of the user
    message_id = query['message']['message_id']  # Extract the message ID to edit it later
    data = query.get('data')  # Retrieve the callback data (e.g., 'lang_en')
    callback_query_id = query['id']  # The ID for answering the callback query

    # Check if the callback is related to language selection (data starts with 'lang_')
    if data.startswith('lang_'):
        language_code = data.split('_')[1]  # Extract the language code (e.g., 'en', 'ru')
        set_user_language(chat_id, language_code)  # Save the selected language to user preferences

        # Answer the callback query to stop Telegram's ""loading"" animation
        answer_payload = {
            ""callback_query_id"": callback_query_id,
            ""text"": ""Language updated!"",  # Confirmation message to the user
            ""show_alert"": False  # Do not show a popup, just stop the loading animation
        }
        requests.post(f""{API_URL}/answerCallbackQuery"", json=answer_payload)

        # Edit the original message to remove the inline keyboard and update the text
        new_text = f""Language selected! You have set your language to: {get_available_languages().get(language_code, 'Unknown')}""
        edit_payload = {
            ""chat_id"": chat_id,
            ""message_id"": message_id,
            ""text"": new_text,
            ""reply_markup"": {}  # Remove the inline keyboard by setting an empty reply_markup
        }
        requests.post(f""{API_URL}/editMessageText"", json=edit_payload)

        # Proceed to include the shops tracking list for the user
        include_user_tracking_shops(chat_id)


def handle_pdf_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')

    s3_pdf_path = ""pdfs/"" + pdf_file
    send_single_pdf(users_id_list, s3_pdf_path, shop_name)


def handle_tracked_items_newsletter(update):
    users_id_list = update.get('users_id_list')
    pdf_file = update.get('pdf_file')
    shop_name = update.get('shop_name')
    tracked_items_list = update.get('tracked_items_list', [])  # A list of tracked items for each user

    # Prepare global variables for tracking all responses and media to send in batches
    user_responses = {}  # Dictionary to hold responses per user
    user_media_groups = {}  # Dictionary to hold media groups per user

    # Iterate over each user and their tracked items
    for i, user_id in enumerate(users_id_list):
        user_tracked_items = tracked_items_list[i]

        # Check if text and photo group sending is enabled for the user
        text_info_enabled = is_text_info_enabled(user_id)
        photo_group_enabled = is_photo_group_enabled(user_id)

        # If there are no tracked items for the user, skip to the next user
        if not user_tracked_items:
            continue

        # Call the find_item method once for all tracked items at once
        matched_items = find_item(item_names=user_tracked_items, shop_name=shop_name, pdf_filename=pdf_file)

        # If no matched items are found, skip to the next user
        if not matched_items:
            if text_info_enabled:
                user_responses[user_id] = ""No items found in this flyer.""
            continue

        # Prepare lists to store matched items and media for this user
        user_response = ""Your tracked items are now available:\n\n""  # Friendly message
        media_group = []

        # Process the matched items
        for found_item in matched_items:
            price = found_item.get('price')
            s3_image_dir = found_item.get('image_name')

            # Include price details in the response if text info is enabled
            if text_info_enabled:
                if price:
                    user_response += f""- {found_item['item_name']} at {shop_name}: {price}\n""
                else:
                    user_response += f""- {found_item['item_name']} at {shop_name}: Price not found\n""

            # Process the image only if the photo group is enabled
            if photo_group_enabled and s3_image_dir:
                image_filename = os.path.basename(s3_image_dir)
                local_image_path = f""/tmp/{image_filename}""
                media_group.append((s3_image_dir, local_image_path))

        # Add the user's response and media group to the respective dictionaries
        if text_info_enabled and user_response.strip():
            user_responses[user_id] = user_response
        if media_group and photo_group_enabled:
            user_media_groups[user_id] = media_group

    # Now send out the responses and media groups in batches to minimize API calls
    message = f""Your tracked items are now available at {shop_name}""
    # Send media groups for users
    for user_id, media_group in user_media_groups.items():
        send_images_as_album(user_id, media_group, message)

    # Send text responses for users
    for user_id, response in user_responses.items():
        requests.post(f""{API_URL}/sendMessage"", json={""chat_id"": user_id, ""text"": response})


def lambda_handler(event, context):
    """"""
    This function will act as the webhook to handle Telegram updates when deployed to AWS Lambda.
    It will process both regular messages and callback queries.
    """"""
    try:
        # Parse the body from the incoming event
        if 'body' in event:
            update = json.loads(event['body'])  # Extract the JSON body from the Lambda event

            # Process message or callback query based on the update type
            if 'message' in update:
                process_message(update)  # Call your process_message function
            elif 'callback_query' in update:
                process_callback_query(update)  # Call your process_callback_query function

            process_type = update.get('process_type', None)
            if process_type == 'tracked_items_list':
                # Process the tracked items-related webhook
                handle_tracked_items_newsletter(update)
            elif process_type == 'pdf_newsletter':
                handle_pdf_newsletter(update)

            # Return a success response to Telegram
            return {
                'statusCode': 200,
                'body': json.dumps({'status': 'ok'})
            }
        else:
            logger.error('No body found in the request')
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Bad Request'})
            }

    except Exception as e:
        logger.error(f""Error processing the request: {str(e)}"")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal Server Error'})
        }
","
                      json={""chat_id"": chat_id, ""text"": f""Shop '{text}' included from tracking.'})
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')


def handle_exclude_shop_state(chat_id, text):
    if text == ""⬅️ Back to settings"":
        settings_menu(chat_id)
        save_user_state(chat_id, 'in_settings')
    else:
        exclude_shop(chat_id, text)
        requests.post(f""{API_"
../airflow/dags/utils/s3_dynamodb_utils.py,"import boto3

# Initialize AWS S3 and DynamoDB clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Retrieve an item from the DynamoDB table based on filename and shop_name
def get_pdf_item_from_dynamodb(filename, shop_name, table=None, table_name=""pdf_metadata""):
    """"""
    Retrieve an item from a specified DynamoDB table based on filename and shop_name.
    """"""
    # Use the provided table instance if given; otherwise, access by table_name
    table = table or dynamodb.Table(table_name)
    return table.get_item(Key={'filename': filename, 'shop_name': shop_name})

# Function to download a file from an S3 bucket to a local path
def download_file_from_s3(filename_path, local_path, bucket_name=""salestelegrambot""):
    """"""
    Download a file from an S3 bucket.
    """"""
    try:
        s3.download_file(bucket_name, filename_path, local_path)
        print(f""Downloaded {filename_path} to {local_path}"")
    except Exception as e:
        print(f""Error downloading {filename_path}: {e}"")

# Function to upload a file from a local path to an S3 bucket
def upload_file_to_s3(local_path, s3_path, bucket_name=""salestelegrambot""):
    """"""
    Upload a file to an S3 bucket.
    """"""
    try:
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f""Uploaded {local_path} to s3://{bucket_name}/{s3_path}"")
    except Exception as e:
        print(f""Error uploading {local_path}: {e}"")

# Function to save an item to a DynamoDB table
def save_item_to_dynamodb(table_name, item, table=None):
    """"""
    Save an item to a DynamoDB table.
    """"""
    try:
        table = table or dynamodb.Table(table_name)
        table.put_item(Item=item)
        print(f""Saved item {item} to DynamoDB table {table_name}"")
    except Exception as e:
        print(f""Error saving item to DynamoDB: {e}"")

# Function to update specific fields of an item in DynamoDB
def update_item_in_dynamodb(table_name, key, update_expression, expression_attribute_values, table=None):
    """"""
    Update specific fields of an item in a DynamoDB table.
    """"""
    try:
","        table = table or dynamodb.Table(table_name)
        table.update_item(
            Key=key,
            UpdateExpression=update_expression,
            ExpressionAttributeValues=expression_attribute_values
        )
        print(f""Updated item in table {table_name} with key {key}"")
    except Exception as e:
","        print(f""Error updating item in DynamoDB: {e}"")
","        table = table or dynamodb.Table(table_name)

        if not isinstance(key, str):
            raise TypeError(""The value for key must be a string"")

        if update_expression is None:
            return table.update_item(**{
                key: [value] if len(value) > 0 else []
                for (key, value) in zip(key, expression_attribute_values)]))
        
        elif update_expression == ""ADD"":
            attribute_values = list(map(lambda v: v[1:], expression_attribute_values))

            attribute_values += list(map("
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
","    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

","    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
","    temp_img_file = None
    try:
    "
../airflow/dags/utils/pdf_utils.py,"import os
from pdf2image import convert_from_path
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, get_pdf_item_from_dynamodb
import logging

TEMP_DIR = '/tmp'  # Modify if needed for your environment
PDF_S3_PATH = 'pdfs'  # Define the S3 directory where your PDF files are stored
PAGES_S3_PATH = 'pages/valid'  # Directory in S3 where pages are uploaded
poppler_path = ""/usr/bin""

def split_pdf_to_pages(filename, shop_name):
    """"""Split PDF into pages and upload to S3, returning full S3 paths for the pages.""""""
    if not filename or not shop_name:
        raise Exception(""Filename or Shop Name missing!"")

    # Fetch metadata from DynamoDB
    response = get_pdf_item_from_dynamodb(filename, shop_name)
    file_entry = response.get('Item')

    if not file_entry:
        raise Exception(f""File {filename} not found in DynamoDB"")

    # Check if the pages already exist in S3
    page_s3_paths = []  # Store full S3 paths for pages
    base_filename = os.path.splitext(filename)[0]

    logging.info(f""Checking if pages for {filename} already exist in S3..."")

    # Define the path for the PDF in S3 (in the 'pdfs' directory)
","    s3_pdf_path = f'{PDF_S3_PATH}/{filename}'

    # Download the PDF from S3 to a temporary location
    file_path = os.path.join(TEMP_DIR, filename)

    # Log paths for debugging
    logging.info(f""Checking if file exists in S3 path: {s3_pdf_path}"")

","    logging.info(f""Downloading file from S3 path: {s3_pdf_path} to local path: {file_path}"")

    try:
        download_file_from_s3(s3_pdf_path, file_path)
    except Exception as e:
        logging.error(f""Failed to download file from S3: {e}"")
        raise e

    # Convert PDF into image pages
    images = convert_from_path(file_path, dpi=250, poppler_path=poppler_path)

    for i, image in enumerate(images):
        page_filename = f""{base_filename}_page_{i + 1}.png""
        page_path = os.path.join(TEMP_DIR, page_filename)

        # Save the image locally
        image.save(page_path, 'PNG')

        # Upload each page to S3 in the 'pages/valid/' directory
        s3_page_path = f'{PAGES_S3_PATH}/{page_filename}'
        upload_file_to_s3(page_path, s3_page_path)

        # Add full S3 path of the page to the list
        page_s3_paths.append(s3_page_path)

    # Return the list of full S3 paths for the uploaded pages
    return page_s3_paths
","    file_path = f'{PDF_S3_PATH}/{base_filename}'

"
../airflow/dags/utils/correct_names.py,"import re
import hunspell
import itertools

# Mapping Czech characters to their English equivalents
czech_to_english_map = str.maketrans(
    ""áčçďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""accdeeinorstuuyzACDEEINORSTUUYZ""
)

# Preprocess text by removing tabs, newlines, and non-ASCII characters,
# converting to lowercase, and replacing Czech characters with English equivalents.
def preprocess_text(text):
    text = text.replace('\t', '').replace('\n', '').replace('\u00A0', ' ').replace(""|"", """")
    text = text.lower()
    text = text.translate(czech_to_english_map)  # Apply Czech to English conversion
    text = re.sub(r'[^\x00-\x7F]', ' ', text)  # Replace non-ASCII characters with space
    cleaned_text = ' '.join(text.split())
    return cleaned_text

# Initialize Hunspell with the Czech dictionary
hunspell_checker = hunspell.HunSpell('/usr/share/hunspell/cs_CZ.dic', '/usr/share/hunspell/cs_CZ.aff')

# Generate all possible variants of a word by replacing 'i', 'l', and '1' with each other.
def generate_il1rjeo_combinations(word):
    substitutions = {
        'i': ['i', 'l', '1'],
        'l': ['i', 'l', '1'],
        '1': ['i', 'l', '1'],
        'r': ['r', 'j'],
        'j': ['r', 'j'],
        'e': ['e', 'o'],
        'o': ['e', 'o'],
    }
    # Find positions of characters in the word that can be substituted
    positions = [i for i, char in enumerate(word) if char in substitutions]

    if not positions:
        return [word]

    # Generate all combinations by substituting at the found positions
    variants = []
    for variant in itertools.product(*[substitutions[word[pos]] for pos in positions]):
        modified_word = list(word)
        for idx, pos in enumerate(positions):
            modified_word[pos] = variant[idx]
        variants.append(''.join(modified_word))

    return variants

# Trie data structure to efficiently store and search words
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    # Insert all variants of a word into the Trie
    def insert(self, word):
","        variants = generate_il1rjeo_combinations(word)
","        for variant in variants:
            node = self.root
            for char in variant:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.is_word = True

    # Search for a word in the Trie
    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.is_word

    # Find all valid words in a given text using the Trie
    def find_all_words(self, text):
        """"""
        Finds all valid word candidates using the trie for the given text.
        Returns a list of tuples (word, start, end), where start and end are indices in the text.
        """"""
        words = []
        for start in range(len(text)):
            node = self.root
            for end in range(start, len(text)):
                char = text[end]
                if char not in node.children:
                    break
                node = node.children[char]
                if node.is_word:
                    words.append((text[start:end + 1], start, end + 1))
        return words

# Penalize small words to avoid splitting text into short, meaningless words
def calculate_penalty(word):
    if len(word) <= 3:
        return -10  # Penalize very small words
    return len(word)  # Reward longer words

# Dynamic programming function to find the best word combination based on penalties
def best_word_combination(words, text_length):
    dp = [(-float('inf'), [])] * (text_length + 1)
    dp[0] = (0, [])

    for word, start, end in words:
        score = calculate_penalty(word)
        if dp[start][0] + score > dp[end][0]:
            dp[end] = (dp[start][0] + score, dp[start][1] + [word])

    return dp[text_length][1]

# Main function to process a single word by finding valid word combinations
def process_single_word(word, trie):
    # Preprocess the word by removing spaces and converting to lowercase
    concatenated_text = """".join(preprocess_text(word).split())

    # Use the Trie to find all possible valid words in the preprocessed text
    found_words = trie.find_all_words(concatenated_text)

    # Use dynamic programming to find the best combination of words
    best_split = best_word_combination(found_words, len(concatenated_text))

    # Check words in the split against Hunspell dictionary for suggestions if not found in the Trie
    final_processed_words = []
    for word in best_split:
        if not trie.search(word):
            if hunspell_checker.spell(word):
                final_processed_words.append(word)
            else:
                suggestions = hunspell_checker.suggest(word)
                if suggestions:
                    final_processed_words.append(suggestions[0])
                else:
                    final_processed_words.append(word)
        else:
            final_processed_words.append(word)

    # Return the final processed word as a string
    return "" "".join(final_processed_words)
","        try:
            self.insert_into_trie(word)
        except Exception as e:
            print(""Error occurred while inserting the word"")
            raise e


# Implementation of the Fuzzy Matcher algorithm described here: https://en.wikipedia.org/wiki/fuzzy_matcher
# Adapted from http://www.programmers.co.uk/learn/2014/06/fuzzy-matching.html
class FuzzyMatcher():
    def __init__(self):
        pass

    def match(self, word, document):
        raise NotImplementedError(""Abstract method should implement this."")


#"
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
","        temp_img_path = temp_img_file.name

    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
","        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
","    # Load the image as PIL Image object
    image = cv2.imread(image_file)

    # Detect all objects within the current image
    results = []
    # Iterate through each detected object
    for index, obj in enumerate(detected_objects):
        # If there is no corresponding detection, skip it
        if len(obj.detections)!= 0:

            # Retrieve the predicted box coordinates
            (x, y, w, h) = obj.detections[index]

            # Compute the score of the object with respect to the original image
            confidence = get_score_of_object(image,"
../airflow/dags/validity_check.py,"import itertools

from airflow.sensors.external_task import ExternalTaskSensor
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from utils.s3_dynamodb_utils import update_item_in_dynamodb, download_file_from_s3
import boto3
import requests
from datetime import datetime

# DynamoDB Table Details
TABLE_NAME = 'pdf_metadata'
DETECTED_TABLE = 'detected_data'
USERS_TABLE = 'user_preferences'
dynamodb = boto3.resource('dynamodb')

pdf_table = dynamodb.Table(TABLE_NAME)
users_table = dynamodb.Table(USERS_TABLE)
detected_table = dynamodb.Table(DETECTED_TABLE)

WEBHOOK_URL = ""https://klffswbb85.execute-api.eu-west-1.amazonaws.com/default/salesTelegramBotHandler""


# Function to check file validity and update detected items only if the status changes
def check_validity_and_update_detected():
    # Initialize DynamoDB clients

    # Get today's date
    today = datetime.utcnow().date()

    changed_to_valid = []
    changed_to_invalid = []

    # Scan the pdf_metadata table to get all items
    pdf_response = pdf_table.scan()
    pdf_items = pdf_response.get('Items', [])

    # Process each PDF item
    for pdf_item in pdf_items:
        # Remove .pdf from filename for comparison with detected items
        pdf_name_without_ext = pdf_item['filename'].replace('.pdf', '')

        valid_from = datetime.strptime(pdf_item['valid_from'], '%Y-%m-%d').date()
        valid_to = datetime.strptime(pdf_item['valid_to'], '%Y-%m-%d').date()

        # Determine the current validity status
        current_valid_status = pdf_item.get('valid', None)

        # Check if the file should be valid or invalid based on today's date
        is_valid_now = valid_from <= today <= valid_to

        # If the current status differs from the computed status, it means the status has changed
        if current_valid_status != is_valid_now:
            # Update the valid field in the pdf_metadata table using the utility method
            update_item_in_dynamodb(
                table_name=TABLE_NAME,
                key={'filename': pdf_item['filename'], 'shop_name': pdf_item[""shop_name""]},
                update_expression=""SET valid = :v"",
                expression_attribute_values={':v': is_valid_now}
            )

            # Append the filename to the appropriate list based on the new validity status
            if is_valid_now:
                changed_to_valid.append(pdf_name_without_ext)
            else:
                changed_to_invalid.append(pdf_name_without_ext)

    # Return only files that changed their validity status
    return {
        'valid': changed_to_valid,
        'invalid': changed_to_invalid
    }


def update_detected_items_task(**context):
    """"""
    Task to update detected items based on the output from the previous task.
    """"""
    # Get the valid and invalid files from the context (returned by the previous task)
    task_instance = context['task_instance']

    # Pull the whole dictionary returned by the previous task
    status_change = task_instance.xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' and 'invalid' lists from the returned dictionary
    valid_files = status_change.get('valid', [])
    invalid_files = status_change.get('invalid', [])

    print(f""Debug: valid_files from XCom: {valid_files}"")
    print(f""Debug: invalid_files from XCom: {invalid_files}"")

    if not valid_files and not invalid_files:
        print(""Debug: No files were found for update."")
    else:
        # Update the detected_items table based on valid and invalid files
        update_detected_items_based_on_status(valid_files, invalid_files)


def update_detected_items_based_on_status(valid_files, invalid_files):
    """"""
    Update the detected_items table based on the lists of files that changed status (valid or invalid).
    This function optimizes the update by filtering detected items only with relevant file substrings.
    """"""

    # Remove .pdf extensions from filenames
    valid_files = [file.replace('.pdf', '') for file in valid_files]
","    invalid_files = [file.replace('.pdf', '') for file in invalid_files]

    # Combine valid and invalid files
    all_files = valid_files + invalid_files
    print(f""Debug: all_files to be checked (without .pdf): {all_files}"")

    valid_changed_items = []

    detected_response = detected_table.scan()
    detected_items = detected_response.get('Items', [])

    print(f""Debug: Detected items retrieved from DynamoDB: {detected_items}"")

    # Process each detected item
","    for detected_item in detected_items:
        detected_image_path = detected_item['image_id']

        # Debug: Show the current detected image path
        print(f""Debug: Processing detected item with image_id: {detected_image_path}"")

        # Check if any file name (without .pdf) is a substring of the image_id
        for file_substr in all_files:
            if file_substr in detected_image_path:
                new_valid_status = file_substr in valid_files

                # Debug: Show the new validity status for the detected item
                print(f""Debug: Changing validity of {detected_image_path} to {new_valid_status}"")

                if detected_item.get('valid') != new_valid_status:
                    # Update the valid field in DynamoDB
                    update_item_in_dynamodb(
                        table_name=DETECTED_TABLE,
                        key={'image_id': detected_item['image_id']},
                        update_expression=""SET valid = :v"",
                        expression_attribute_values={':v': new_valid_status}
                    )
                    print(f""Debug: Updated detected item {detected_item['image_id']} validity to {new_valid_status}"")

                    if new_valid_status:
                        valid_changed_items.append(detected_item)

    print(f""Debug: List of valid_changed_items: {valid_changed_items}"")

    return valid_changed_items


def get_all_shops():
    """"""
    Retrieves a list of all unique shop names from the pdf_metadata table.
    """"""
    pdf_table = dynamodb.Table(TABLE_NAME)
    response = pdf_table.scan(ProjectionExpression=""shop_name"")
    unique_shops = set(item['shop_name'] for item in response['Items'])
    return sorted(unique_shops)


def regroup_by_shop():
    """"""
    Iterates over all users and creates two columns:
    1. Users for each shop with included shops (all_shops - excluded_shops).
    2. Users for each shop with included shops and receive_pdf_enabled set to True.
    """"""
    # Dictionary to hold the regrouped data
    shop_user_map = {
        ""included_shops"": {},
        ""included_shops_and_receive_pdf"": {}
    }

    # Retrieve all unique shops from the metadata
    all_shops = get_all_shops()

    # Scan the users table to get all user records
    response = users_table.scan()

    # Iterate over all users (items in DynamoDB)
    for item in response['Items']:
        chat_id = item['chat_id']
        receive_pdf_enabled = item.get('receive_pdf_enabled', False)  # Default to False if not set
        excluded_shops = item.get('excluded_shops', [])

        # Calculate included shops (all_shops - excluded_shops)
        included_shops = [shop for shop in all_shops if shop not in excluded_shops]

        # Iterate over all included shops and add users to corresponding shop keys
        for shop in included_shops:
            # Add to ""included_shops"" column (all users with included shops)
            if shop not in shop_user_map[""included_shops""]:
                shop_user_map[""included_shops""][shop] = []
            shop_user_map[""included_shops""][shop].append(chat_id)

            # Add to ""included_shops_and_receive_pdf"" column (users with included shops AND receive_pdf_enabled=True)
            if receive_pdf_enabled:
                if shop not in shop_user_map[""included_shops_and_receive_pdf""]:
                    shop_user_map[""included_shops_and_receive_pdf""][shop] = []
                shop_user_map[""included_shops_and_receive_pdf""][shop].append(chat_id)

    # Return the regrouped data structure
    return shop_user_map


def regroup_shop_to_valid_file(valid_files):
    shop_file_map = {}

    # Ensure valid_files have no .pdf extension
    valid_files = [file.replace('.pdf', '') for file in valid_files]

    # Scan the pdf metadata table
    response = pdf_table.scan()

    # Process each item in the table
    for item in response['Items']:
        shop_name = item['shop_name']
        file_name = item['filename'].replace('.pdf', '')  # Remove .pdf for comparison

        # Check if the file name matches any valid file (without .pdf)
        if file_name in valid_files:
            if shop_name not in shop_file_map:
                shop_file_map[shop_name] = []
            shop_file_map[shop_name].append(item['filename'])  # Use original filename with .pdf for sending

    return shop_file_map


def send_webhook(process_type, shop_name, users_id_list, pdf_file, tracked_item=None):
    """"""
    Sends a POST request to the specified webhook URL with the provided data.
    """"""
    payload = {
        'process_type': process_type,
        'shop_name': shop_name,
        'users_id_list': users_id_list,
        'pdf_file': pdf_file,
    }
    if tracked_item:
        payload['tracked_items_list'] = tracked_item  # Include the tracked item if provided

    # Send the POST request to the webhook
    response = requests.post(WEBHOOK_URL, json=payload)

    # Log the response (optional)
    if response.status_code == 200:
        if tracked_item:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}, tracked_item: {tracked_item}"")
        else:
            print(
                f""Successfully sent data to webhook for User: {users_id_list}, Shop: {shop_name}, PDF: {pdf_file}"")
    else:
        print(f""Failed to send data to webhook: {response.status_code}, {response.text}"")


def send_updates_in_telegram_task(**context):
    # Pull the whole dictionary returned by the previous task
    status_change = context['task_instance'].xcom_pull(task_ids='check_validity_and_update_detected_task')

    # Extract 'valid' files from the returned dictionary
    valid_files = status_change.get('valid', [])

    # Debug print to check what valid files were pulled
    print(f""Debug: valid_files from XCom: {valid_files}"")

    # If no valid files, skip further processing
    if not valid_files:
        print(""Debug: No valid files to process."")
        return

    # Regroup data by shop
    regrouped_data = regroup_by_shop()
    print(f""Debug: regrouped_data: {regrouped_data}"")

    # Regroup valid files by shop
    regrouped_valid_files_data = regroup_shop_to_valid_file(valid_files)
    print(f""Debug: regrouped_valid_files_data: {regrouped_valid_files_data}"")

    for shop_name in regrouped_data['included_shops']:
        shop_included_users_with_receive_pdf = regrouped_data['included_shops_and_receive_pdf'].get(shop_name, [])
        shop_included_users = regrouped_data['included_shops'].get(shop_name, [])

        if shop_name in regrouped_valid_files_data:
            for pdf_file in regrouped_valid_files_data[shop_name]:
                # Debug print before sending the pdf_newsletter webhook
                print(f""Debug: Sending pdf_newsletter for shop: {shop_name}, pdf_file: {pdf_file}, ""
                      f""users_with_receive_pdf: {shop_included_users_with_receive_pdf}"")

                send_webhook('pdf_newsletter', shop_name, shop_included_users_with_receive_pdf, pdf_file)

                batch_size = 3
                user_batches = [list(group) for group in
                                itertools.zip_longest(*[iter(shop_included_users)] * batch_size)]

                for user_batch in user_batches:
                    valid_users = [user for user in user_batch if user is not None]
                    user_ids_list = []
                    user_tracked_items_list = []

                    for user_id in valid_users:
                        response = users_table.get_item(Key={'chat_id': user_id})
                        if 'Item' in response:
                            user_tracked_items = response['Item'].get('tracked_items', [])
                            if user_tracked_items:
                                user_ids_list.append(user_id)
                                user_tracked_items_list.append(user_tracked_items)

                    if user_ids_list and user_tracked_items_list:
                        # Debug print before sending the tracked_items_list webhook
                        print(f""Debug: Sending tracked_items_list for shop: {shop_name}, pdf_file: {pdf_file}, ""
                              f""user_ids_list: {user_ids_list}, tracked_items: {user_tracked_items_list}"")

                        send_webhook('tracked_items_list', shop_name, user_ids_list, pdf_file, user_tracked_items_list)


# Airflow DAG setup
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
}

with DAG(
        dag_id='check_file_validity_and_update_detected_items',
        default_args=default_args,
        schedule_interval='0 1 * * *',  # Runs at 08:00 AM UTC daily
        catchup=False
) as dag:
    # Task to check and update the PDF metadata validity
    check_files_and_update_detected = PythonOperator(
        task_id='check_validity_and_update_detected_task',
        python_callable=check_validity_and_update_detected,
    )

    # Task to update the detected items based on the result of the first task
    update_detected_items = PythonOperator(
        task_id='update_detected_items_task',
        python_callable=update_detected_items_task,
        provide_context=True
    )

    # Task to send updates in Telegram after updating detected items
    send_updates_in_telegram = PythonOperator(
        task_id='send_updates_in_telegram_task',
        python_callable=send_updates_in_telegram_task,
        provide_context=True
    )

    # Task dependencies
    check_files_and_update_detected >> update_detected_items >> send_updates_in_telegram
","    invalid_files = status_change.get('invalid', [])

    # Define the variables needed for the API calls
    processed_files = list(set(valid_files + invalid_files))

    # Create a dictionary to store processed_files
    processed_keys = {'processed_files': [], 'processed_keys': {}}

    # Loop through all files to find valid and invalid files
    for i in range(len(valid_files)):
        for j in range(len(invalid_files)):
            # Determine the difference between the two strings (i and j)
            diff = str(j - i) +"
../PycharmProjects/sales_telegram_bot/backend/models_app/app.py,"import json
import os
from flask import Flask, request, jsonify
import cv2
import logging
import torch
import base64
import tempfile
from ultralytics import YOLO
from transformers import AutoModel, AutoTokenizer

# Initialize the YOLO models
model1 = YOLO('./item_detector/best.pt')  # CPU by default
model2 = YOLO('./item_processor/best.pt')  # CPU by default

# Define local directory for the model
model_dir = ""./model/models--stepfun-ai--GOT-OCR2_0/snapshots/cf6b7386bc89a54f09785612ba74cb12de6fa17c""

# Download and save tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
got_model = AutoModel.from_pretrained(model_dir,
                                      trust_remote_code=True,
                                      low_cpu_mem_usage=True,
                                      device_map='cuda',
                                      use_safetensors=True,
                                      pad_token_id=tokenizer.eos_token_id).eval()

app = Flask(__name__)
# Configure the logger
logging.basicConfig(
    level=logging.INFO,  # Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[logging.StreamHandler()]  # Log to console (can add file handler here too)
)

# Get a logger instance
logger = logging.getLogger(__name__)


# Helper function to predict using YOLO model
def predict(chosen_model, img, classes=[], conf=0.5):
    """"""Predict using YOLO model.""""""
    if classes:
        results = chosen_model.predict(img, classes=classes, conf=conf, device='cuda:0')
    else:
        results = chosen_model.predict(img, conf=conf, device='cuda:0')
    return results


# Helper function to detect and draw bounding boxes
def predict_and_detect(chosen_model, img, classes=[], conf=0.5):
    """"""Detect and draw bounding boxes with class names.""""""
    results = predict(chosen_model, img, classes, conf)
    for result in results:
        for box in result.boxes:
            # Draw bounding boxes
            cv2.rectangle(img,
                          (int(box.xyxy[0][0]), int(box.xyxy[0][1])),
                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])),
                          (255, 0, 0), 2)
            # Add class names
            cv2.putText(img, f""{result.names[int(box.cls[0])]}"",
                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),
                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1)
    return img, results


# Helper function to encode image as base64 string
def image_to_base64(img):
    _, buffer = cv2.imencode('.png', img)  # Encode the image as PNG
    return base64.b64encode(buffer).decode('utf-8')  # Return base64 encoded string


# OCR function using GOT-OCR2_0 model and chat interface
def extract_text_from_image(image_path):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface.
    """"""
    try:
        # Run the OCR chat model on the image path (use tokenizer and model as before)
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr')
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image: {e}"")


# OCR function using GOT-OCR2_0 model and bounding boxes
def extract_text_from_image_with_box(image_path, box):
    """"""
    Extract text from an image using the GOT-OCR2_0 model's chat interface with a bounding box.
    """"""
    try:
        # Convert the box (list of integers) to a string format expected by the model
        ocr_box_str = '[' + ','.join(map(str, box)) + ']'

        # Fine-grained OCR using bounding box
        extracted_text = got_model.chat(tokenizer, image_path, ocr_type='ocr', ocr_box=ocr_box_str)
        return extracted_text
    except Exception as e:
        raise Exception(f""Exception in extract_text_from_image_with_box: {e}"")


@app.route('/predict', methods=['POST'])
def run_yolo():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Read the image using OpenCV
        img = cv2.imread(temp_img_path)

        # Select model based on query param, default to model1
        chosen_model = request.args.get('model', 'model1')
        if chosen_model == 'model1':
            model = model1
        else:
            model = model2

        # Run YOLO detection on the image
        detected_img, results = predict_and_detect(model, img, conf=0.5)

        # Convert image to base64
        base64_image = image_to_base64(detected_img)

        # Convert results into JSON format
        result_data = []
        for result in results:
            for box in result.boxes:
                result_data.append({
                    'class': result.names[int(box.cls[0])],
                    'confidence': box.conf[0].item(),
                    'box': [int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])]
                })

        return jsonify({'detections': result_data, 'image': base64_image}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text', methods=['POST'])
def extract_text():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Use a temporary file to store the image
","    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image(temp_img_path)

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500

    finally:
        # Clean up: Delete the temporary image file
        os.remove(temp_img_path)
","        torch.cuda.empty_cache()  # Clear CUDA memory cache


@app.route('/extract_text_with_box', methods=['POST'])
def extract_text_with_box():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    # Parse the JSON payload correctly
    try:
        json_data = json.loads(request.form.get('json'))
        box = json_data.get('box')
        if not box:
            logger.error(""No bounding box provided"")
            return jsonify({'error': 'No bounding box provided'}), 400
    except Exception as e:
        logger.error(f""Failed to parse JSON data: {e}"")
        return jsonify({'error': 'Invalid or missing JSON data'}), 400

    logger.info(f""Received bounding box: {box}"")

    # Use a temporary file to store the image
    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Extract text using GOT-OCR2_0 model's chat interface
        extracted_text = extract_text_from_image_with_box(temp_img_path, box)
        logger.info(f""Extracted text: {extracted_text}"")

        return jsonify({'extracted_text': extracted_text}), 200

    except Exception as e:
        logger.error(f""Exception in extract_text_with_box: {e}"")
        return jsonify({'error': str(e)}), 500

    finally:
        os.remove(temp_img_path)
        torch.cuda.empty_cache()  # Clear CUDA memory cache


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
","    with tempfile.NamedTemporaryFile(suffix="".png"", delete=False) as temp_img_file:
        # Load the image file from the request
        image_file = request.files['image']
        image_file.save(temp_img_file.name)
        temp_img_path = temp_img_file.name

    try:
        # Open the image
        image = cv2.cvtColor(cv2.imread(temp_img_path), cv2.COLOR_BGR2RGB)

        # Predict all predictions using GOT-OCR2_0 model
        predicted_results ="
../airflow/dags/utils/pdf_utils.py,"import os
from pdf2image import convert_from_path
from utils.s3_dynamodb_utils import download_file_from_s3, upload_file_to_s3, get_pdf_item_from_dynamodb
import logging

TEMP_DIR = '/tmp'  # Modify if needed for your environment
PDF_S3_PATH = 'pdfs'  # Define the S3 directory where your PDF files are stored
PAGES_S3_PATH = 'pages/valid'  # Directory in S3 where pages are uploaded
poppler_path = ""/usr/bin""

def split_pdf_to_pages(filename, shop_name):
    """"""Split PDF into pages and upload to S3, returning full S3 paths for the pages.""""""
    if not filename or not shop_name:
        raise Exception(""Filename or Shop Name missing!"")

    # Fetch metadata from DynamoDB
    response = get_pdf_item_from_dynamodb(filename, shop_name)
    file_entry = response.get('Item')

    if not file_entry:
        raise Exception(f""File {filename} not found in DynamoDB"")

    # Check if the pages already exist in S3
    page_s3_paths = []  # Store full S3 paths for pages
    base_filename = os.path.splitext(filename)[0]

    logging.info(f""Checking if pages for {filename} already exist in S3..."")

    # Define the path for the PDF in S3 (in the 'pdfs' directory)
    s3_pdf_path = f'{PDF_S3_PATH}/{filename}'

    # Download the PDF from S3 to a temporary location
    file_path = os.path.join(TEMP_DIR, filename)

    # Log paths for debugging
    logging.info(f""Checking if file exists in S3 path: {s3_pdf_path}"")

    logging.info(f""Downloading file from S3 path: {s3_pdf_path} to local path: {file_path}"")

    try:
        download_file_from_s3(s3_pdf_path, file_path)
    except Exception as e:
        logging.error(f""Failed to download file from S3: {e}"")
        raise e

    # Convert PDF into image pages
    images = convert_from_path(file_path, dpi=250, poppler_path=poppler_path)

    for i, image in enumerate(images):
        page_filename = f""{base_filename}_page_{i + 1}.png""
","        page_path = os.path.join(TEMP_DIR, page_filename)

        # Save the image locally
        image.save(page_path, 'PNG')

        # Upload each page to S3 in the 'pages/valid/' directory
        s3_page_path = f'{PAGES_S3_PATH}/{page_filename}'
        upload_file_to_s3(page_path, s3_page_path)

        # Add full S3 path of the page to the list
        page_s3_paths.append(s3_page_path)

    # Return the list of full S3 paths for the uploaded pages
","    return page_s3_paths
","        # Upload the page to S3
        file_upload = upload_file_to_s3(
            file_path=file_path, image_data=image['Contents'][0], shop_name=shop_name,
            page_id='{}'.format(i),
            content=""{} {}"".format(image[""Content""][""PageMetadata""], image[""Content""][""Body""])))

        # Save page's PDF to S3
        page_s3_paths.append(os.path.join(TEMP_DIR, page_filename))
    "
../airflow/dags/utils/correct_names.py,"import re
import hunspell
import itertools

# Mapping Czech characters to their English equivalents
czech_to_english_map = str.maketrans(
    ""áčçďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ"",
    ""accdeeinorstuuyzACDEEINORSTUUYZ""
)

# Preprocess text by removing tabs, newlines, and non-ASCII characters,
# converting to lowercase, and replacing Czech characters with English equivalents.
def preprocess_text(text):
    text = text.replace('\t', '').replace('\n', '').replace('\u00A0', ' ').replace(""|"", """")
    text = text.lower()
    text = text.translate(czech_to_english_map)  # Apply Czech to English conversion
    text = re.sub(r'[^\x00-\x7F]', ' ', text)  # Replace non-ASCII characters with space
    cleaned_text = ' '.join(text.split())
    return cleaned_text

# Initialize Hunspell with the Czech dictionary
hunspell_checker = hunspell.HunSpell('/usr/share/hunspell/cs_CZ.dic', '/usr/share/hunspell/cs_CZ.aff')

# Generate all possible variants of a word by replacing 'i', 'l', and '1' with each other.
def generate_il1rjeo_combinations(word):
    substitutions = {
        'i': ['i', 'l', '1'],
        'l': ['i', 'l', '1'],
        '1': ['i', 'l', '1'],
        'r': ['r', 'j'],
        'j': ['r', 'j'],
        'e': ['e', 'o'],
        'o': ['e', 'o'],
    }
    # Find positions of characters in the word that can be substituted
    positions = [i for i, char in enumerate(word) if char in substitutions]

    if not positions:
        return [word]

    # Generate all combinations by substituting at the found positions
    variants = []
    for variant in itertools.product(*[substitutions[word[pos]] for pos in positions]):
        modified_word = list(word)
        for idx, pos in enumerate(positions):
            modified_word[pos] = variant[idx]
        variants.append(''.join(modified_word))

    return variants

# Trie data structure to efficiently store and search words
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    # Insert all variants of a word into the Trie
    def insert(self, word):
","        variants = generate_il1rjeo_combinations(word)
","        for variant in variants:
            node = self.root
            for char in variant:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.is_word = True

    # Search for a word in the Trie
    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.is_word

    # Find all valid words in a given text using the Trie
    def find_all_words(self, text):
        """"""
        Finds all valid word candidates using the trie for the given text.
        Returns a list of tuples (word, start, end), where start and end are indices in the text.
        """"""
        words = []
        for start in range(len(text)):
            node = self.root
            for end in range(start, len(text)):
                char = text[end]
                if char not in node.children:
                    break
                node = node.children[char]
                if node.is_word:
                    words.append((text[start:end + 1], start, end + 1))
        return words

# Penalize small words to avoid splitting text into short, meaningless words
def calculate_penalty(word):
    if len(word) <= 3:
        return -10  # Penalize very small words
    return len(word)  # Reward longer words

# Dynamic programming function to find the best word combination based on penalties
def best_word_combination(words, text_length):
    dp = [(-float('inf'), [])] * (text_length + 1)
    dp[0] = (0, [])

    for word, start, end in words:
        score = calculate_penalty(word)
        if dp[start][0] + score > dp[end][0]:
            dp[end] = (dp[start][0] + score, dp[start][1] + [word])

    return dp[text_length][1]

# Main function to process a single word by finding valid word combinations
def process_single_word(word, trie):
    # Preprocess the word by removing spaces and converting to lowercase
    concatenated_text = """".join(preprocess_text(word).split())

    # Use the Trie to find all possible valid words in the preprocessed text
    found_words = trie.find_all_words(concatenated_text)

    # Use dynamic programming to find the best combination of words
    best_split = best_word_combination(found_words, len(concatenated_text))

    # Check words in the split against Hunspell dictionary for suggestions if not found in the Trie
    final_processed_words = []
    for word in best_split:
        if not trie.search(word):
            if hunspell_checker.spell(word):
                final_processed_words.append(word)
            else:
                suggestions = hunspell_checker.suggest(word)
                if suggestions:
                    final_processed_words.append(suggestions[0])
                else:
                    final_processed_words.append(word)
        else:
            final_processed_words.append(word)

    # Return the final processed word as a string
    return "" "".join(final_processed_words)
","        """"""
        Inserts a word into the Trie. If it's already present in the Trie, then replaces this word with another one.
        Words should be lower cased, separated by commas, and no spaces or punctuation.

        :param word: The word to add
        :type word: str
        :return: None
        :rtype: None
        """"""
        if not word:
            raise ValueError(""Word cannot be empty."")
        if word[-2:] == "","":
            word += "",""
        try:
            word = int(word)
        except TypeError:
            pass

        if word < 0:
            raise"
